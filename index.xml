<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zhangzw</title>
    <link>https://www.ngirl.xyz/</link>
    <description>Recent content on zhangzw</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Wed, 08 Jun 2022 16:35:12 +0800</lastBuildDate><atom:link href="https://www.ngirl.xyz/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>linux遇到一些问题统计</title>
      <link>https://www.ngirl.xyz/posts/9-linux%E9%81%87%E5%88%B0%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 08 Dec 2020 17:42:54 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/9-linux%E9%81%87%E5%88%B0%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</guid>
      <description>记录一些Linux,nginx或其他服务一些问题
  分布式问题: 分布式算法 Basic paxos 算法 1. 获取一个Proposal ID n，为了保证Proposal ID唯一，可采用时间戳+Server ID生成； 2. Proposer向所有Acceptors广播Prepare(n)请求； 3. Acceptor比较n和minProposal，如果n&amp;gt;minProposal，minProposal=n，并且将 acceptedProposal 和 acceptedValue 返回； 4. Proposer接收到过半数回复后，如果发现有acceptedValue返回，将所有回复中acceptedProposal最大的acceptedValue作为本次提案的value，否则可以任意决定本次提案的value； 5. 到这里可以进入第二阶段，广播Accept (n,value) 到所有节点； 6. Acceptor比较n和minProposal，如果n&amp;gt;=minProposal，则acceptedProposal=minProposal=n，acceptedValue=value，本地持久化后，返回；否则，返回minProposal。 7. 提议者接收到过半数请求后，如果发现有返回值result &amp;gt;n，表示有更新的提议，跳转到1；否则value达成一致。 Multi-Paxos 算法  为了解决实际应用中连续多值传输的高效性,改进了Basix Paxos为Multi-Paxos:
 1. 针对每一个要确定的值，运行一次Paxos算法实例（Instance），形成决议。每一个Paxos实例使用唯一的Instance ID标识。 2. 在所有Proposers中选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决。这样没有Proposer竞争，解决了活锁问题。在系统中仅有一个Leader进行Value提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。 raft  为了更简便理解和实现,改进了Multi-Paxos 为raft
 1. 发送的log的是连续的, 也就是说raft 的append 操作必须是连续的. 而paxos 可以并发的. (其实这里并发只是append log 的并发提高, 应用的state machine 还是必须是有序的) 2. 选主是有限制的, 必须有最新, 最全的日志节点才可以当选. 而multi-paxos 是随意的 所以raft 可以看成是简化版本的multi paxos(这里multi-paxos 因为允许并发的写log, 因此不存在一个最新, 最全的日志节点, 因此只能这么做.</description>
    </item>
    
    <item>
      <title>k8s遇到的一些问题统计总结</title>
      <link>https://www.ngirl.xyz/posts/3-k8s%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 10 Nov 2020 14:06:41 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/3-k8s%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</guid>
      <description>不定时更新,文章可能比较散乱,&amp;gt;_&amp;lt;
  1. 单机版k8s pod一直是pending的问题  describe一下pod会发现错误: 1 node(s) had taints that the pod didnt tolerate. 这是因为master上存在污点,pod不会再改节点上创建 两种办法:
  deploy 的时候加上 容忍该污点 直接取消master上的污点  # 取消master上污点 kubectl taint nodes --all node-role.kubernetes.io/master- # 查看taint kubectl describe node node1   2. 修改service-node-port-range  由于traefik部署需要对外开放80端口, 但默认仅允许30000以上端口
 # kubeadm 1.14 配置 apiServer: extraArgs: authorization-mode: Node,RBAC service-node-port-range: 79-33000 # kubeadm 1.10配置 apiServerExtraArgs: service-node-port-range: 79-33000   3. traefik断电后重新启动报错 command traefik error: field not found, node: redirect 看到这个错误猜测可能是用的latest镜像问题, 从`hub.</description>
    </item>
    
    <item>
      <title>K8s源码学习 简单记录</title>
      <link>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Wed, 08 Jun 2022 16:35:12 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95/</guid>
      <description>源码查看rc和rs的区别 通过追寻 NewControllerInitializers 函数中到RS和RC的源码发现, startReplicationController 和 startReplicaSetController 都是 NewBaseController 函数初始化的 ReplicaSetController 结构体, 不过两个传参不通 RS是: ctx.InformerFactory.Apps().V1().ReplicaSets() RC是: ctx.InformerFactory.Core().V1().ReplicationControllers(), 所以我们就需要找到RC和RS的InformerFactory的不同 沿着 NewControllerInitializers 往回找, 我们看到了 StartControllers 中会真正的执行 NewControllerInitializers 初始化的 controllers 代码: for controllerName, initFn := range controllers { debugHandler, started, err := initFn(ctx) } 这里的循环就会执行 startReplicationController(ctx) 和 startReplicaSetController(ctx), 这里的ctx就是StartControllers的第一个参数 回到 (cmd/kube-controller-manager/app/controllermanager.go) Run方法中, StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux) 的controllerContext 正是 我们要找的ctx 往上我们看到 -&amp;gt; CreateControllerContext() -&amp;gt; NewSharedInformerFactory() -&amp;gt; NewSharedInformerFactoryWithOptions 找到了 sharedInformerFactory 结构体 sharedInformerFactory 就是真正的执行了 .Apps().V1().ReplicaSets() 和 .</description>
    </item>
    
    <item>
      <title>Go学习 6.824 lab2 raft2B</title>
      <link>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-6.824-lab2-raft2b/</link>
      <pubDate>Thu, 17 Mar 2022 14:16:09 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-6.824-lab2-raft2b/</guid>
      <description>文档  Raft-Extended 论文翻译 MIT 6.824 Lab2 实验翻译  简单记录 </description>
    </item>
    
    <item>
      <title>Go学习 6.824 lab2 raft2A</title>
      <link>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-6.824-lab2-raft2a/</link>
      <pubDate>Mon, 07 Mar 2022 17:58:09 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-6.824-lab2-raft2a/</guid>
      <description>文档  Raft-Extended 论文翻译 MIT 6.824 Lab2 实验翻译  实现 Raft Leader选举和心跳检测 首先对Raft-Extended的相关几点做个说明 1. raft集群在 ticker 超时 后进行选举, 节点未收到心跳检测就会发起选举 2. 选举的节点会存在几种状态切换 follower -&amp;gt; candidate : 开始选举的时候 candidate -&amp;gt; leader : 选举成功的时候 candidate -&amp;gt; follower : 选举失败的时候 leader -&amp;gt; follower : 旧的过期节点 对于 candidate -&amp;gt; candidate的情况, 这里是被分解成 candidate -&amp;gt; follower -&amp;gt; candidate 3. raft节点会在延迟不同的时间后 在切换成 candidate, 防止多节点同时切换 4. raft节点在成为 candidate 后, 会并发向每个节点发一次 RequestVote 来进行投票, 然后等到有半数同意就成为leader 并启动goroutine 进行持续心跳检测(AppendEntries RPC), 如果没有人同意那么就没人成为leader会等到下次超时发起新的选举 在成为leader之前, raft节点同样可能收到别的 旧leader发来的AppendEntries RPC, 所以在 AppendEntries RPC 方法里面,我们需要如下判断: i) 如果收到的任期号小于当前任期号, 那么直接拒绝忽略 ii) 如果收到的任期号大于等于那就做如下修改: 把自己的状态改为 follower, 记录候选人id, 领导人id , 修改自己的当前任期号 和已提交任期号 代码部分 raft结构体中的Log type Raft struct { .</description>
    </item>
    
    <item>
      <title>Go学习 6.824-lab1-MapReduce实验</title>
      <link>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-6.824-lab1-mapreduce/</link>
      <pubDate>Fri, 04 Mar 2022 13:55:22 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-6.824-lab1-mapreduce/</guid>
      <description>相关知识点了解 课程翻译内容  mit6.824课程 中文翻译 文字版 mit6.824课程 2021 lab1-MapReduce实验翻译   阅读以上内容之后, 对MapReduce概念上 和lab1实验的内容有了大概的了解了
 代码逻辑 简单的流程分析 通过lab1实验内容的了解, 我们知道 我们需要实现的内容是 src/mr/目录的三个文件coordinator.go, rpc.go, worker.go coordinator.go 主要是服务端, 实现任务调度功能(master) worker.go 主要是客户端, 实现任务执行功能(node) rpc.go 主要是rpc端, 实现一些rpc数据传输的结构体 晚上代码的编写之后,我们可以首先测试wc.go的功能 cd 6.824/src/main # 在当前目录生成wc.so文件(之后测试rtiming.go, crash.go同样) go build -race -buildmode=plugin ../mrapps/wc.go # 启动服务端 go run -race mrcoordinator.go pg-*.txt # 启动客户端 go run -race mrworker.go wc.so # 完整之后, 客户端和服务端都会退出, 当前目录生成如下文件: # 其中 mr-X-Y.txt 是map 函数生成的中间文件 NMap * NReduce个 # 其中 mr-out-Y.</description>
    </item>
    
    <item>
      <title>Go学习 令牌桶</title>
      <link>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-%E4%BB%A4%E7%89%8C%E6%A1%B6/</link>
      <pubDate>Fri, 10 Dec 2021 15:32:16 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-%E4%BB%A4%E7%89%8C%E6%A1%B6/</guid>
      <description>首先看个简单例子 package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/gin-gonic/gin&amp;#34; &amp;#34;golang.org/x/time/rate&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;time&amp;#34; &amp;#34;errors&amp;#34; &amp;#34;io/ioutil&amp;#34; ) var ( limit = rate.NewLimiter(rate.Limit(1), 2) ) func limitMiddleware() func(*gin.Context) { return func(ctx *gin.Context) { if !limit.Allow() { ctx.String(http.StatusServiceUnavailable, &amp;#34;服务器忙...&amp;#34;) ctx.Abort() return } ctx.Next() } } func GetOnce(addr string) ([]byte,error) { resp, err := http.Get(&amp;#34;http://&amp;#34;+addr) if err != nil { return nil, err } defer resp.Body.Close() b,err := ioutil.ReadAll(resp.Body) if err != nil { return nil, err } if resp.</description>
    </item>
    
    <item>
      <title>K8s源码学习 Job源码分析</title>
      <link>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-job%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 09 Dec 2021 10:22:02 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-job%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>首先我们知道cronjob和job是受控于controller manager组件的, 下面我们先看看job controller是怎么初始化的
 初始化的流程 1. cmd/kube-controller-manager/controller-manager.go main() -&amp;gt; app.NewControllerManagerCommand() 2. cmd/kube-controller-manager/app/controllermanager.go NewControllerManagerCommand() *cobra.Command -&amp;gt; Run(...) -&amp;gt; NewControllerInitializers(...) map[string]InitFunc // 这里返回控制器的map,后续会对他遍历执行InitFunc  { controllers[&amp;#34;job&amp;#34;] = startJobController // job的初始化  controllers[&amp;#34;cronjob&amp;#34;] = startCronJobController } 3. cmd/kube-controller-manager/app/batch.go startJobController(...) (...) -&amp;gt; NewJobController(...) *JobController // 返回一个JobController  { jm.syncHandler = jm.syncJob // 核心的handler初始化  } -&amp;gt; (jm *JobController) Run(...) -&amp;gt; (jm *JobController) worker() -&amp;gt; for jm.processNextWorkItem() {} // 这里是循环  -&amp;gt; jm.syncHandler(...) // 最终在processNextWorkItem的函数内部调用了syncHandler, 也就是上面初始化的syncJob JobController 结构体 type JobController struct { kubeClient clientset.</description>
    </item>
    
    <item>
      <title>K8s源码学习 client-go延迟队列</title>
      <link>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-client-go%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97/</link>
      <pubDate>Wed, 08 Dec 2021 16:04:39 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-client-go%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97/</guid>
      <description>我们知道在部署 pod的时候, 如果失败了, pod会不断的重试, 重试的时间间隔也是不一样的, 重试到一定次数就不在重试
那我们这里的重试间隔时间是如何控制的呢?
下面我们去了解一下
从 controller 找到 deployment 初始化方法看一下
 1 deployment 初始化  k8s的大多组件代码格式结构是类似的
 1. cmd/kube-controller-manager/controller-manager.go main() -&amp;gt; app.NewControllerManagerCommand() 2. cmd/kube-controller-manager/app/controllermanager.go NewControllerManagerCommand() *cobra.Command -&amp;gt; Run(...) -&amp;gt; NewControllerInitializers(...) map[string]InitFunc // 这里返回控制器的map,后续会对他遍历执行InitFunc  { controllers[&amp;#34;deployment&amp;#34;] = startDeploymentController } 3. cmd/kube-controller-manager/app/apps.go startDeploymentController(...) (...) { dc, err := deployment.NewDeploymentController(...) go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) } 3.1 pkg/controller/deployment/deployment_controller.go -&amp;gt; NewDeploymentController(...) (...) { dc := &amp;amp;DeploymentController{ queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &amp;#34;deployment&amp;#34;), } } -&amp;gt; NewNamedRateLimitingQueue(.</description>
    </item>
    
    <item>
      <title>Go学习 Map源码分析</title>
      <link>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-map%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 22 Nov 2021 13:59:53 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/golang/go%E5%AD%A6%E4%B9%A0-map%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>在看map之前,我们从一个简单的代码开始 1 package main 2 func main() { 3	a := make(map[int]int,209) 4	a[1] = 1 5	print(a[1]) 6 }  以上代码我们看下编译的map相关调用  go tool compile -S map源码分析.go|egrep &amp;quot;map源码分析.*CALL.*map&amp;quot; 0x0051 00081 (map源码分析.go:3)	CALL	runtime.makemap(SB)	// 显然第3行是初始化 0x0079 00121 (map源码分析.go:4)	CALL	runtime.mapassign_fast64(SB) //第4行是写入 0x00a8 00168 (map源码分析.go:5)	CALL	runtime.mapaccess1_fast64(SB) //第5行是读取 首先看下相关结构体hmap type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go. 	// Make sure this stays in sync with the compiler&amp;#39;s definition.</description>
    </item>
    
  </channel>
</rss>
