<!DOCTYPE html>
<html lang="zh-hans">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#494f5c">
	<meta name="msapplication-TileColor" content="#494f5c">
<meta itemprop="name" content="k8s安装promethues&#43;alertmanager&#43;grafana">
<meta itemprop="description" content="本文主要是针对prometheus 简单部署 , 考虑到测试资源有限,为更精简自定义按照监控,减少多余资源占用, 也同时更了解prometheus的更多细节, 这里没有使用prometheus-operator
 1 K8s上部署原生的prometheus 2 prometheus-operator 方式部署 3 docker-compose快速搭建 Prometheus&#43;Grafana监控系统 4 一套prometheus监控多个k8s集群,详细讲解配置 5 使用prometheus监控traefik、redis、k8s集群各节点、各节点kubelet 6 grafana 监控模板下载     搭建prometheus 创建ns tee ns.yml &lt;&lt;- EOF apiVersion: v1 kind: Namespace metadata: name: monitoring EOF kubectl apply -f ns.yml prometheus-clusterRole.yml tee prometheus-clusterRole.yml &lt;&lt;- EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - &quot;&quot; resources: - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get EOF prometheus-clusterRoleBinding.">
<meta itemprop="datePublished" content="2020-05-14T18:34:47+00:00" />
<meta itemprop="dateModified" content="2020-05-14T18:34:47+00:00" />
<meta itemprop="wordCount" content="2957">



<meta itemprop="keywords" content="k8s,promethues," />
<meta property="og:title" content="k8s安装promethues&#43;alertmanager&#43;grafana" />
<meta property="og:description" content="本文主要是针对prometheus 简单部署 , 考虑到测试资源有限,为更精简自定义按照监控,减少多余资源占用, 也同时更了解prometheus的更多细节, 这里没有使用prometheus-operator
 1 K8s上部署原生的prometheus 2 prometheus-operator 方式部署 3 docker-compose快速搭建 Prometheus&#43;Grafana监控系统 4 一套prometheus监控多个k8s集群,详细讲解配置 5 使用prometheus监控traefik、redis、k8s集群各节点、各节点kubelet 6 grafana 监控模板下载     搭建prometheus 创建ns tee ns.yml &lt;&lt;- EOF apiVersion: v1 kind: Namespace metadata: name: monitoring EOF kubectl apply -f ns.yml prometheus-clusterRole.yml tee prometheus-clusterRole.yml &lt;&lt;- EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - &quot;&quot; resources: - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get EOF prometheus-clusterRoleBinding." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.ngirl.xyz/posts/47-k8s%E5%AE%89%E8%A3%85promethues-alertmanager-grafana/" />
<meta property="article:published_time" content="2020-05-14T18:34:47+00:00" />
<meta property="article:modified_time" content="2020-05-14T18:34:47+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="k8s安装promethues&#43;alertmanager&#43;grafana"/>
<meta name="twitter:description" content="本文主要是针对prometheus 简单部署 , 考虑到测试资源有限,为更精简自定义按照监控,减少多余资源占用, 也同时更了解prometheus的更多细节, 这里没有使用prometheus-operator
 1 K8s上部署原生的prometheus 2 prometheus-operator 方式部署 3 docker-compose快速搭建 Prometheus&#43;Grafana监控系统 4 一套prometheus监控多个k8s集群,详细讲解配置 5 使用prometheus监控traefik、redis、k8s集群各节点、各节点kubelet 6 grafana 监控模板下载     搭建prometheus 创建ns tee ns.yml &lt;&lt;- EOF apiVersion: v1 kind: Namespace metadata: name: monitoring EOF kubectl apply -f ns.yml prometheus-clusterRole.yml tee prometheus-clusterRole.yml &lt;&lt;- EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - &quot;&quot; resources: - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get EOF prometheus-clusterRoleBinding."/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>k8s安装promethues&#43;alertmanager&#43;grafana</title>
	<link rel="stylesheet" href="https://www.ngirl.xyz/css/style.min.d3141168199607bf3a517216ce3c263814eecdbc8fca72a9a88700799a838219.css">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp faster">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://www.ngirl.xyz">zhangzw</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					<a href="https://www.ngirl.xyz/golang/">golang</a>
					<a href="https://www.ngirl.xyz/k8s/">k8s</a>
					<a href="https://www.ngirl.xyz/posts/">文章</a>
				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<button id="toc-btn" class="hdr-btn desktop-only-ib" title="Table of Contents"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-list"><line x1="8" y1="6" x2="21" y2="6"></line><line x1="8" y1="12" x2="21" y2="12"></line><line x1="8" y1="18" x2="21" y2="18"></line><line x1="3" y1="6" x2="3" y2="6"></line><line x1="3" y1="12" x2="3" y2="12"></line><line x1="3" y1="18" x2="3" y2="18"></line></svg></button><span class="hdr-social hide-in-mobile"><a href="https://github.com/zhangzw001" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://www.ngirl.xyz/posts/">文章</a></li>
			<li><a href="https://www.ngirl.xyz/tags/">标签</a></li>
			<li><a href="https://www.ngirl.xyz/about/">关于</a></li>
		</ul>
	</div>



	<main class="site-main section-inner animated fadeIn faster">
		<article class="thin">
			<header class="post-header">
				<div class="post-meta"><span>May 14, 2020</span></div>
				<h1>k8s安装promethues&#43;alertmanager&#43;grafana</h1>
			</header>
			<div class="content">
				<p>本文主要是针对prometheus 简单部署 , 考虑到测试资源有限,为更精简自定义按照监控,减少多余资源占用, 也同时更了解prometheus的更多细节, 这里没有使用prometheus-operator</p>
<!-- more -->
<ul>
<li>1 <a href="https://juejin.im/post/5d4ac8e9f265da03e921b463">K8s上部署原生的prometheus</a></li>
<li>2 <a href="https://www.qikqiak.com/post/first-use-prometheus-operator/">prometheus-operator 方式部署</a></li>
<li>3 <a href="https://blog.51cto.com/msiyuetian/2369130">docker-compose快速搭建 Prometheus+Grafana监控系统</a></li>
<li>4 <a href="https://jeremy-xu.oschina.io/2018/11/%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7%E5%A4%9Ak8s%E9%9B%86%E7%BE%A4/">一套prometheus监控多个k8s集群,详细讲解配置</a></li>
<li>5 <a href="https://blog.csdn.net/ywq935/article/details/80847161">使用prometheus监控traefik、redis、k8s集群各节点、各节点kubelet</a></li>
<li>6 <a href="https://grafana.com/grafana/dashboards?dataSource=prometheus&amp;search=docker&amp;orderBy=name&amp;direction=asc">grafana 监控模板下载</a></li>
</ul>
<hr>
<center>
<img src="//zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
</center>
<h3 id="搭建prometheus">搭建prometheus<a href="#搭建prometheus" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<h4 id="创建ns">创建ns<a href="#创建ns" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee ns.yml &lt;&lt;- EOF
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
EOF


kubectl apply -f ns.yml

</code></pre><h4 id="prometheus-clusterroleyml">prometheus-clusterRole.yml<a href="#prometheus-clusterroleyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-clusterRole.yml &lt;&lt;- EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
  - apiGroups:
      - &quot;&quot;
    resources:
      - nodes/metrics
    verbs:
      - get
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
EOF
</code></pre><h4 id="prometheus-clusterrolebindingyml">prometheus-clusterRoleBinding.yml<a href="#prometheus-clusterrolebindingyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-clusterRoleBinding.yml &lt;&lt;- EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-k8s
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-k8s
subjects:
  - kind: ServiceAccount
    name: prometheus-k8s
    namespace: monitoring
EOF
</code></pre><h4 id="prometheus-serviceaccountyml">prometheus-serviceAccount.yml<a href="#prometheus-serviceaccountyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-serviceAccount.yml &lt;&lt;- EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-k8s
  namespace: monitoring
EOF
</code></pre><h4 id="创建角色">创建角色<a href="#创建角色" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>kubectl apply -f prometheus-clusterRole.yml
kubectl apply -f prometheus-clusterRoleBinding.yml
kubectl apply -f prometheus-serviceAccount.yml

</code></pre><blockquote>
<p>这样我们就创建了一个 ServiceAccount，名为 prometheus-k8s，这个 ServiceAccount 不仅现在可以用来获取 kubelet 的监控指标，后续 Prometheus 也会使用这个 serviceAccount 启动。</p>
</blockquote>
<h4 id="创建完成后会自动在生成一个-secret里面包含了-token">创建完成后，会自动在生成一个 secret，里面包含了 token：<a href="#创建完成后会自动在生成一个-secret里面包含了-token" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>kubectl get secret -n monitoring
NAME                         TYPE                                  DATA      AGE
prometheus-k8s-token-6v9m9   kubernetes.io/service-account-token   3         13s
</code></pre><h4 id="获取token">获取token<a href="#获取token" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>token=$(kubectl get secret prometheus-k8s-token-6v9m9 -n monitoring -o json|jq -r '.data.token'|base64 -d)
token=$(kubectl get secret prometheus-k8s-token-pl2wx -n monitoring -o json|jq -r '.data.token'|base64 -d)
</code></pre><h4 id="通过token查看metrics">通过token查看metrics<a href="#通过token查看metrics" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>curl https://127.0.0.1:10250/metrics/cadvisor -k -H &quot;Authorization: Bearer $token&quot;
</code></pre><blockquote>
<p>kubelet 除了 /metrics/cadvisor 这个 url 之外，还有一个 /metrics，这是它本身的监控指标而非 pod 的</p>
</blockquote>
<h3 id="查看etc指标页面">查看etc指标页面<a href="#查看etc指标页面" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>etcd 的指标页面的 url 也是 /metrics，但是你想要访问它需要提供证书，因为它会验证客户端证书。当然你可以在它的启动参数中通过 &ndash;listen-metrics-urls http://ip:port 让监控指标页使用 http 而非 https，这样就不用提供证书了。
etcd 虽然部署在容器中，但是由于使用了 hostNetwork，所以我们可以通过直接访问 master 的 2379 端口访问它。默认它会采用了 https，因此我们需要提供它的 peer 证书。如果 k8s 是使用 kubeadm 安装的，etcd 的证书在 /etc/kubernetes/pki/etcd/ 目录下。
因此访问 etcd 的命令为：
curl https://127.0.0.1:2379/metrics &ndash;cacert /etc/kubernetes/pki/etcd/ca.crt &ndash;cert /etc/kubernetes/pki/etcd/healthcheck-client.crt &ndash;key /etc/kubernetes/pki/etcd/healthcheck-client.key
复制代码后面我们需要将这三个文件挂载到 Prometheus 容器中，以便它能收集 etcd 监控数据。</p>
<p>如果你并非容器部署的etcd,请使用你的etcd端口访问</p>
<hr>
<center>
<img src="//zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
</center>
<h3 id="安装-prometheus">安装 Prometheus<a href="#安装-prometheus" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<h4 id="我们先创建两个-configmap一个是-prometheus-的配置文件另一个是告警的规则文件">我们先创建两个 configmap，一个是 Prometheus 的配置文件，另一个是告警的规则文件<a href="#我们先创建两个-configmap一个是-prometheus-的配置文件另一个是告警的规则文件" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-configmap.yml &lt;&lt;- EOF
apiVersion: v1
data:
  prometheus.yml: |
    global:
      evaluation_interval: 30s
      scrape_interval: 30s
      external_labels:
        prometheus: monitoring/k8s
    rule_files:
    - /etc/prometheus/rules/*.yml
    scrape_configs:
    - job_name: prometheus
      honor_labels: false
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - monitoring
      scrape_interval: 30s
      relabel_configs:
      - action: keep
        source_labels:
        - __meta_kubernetes_service_label_prometheus
        regex: k8s
      - source_labels:
        - __meta_kubernetes_endpoint_address_target_kind
        - __meta_kubernetes_endpoint_address_target_name
        separator: ;
        regex: Pod;(.*)
        replacement: ${1}
        target_label: pod
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: job
        replacement: ${1}
      - target_label: endpoint
        replacement: web
kind: ConfigMap
metadata:
  name: prometheus
  namespace: monitoring
EOF
</code></pre><blockquote>
<p>首先看这段配置</p>
</blockquote>
<pre><code>kubernetes_sd_configs:
- role: endpoints
  namespaces:
    names:
    - monitoring
scrape_interval: 30s
</code></pre><p>这段配置使用的是 endpoint 的方式对 Prometheus 本身进行自动发现，你可以有疑问了，为什么不直接对自身的 127.0.0.1:9090 进行采集呢？因为考虑到 Prometheus 可能会有多台，这样即使有多台，它们也都在一个 job 下面。</p>
<p>kubernetes_sd_configs 配置可以自动发现 k8s 中 node、service、pod、endpoint、ingress，并为其添加监控, 详见:  <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">kubernetes_sd_configs官方文档</a></p>
<blockquote>
<p>再看下面这段配置</p>
</blockquote>
<pre><code>- action: keep
  source_labels:
    - __meta_kubernetes_service_label_prometheus
  regex: k8s
</code></pre><p>表示并非所有的endpoint 都会被抓取,  这里只抓取label 是 prometheus=k8s的标签, 所以只会监控prometheus的endpoint</p>
<p>默认不指定url 就是/metrics</p>
<blockquote>
<p>接着看这段配置</p>
</blockquote>
<pre><code>- source_labels:
    - __meta_kubernetes_endpoint_address_target_kind
    - __meta_kubernetes_endpoint_address_target_name
  separator: ;
  regex: Pod;(.*)
  replacement: ${1}
  target_label: pod

</code></pre><p>如果 __meta_kubernetes_endpoint_address_target_kind 的值为 Pod，__meta_kubernetes_endpoint_address_target_name 的值为 prometheus-0，在它们之间加上一个 ; 之后，它们合起来就是 Pod;prometheus-0。使用正则表达式 Pod;(.*) 对其进行匹配，那么 ${1} 就是取第一个分组，它值就是 prometheus-0，最后将这个值交给 pod 这个标签。
因此这一段配置就是为所有采集到的监控指标增加一个 pod=prometheus-0 的标签。</p>
<blockquote>
<p>以上配置其实可以去掉, 这里prometheus=k8s的匹配条件以及唯一,这段kind和name配置去掉影响不大</p>
</blockquote>
<h4 id="创建configmap">创建configmap<a href="#创建configmap" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>kubectl apply -f prometheus-configmap.yml
</code></pre><h4 id="prometheus-规则文件-后面会更新">Prometheus 规则文件 (后面会更新)<a href="#prometheus-规则文件-后面会更新" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-config-rulefiles.yml &lt;&lt;- EOF
apiVersion: v1
data:
  k8s.yml: &quot;&quot;
kind: ConfigMap
metadata:
  name: prometheus-rulefiles
  namespace: monitoring
EOF


kubectl apply -f prometheus-config-rulefiles.yml

</code></pre><h3 id="role-和-rolebinding">role 和 roleBinding<a href="#role-和-rolebinding" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>因为 Prometheus 会使用之前创建的 sa（serviceAccount）prometheus-k8s 运行，那么光现在 prometheus-k8s 这个 sa 的权限是没有办法查看 service 以及 endpoint 的。
我们使用 kubernetes_sd_config 主要会使用 endpoint 进行发现，因此 prometheus-k8s 必须具备更多的权限。
我们需要创建更多的 role，并通过 roleBinding 将这些权限绑定到 prometheus-k8s 这个 sa 上，之所以不使用 clusterRole 是为了权限最小化。
这里会创建 prometheus-roleConfig.yml、prometheus-roleBindingConfig.yml、prometheus-roleSpecificNamespaces.yml、prometheus-roleBindingSpecificNamespaces.yml 这四个文件，它们的内容如下。</p>
<pre><code>tee prometheus-roleConfig.yml &lt;&lt;- EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: prometheus-k8s-config
  namespace: monitoring
rules:
  - apiGroups:
      - &quot;&quot;
    resources:
      - configmaps
    verbs:
      - get
EOF


tee prometheus-roleBindingConfig.yml &lt;&lt;- EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: prometheus-k8s-config
  namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: prometheus-k8s-config
subjects:
  - kind: ServiceAccount
    name: prometheus-k8s
    namespace: monitoring
EOF

tee prometheus-roleSpecificNamespaces.yml &lt;&lt;- EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleList
items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: prometheus-k8s
      namespace: default
    rules:
      - apiGroups:
          - &quot;&quot;
        resources:
          - services
          - endpoints
          - pods
        verbs:
          - get
          - list
          - watch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: prometheus-k8s
      namespace: kube-system
    rules:
      - apiGroups:
          - &quot;&quot;
        resources:
          - services
          - endpoints
          - pods
        verbs:
          - get
          - list
          - watch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: prometheus-k8s
      namespace: monitoring
    rules:
      - apiGroups:
          - &quot;&quot;
        resources:
          - services
          - endpoints
          - pods
        verbs:
          - get
          - list
          - watch
EOF

tee prometheus-roleBindingSpecificNamespaces.yml &lt;&lt;- EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBindingList
items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: prometheus-k8s
      namespace: default
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: prometheus-k8s
    subjects:
      - kind: ServiceAccount
        name: prometheus-k8s
        namespace: monitoring
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: prometheus-k8s
      namespace: kube-system
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: prometheus-k8s
    subjects:
      - kind: ServiceAccount
        name: prometheus-k8s
        namespace: monitoring
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: prometheus-k8s
      namespace: monitoring
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: prometheus-k8s
    subjects:
      - kind: ServiceAccount
        name: prometheus-k8s
        namespace: monitoring
EOF

</code></pre><p>上面的权限中，config 是用来读 configmap 的，后面的就是 Prometheus 用来进行 k8s 发现时必须要的权限了，最后使用 rulebinding 将这些所有的权限都绑定到 prometheus-k8s 这个 sa 上。</p>
<pre><code>kubectl delete -f prometheus-roleBindingConfig.yml
kubectl delete -f prometheus-roleBindingSpecificNamespaces.yml
kubectl delete -f prometheus-roleConfig.yml
kubectl delete -f prometheus-roleSpecificNamespaces.yml


kubectl apply -f prometheus-roleBindingConfig.yml
kubectl apply -f prometheus-roleBindingSpecificNamespaces.yml
kubectl apply -f prometheus-roleConfig.yml
kubectl apply -f prometheus-roleSpecificNamespaces.yml

</code></pre><h4 id="手动创建pv-我这里没用上-用的是storageclass">手动创建pv (我这里没用上, 用的是storageclass)<a href="#手动创建pv-我这里没用上-用的是storageclass" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-pv.yml &lt;&lt;- EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: prometheus
  labels:
    name: prometheus
spec:
  nfs:
    path: /disk/k8s-nfs-data/k8s-db-t/prometheus
    server: 172.16.xx.xx
  accessModes: [&quot;ReadWriteMany&quot;, &quot;ReadWriteOnce&quot;]
  capacity:
    storage: 50Gi
EOF

kubectl apply -f prometheus-pv.yml

</code></pre><h4 id="创建-service">创建 service<a href="#创建-service" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-service.yml &lt;&lt;- EOF
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
  labels:
    prometheus: k8s
spec:
  clusterIP: None
  ports:
    - name: web
      port: 9090
      protocol: TCP
      targetPort: web
  selector:
    app: prometheus
  type: ClusterIP

EOF

kubectl apply -f prometheus-service.yml
</code></pre><p>这里service定义了 app=prometheus 这样的标签选择器，因此 Prometheus 容器StatefulSet的时候必须存在这个标签。</p>
<h4 id="部署-prometheus">部署 Prometheus<a href="#部署-prometheus" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-statefulset.yml &lt;&lt;- EOF
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: prometheus
    prometheus: k8s
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  volumeClaimTemplates:
  - metadata:
      name: prometheus-data
      annotations:
        volume.beta.kubernetes.io/storage-class: &quot;nfs-retain&quot; # 这里配置 上面创建的 storageclass 的名称
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 20Gi
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: prometheus
      prometheus: k8s
  serviceName: prometheus
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: prometheus
        prometheus: k8s
    spec:
      serviceAccount: prometheus-k8s
      containers:
        - args:
            - --web.console.templates=/etc/prometheus/consoles
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --config.file=/etc/prometheus/config/prometheus.yml
            - --storage.tsdb.path=/prometheus
            - --web.enable-admin-api
            - --storage.tsdb.retention.time=20d
            - --web.enable-lifecycle
            - --storage.tsdb.no-lockfile
            - --web.external-url=http://promethes-dev.k1s.club/
            - --web.route-prefix=/
          image: prom/prometheus:v2.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
            - containerPort: 9090
              name: web
              protocol: TCP
          readinessProbe:
            failureThreshold: 120
            httpGet:
              path: /-/ready
              port: web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              memory: 400Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - mountPath: /etc/prometheus/config
              name: config
              readOnly: true
            - mountPath: /prometheus
              name: prometheus-data
              #subPath: prometheus-db
            - mountPath: /etc/prometheus/rules/
              name: prometheus-rulefiles
            - mountPath: /etc/prometheus/secrets/etcd-client-cert
              name: secret-etcd-client-cert
              readOnly: true
      volumes:
        - name: config
          configMap:
            defaultMode: 420
            name: prometheus
        - name: prometheus-rulefiles
          configMap:
            defaultMode: 420
            name: prometheus-rulefiles
        - name: secret-etcd-client-cert
          secret:
            defaultMode: 420
            secretName: etcd-client-cert

EOF

kubectl apply -f prometheus-statefulset.yml
</code></pre><blockquote>
<p>注意下这里如果你并非容器安装的etcd, 则mount的secret-etcd-client-cert可能不存在, 请自行挂载正确的目录, 如果etcd是http访问, 则不需要证书挂载</p>
</blockquote>
<p>基础的 statfulset 相关的知识我就不多提了，说几个重点吧：</p>
<p>1 &ndash;storage.tsdb.retention.time=20d 这个启动选项表示 Prometheus 所收集的监控数据只保留 20 天，这个值最好不要太大。如果历史数据保存很久，建议写到持久存储中，比如 VictoriaMetrics、thanos、influxdb、opentsdb 等；
2 &ndash;web.enable-admin-api 这个启动选项表示启动管理员 api，你可以通过 api 对监控数据进行删除等；
3 serviceAccount 它的值必须是 prometheus-k8s，不然前面的赋权操作都白干了；
4 pod 必须存在 app: prometheus 这个标签，不然无法被前面创建的 service 选择到；
5 挂载了两个 configmap、一个 secret 还有一个存储卷。(我这里是采用storageclass, 当然你也可以用上面的手动创建pv)</p>
<h4 id="创建一个ingress">创建一个ingress<a href="#创建一个ingress" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee prometheus-ingress.yml &lt;&lt;- EOF
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: prometheus
  namespace: monitoring
spec:
  rules:
    - host: prometheus-dev.k1s.club
      http:
        paths:
          - path: /
            backend:
              serviceName: prometheus
              servicePort: 9090

EOF

kubectl apply -f prometheus-ingress.yml
</code></pre><p>至此, 我们就监控了prometheus本身
<img src="assets/prometheus-01-01.png" alt=""></p>
<hr>
<h3 id="问题1-k8s110报错rbac权限错误">问题1 k8s1.10报错rbac权限错误<a href="#问题1-k8s110报错rbac权限错误" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<blockquote>
<p>level=error ts=2020-05-11T10:00:03.091Z caller=klog.go:94 component=k8s_client_runtime func=ErrorDepth msg=&quot;/app/discovery/kubernetes/kubernetes.go:335: Failed to list *v1.Node: nodes is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list nodes at the cluster scope&quot;</p>
</blockquote>
<p>按照以下rbac增加了权限,则正常</p>
<pre><code>prometheus-rbac.yml
apiVersion: v1
kind: ServiceAccount
metadata:
 name: prometheus-k8s
 namespace: monitoring
 labels:
   kubernetes.io/cluster-service: &quot;true&quot;
   addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: prometheus-k8s
 labels:
   kubernetes.io/cluster-service: &quot;true&quot;
   addonmanager.kubernetes.io/mode: Reconcile
rules:
 - apiGroups:
     - &quot;&quot;
   resources:
     - nodes
     - nodes/metrics
     - services
     - endpoints
     - pods
   verbs:
     - get
     - list
     - watch
 - apiGroups:
     - &quot;&quot;
   resources:
     - configmaps
   verbs:
     - get
 - nonResourceURLs:
     - &quot;/metrics&quot;
   verbs:
     - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 name: prometheus-k8s
 labels:
   kubernetes.io/cluster-service: &quot;true&quot;
   addonmanager.kubernetes.io/mode: Reconcile
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: ClusterRole
 name: prometheus-k8s
subjects:
- kind: ServiceAccount
 name: prometheus-k8s
 namespace: monitoring

</code></pre><center>
<img src="//zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
</center>
<h3 id="监控etcd">监控etcd<a href="#监控etcd" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<h4 id="如果是容器安装的etcd集群">如果是容器安装的etcd集群<a href="#如果是容器安装的etcd集群" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>tee kube-etcd-service.yml &lt;&lt;- EOF
apiVersion: v1
kind: Service
metadata:
  name: kube-etcd
  labels:
    k8s-app: kube-etcd
  namespace: kube-system
spec:
  clusterIP: None
  ports:
    - name: http-metrics
      port: 2379
      protocol: TCP
      targetPort: 2379
  selector:
    component: etcd
  type: ClusterIP

EOF
</code></pre><ul>
<li>由于 etcd 处于 kube-system 名称空间，所以这里的 namespace 也应该是 kube-system；</li>
<li>因为 etcd pod 本身会存在 component=etcd 这个标签，所以这里的选择器使用的就是这个。</li>
</ul>
<pre><code>kubectl apply -f kube-etcd-service.yml
kubectl -n kube-system get endpoints kube-etcd

</code></pre><p>现在通过这个 endpoint 就能够访问到后面三台 etcd，现在只需要在 Prometheus 中添加对应的配置即可，配置内容如下。</p>
<pre><code>- job_name: kube-etcd
  honor_labels: false
  kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
          - kube-system
  scheme: https
  tls_config:
    insecure_skip_verify: false
    ca_file: /etc/prometheus/secrets/etcd-client-cert/ca.crt
    cert_file: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.crt
    key_file: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.key
  relabel_configs:
    - action: keep
      source_labels:
        - __meta_kubernetes_service_label_k8s_app
      regex: kube-etcd
    - source_labels:
        - __meta_kubernetes_namespace
      target_label: namespace
    - source_labels:
        - __meta_kubernetes_service_name
      target_label: service
    - source_labels:
        - __meta_kubernetes_pod_name
      target_label: pod
    - target_label: endpoint
      replacement: http-metrics
  metric_relabel_configs:
    - action: drop
      regex: (etcd_debugging|etcd_disk|etcd_request|etcd_server|grpc_server).*
      source_labels:
        - __name__

</code></pre><h4 id="我这里有一个环境是http访问的-则直接通过http17216xxxx2379metrics-监控即可">我这里有一个环境是http访问的, 则直接通过http://172.16.xx.xx:2379/metrics 监控即可<a href="#我这里有一个环境是http访问的-则直接通过http17216xxxx2379metrics-监控即可" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>- job_name: 'etcd'
  scrape_interval: 60s
  static_configs:
    - targets: ['172.16.xx.xx:2379']
  metric_relabel_configs:
  - action: drop
    regex: (etcd_debugging|etcd_disk|etcd_request|etcd_server|grpc_server).*
    source_labels:
      - __name__
</code></pre><blockquote>
<p>然后重新加载configmap</p>
</blockquote>
<pre><code>kubectl apply -f prometheus-configmap.yml

# 该配置不用重启prometheus即可重新加载配置
curl -XPOST prometheus-dev.k1s.club/-/reload
</code></pre><h3 id="监控-apiserver">监控 apiserver<a href="#监控-apiserver" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>apiserver 的监控方式更简单，因为它的 service 已经自动创建了。但你需要注意的是，它的 service 创建在 default 名称空间，名为 kubernetes。</p>
<pre><code>- job_name: kube-apiserver
  honor_labels: false
  kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
          - default
  scrape_interval: 30s
  scheme: https
  tls_config:
    insecure_skip_verify: false
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
    - action: keep
      source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
      separator: ;
      regex: default;kubernetes;https
  metric_relabel_configs:
    - source_labels:
        - __name__
      action: drop
      regex: (apiserver_storage_data_key_generation_latencies_microseconds_bucket|apiserver_admission_controller_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_summary|apiserver_request_latencies_bucket|apiserver_request_latencies_summary|apiserver_storage_data_key_generation_latencies_microseconds_bucket|rest_client_request_latency_seconds_bucket)
</code></pre><h3 id="监控-pod">监控 pod<a href="#监控-pod" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>pod 的监控指标是 kubelet 提供的，前面也已经使用 curl 命令看到了，因此这里也是直接干。
prometheus-operator 使用的同样是 endpoints 发现的方式，但是 kubelet 是操作系统的进程，并不是 pod，因此通过创建 service 的方式是不可能创建对应的 endpoint 的，也不知道它为啥可以做到。
为了更通用，我们这里是通过 node 发现的方式进行的。使用 node 发现，你无法指定端口，prometheus 会自动访问发现 node 的 10250 端口。</p>
<pre><code>- job_name: pods
  honor_labels: true
  kubernetes_sd_configs:
  - role: node
  scrape_interval: 30s
  metrics_path: /metrics/cadvisor
  scheme: https
  tls_config:
    insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token


</code></pre><blockquote>
<p>k8s 的其他组件我就不继续监控了，包括 kubelet、controller manager、coredns 等，它们监控的手段和之前的几个组件都差不多</p>
</blockquote>
<h3 id="安装-kube-state-metrics">安装 kube-state-metrics<a href="#安装-kube-state-metrics" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<blockquote>
<p>常见应用
使用kube-state-metrics后的常用场景有：</p>
</blockquote>
<p>存在执行失败的Job:</p>
<ul>
<li>kube_job_status_failed{job=&ldquo;kubernetes-service-endpoints&rdquo;,k8s_app=&ldquo;kube-state-metrics&rdquo;}==1</li>
<li>集群节点状态错误: kube_node_status_condition{condition=&ldquo;Ready&rdquo;,status!=&ldquo;true&rdquo;}==1</li>
<li>集群中存在启动失败的Pod：kube_pod_status_phase{phase=~&ldquo;Failed|Unknown&rdquo;}==1</li>
<li>最近30分钟内有Pod容器重启: changes(kube_pod_container_status_restarts[30m])&gt;0
配合报警可以更好地监控集群的运行</li>
</ul>
<h4 id="rbac-权限">RBAC 权限<a href="#rbac-权限" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<blockquote>
<p>因为它要访问集群内的所有资源，才能将它们的信息提供出去，因此部署它之前，先为它创建一些权限。这些权限都会绑定到一个 serviceAccount 上，然后我们用这个 sa 运行 kube-state-metrics 就行</p>
</blockquote>
<h4 id="kube-state-metrics-clusterroleyml">kube-state-metrics-clusterRole.yml<a href="#kube-state-metrics-clusterroleyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
  - apiGroups:
      - &quot;&quot;
    resources:
      - configmaps
      - secrets
      - nodes
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs:
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
      - ingresses
    verbs:
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - list
      - watch
  - apiGroups:
      - authentication.k8s.io
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups:
      - authorization.k8s.io
    resources:
      - subjectaccessreviews
    verbs:
      - create
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - list
      - watch
  - apiGroups:
      - certificates.k8s.io
    resources:
      - certificatesigningrequests
    verbs:
      - list
      - watch

</code></pre><h4 id="kube-state-metrics-clusterrolebindingyml">kube-state-metrics-clusterRoleBinding.yml<a href="#kube-state-metrics-clusterrolebindingyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: monitoring

</code></pre><h4 id="kube-state-metrics-roleyml">kube-state-metrics-role.yml<a href="#kube-state-metrics-roleyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kube-state-metrics
  namespace: monitoring
rules:
  - apiGroups:
      - &quot;&quot;
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - extensions
    resourceNames:
      - kube-state-metrics
    resources:
      - deployments
    verbs:
      - get
      - update
  - apiGroups:
      - apps
    resourceNames:
      - kube-state-metrics
    resources:
      - deployments
    verbs:
      - get
      - update

</code></pre><h4 id="kube-state-metrics-rolebindingyml">kube-state-metrics-roleBinding.yml<a href="#kube-state-metrics-rolebindingyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kube-state-metrics
  namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: kube-state-metrics

</code></pre><h4 id="kube-state-metrics-serviceaccountyml">kube-state-metrics-serviceAccount.yml<a href="#kube-state-metrics-serviceaccountyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: monitoring

</code></pre><pre><code>kubectl apply -f kube-state-metrics-clusterRole.yml
kubectl apply -f kube-state-metrics-clusterRoleBinding.yml
kubectl apply -f kube-state-metrics-role.yml
kubectl apply -f kube-state-metrics-roleBinding.yml
kubectl apply -f kube-state-metrics-serviceAccount.yml
</code></pre><h3 id="deployment-和-service">deployment 和 service<a href="#deployment-和-service" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<blockquote>
<p>kube-state-metrics 会提供两个指标页面，一个是暴露集群内资源的，另一个是它自身的，它自身的可以选择性的关注</p>
</blockquote>
<h4 id="kube-state-metrics-deploymentyml">kube-state-metrics-deployment.yml<a href="#kube-state-metrics-deploymentyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kube-state-metrics
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
        - args:
            - --port=10000
            - --telemetry-port=10001
          image: quay.io/coreos/kube-state-metrics:v1.6.0
          name: kube-state-metrics
          resources:
            limits:
              cpu: 100m
              memory: 150Mi
            requests:
              cpu: 100m
              memory: 150Mi
        - command:
            - /pod_nanny
            - --container=kube-state-metrics
            - --cpu=100m
            - --extra-cpu=2m
            - --memory=150Mi
            - --extra-memory=30Mi
            - --threshold=5
            - --deployment=kube-state-metrics
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.4
          name: addon-resizer
          resources:
            limits:
              cpu: 50m
              memory: 30Mi
            requests:
              cpu: 10m
              memory: 30Mi
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: kube-state-metrics
</code></pre><p>指定了两个启动参数，也就是两个端口，其中 10000 是暴露集群资源指标的端口，10001 就是它自身了。除了 kube-state-metrics 之外，还启动了 addon-resizer 这个容器</p>
<h4 id="最后是-service-文件-kube-state-metrics-serviceyml">最后是 service 文件 kube-state-metrics-service.yml<a href="#最后是-service-文件-kube-state-metrics-serviceyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: kube-state-metrics
  name: kube-state-metrics
  namespace: monitoring
spec:
  clusterIP: None
  ports:
    - name: http-main
      port: 10000
      targetPort: 10000
    - name: http-self
      port: 10001
      targetPort: 10001
  selector:
    app: kube-state-metrics

</code></pre><pre><code>docker pull registry.cn-beijing.aliyuncs.com/minminmsn/addon-resizer:1.8.4
docker tag registry.cn-beijing.aliyuncs.com/minminmsn/addon-resizer:1.8.4 k8s.gcr.io/addon-resizer:1.8.4
</code></pre><pre><code>kubectl apply -f kube-state-metrics-deployment.yml
kubectl apply -f kube-state-metrics-service.yml
</code></pre><p>两个端口都暴露出来，你可以都收集或者只收集 10000 端口。如果只收集 10000，你可以只暴露一个端口，也可以两个都暴露，然后在 Prometheus 配置中过滤掉一个端口即可。</p>
<h4 id="收集监控数据">收集监控数据<a href="#收集监控数据" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<p>将上面所有的文件都 apply 之后，就可以直接配置 Prometheus 进行收集了。在此之前，你可以使用 curl 命令访问它的指标页面，看看里面都有啥：</p>
<pre><code>kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools -n monitoring

# 首先看一下健康情况
curl kube-state-metrics:10000/healthz
# 在看看指标 (这里有非常多指标)
curl kube-state-metrics:10000/metrics
</code></pre><h4 id="修改下prometheus-configmapyml-文件">修改下prometheus-configmap.yml 文件<a href="#修改下prometheus-configmapyml-文件" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>- job_name: kube-state-metrics
  honor_labels: true
  kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
          - monitoring
  scrape_interval: 30s
  scrape_timeout: 30s
  tls_config:
    insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
    - action: keep
      source_labels:
        - __meta_kubernetes_service_label_k8s_app
      regex: kube-state-metrics
    - action: keep
      source_labels:
        - __meta_kubernetes_endpoint_port_name
      regex: http-main
  metric_relabel_configs:
    - source_labels:
        - __name__
      regex: (kube_daemonset_status_number_ready|kube_daemonset_status_number_unavailable|kube_deployment_status_replicas_unavailable|kube_deployment_spec_paused|kube_deployment_spec_strategy_rollingupdate_max_surge|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_endpoint_address_available|kube_endpoint_address_not_ready|kube_node_info|kube_node_spec_unschedulable|kube_node_status_condition|kube_node_status_capacity|kube_node_status_capacity|kube_node_status_allocatable|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_status_phase|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolume_status_phase|kube_persistentvolume_info|kube_persistentvolume_capacity_bytes|kube_pod_info|kube_pod_status_phase|kube_pod_status_ready|kube_pod_container_info|kube_pod_container_status_waiting|kube_pod_container_status_waiting_reason|kube_pod_container_status_running|kube_pod_container_status_terminated_reason|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|kube_service_info|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready)
      action: keep

</code></pre><blockquote>
<p>这里是只关注匹配到的指标, 其他指标忽略(白名单)</p>
</blockquote>
<pre><code>kubectl apply -f prometheus-configmap.yml
# 该配置不用重启prometheus即可重新加载配置
curl -XPOST prometheus-dev.k1s.club/-/reload
</code></pre><h3 id="配置一个grafana">配置一个grafana<a href="#配置一个grafana" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<h4 id="首先查看一下你是否配置了storageclass">首先查看一下你是否配置了storageclass<a href="#首先查看一下你是否配置了storageclass" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>kubectl get storageclass
NAME            PROVISIONER       AGE
nfs (default)   nfs.com/nfs-ssd   248d
nfs-retain      nfs.com/nfs-ssd   248d
</code></pre><h4 id="k8s-statefulset_grafanayml">k8s-StatefulSet_grafana.yml<a href="#k8s-statefulset_grafanayml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: grafana-dev
  namespace: monitoring
spec:
  serviceName: &quot;grafana-dev&quot;
  updateStrategy:
    type: RollingUpdate
  replicas: 1
  volumeClaimTemplates:
  - metadata:
      name: grafana-data
      annotations:
        volume.beta.kubernetes.io/storage-class: &quot;nfs-retain&quot; # 这里配置 上面创建的 storageclass 的名称
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 5Gi
  template:
    metadata:
      labels:
        app: grafana-dev
    spec:
      containers:
      - name: grafana-dev
        image: grafana/grafana
        ports:
        - containerPort: 3000
          name: grafana-port
        resources:
          requests:
            cpu: &quot;50m&quot;
          limits:
            cpu: &quot;512m&quot;
        volumeMounts:
        - mountPath: /var/lib/grafana
          name: grafana-data
---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: grafana-dev
  name: grafana-dev-service
  namespace: monitoring
spec:
  clusterIP: None
  type: ClusterIP
  ports:
    - port: 3000
      name: grafana-port
      protocol: TCP
      targetPort: 3000
  selector:
    app: grafana-dev
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: grafana-dev
  namespace: monitoring
spec:
  rules:
    - host: grafana-dev.k1s.club
      http:
        paths:
          - path: /
            backend:
              serviceName: grafana-dev-service
              servicePort: 3000
</code></pre><h4 id="通过-k8s-grafana-dashboard模板httpsgrafanacomgrafanadashboards8588--监控效果如下">通过 <a href="https://grafana.com/grafana/dashboards/8588">k8s grafana dashboard模板</a>  监控效果如下<a href="#通过-k8s-grafana-dashboard模板httpsgrafanacomgrafanadashboards8588--监控效果如下" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<p><img src="assets/prometheus-02-01.png" alt=""></p>
<h4 id="通过以上的经验-如果我希望通过a集群的prometheus-来监控-b集群-所以一些指标的可通过token访问b-api">通过以上的经验, 如果我希望通过A集群的Prometheus 来监控 B集群, 所以一些指标的可通过token访问B api<a href="#通过以上的经验-如果我希望通过a集群的prometheus-来监控-b集群-所以一些指标的可通过token访问b-api" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code># 查看B集群的api
kubectl cluster-info

kubectl create serviceaccount --namespace=monitoring prometheus-dev
kubectl create clusterrolebinding prometheus-k8s --clusterrole=cluster-admin --serviceaccount=monitoring:prometheus-dev

# 创建一个叫admin的serviceaccount
kubectl -n kube-system create serviceaccount admin
# 给这个admin的serviceaccount绑上cluser-admin的clusterrole
kubectl -n kube-system create clusterrolebinding sa-cluster-admin --serviceaccount kube-system:admin --clusterrole cluser-admin
# 查询admin的secret
kubectl -n kube-system get serviceaccounts admin -o json|jq -r '.secrets[0].name'
admin-token-h4zz4

# 查看token
kubectl get secret admin-token-h4zz4 -n kube-system -o json|jq -r &quot;.data.token&quot;|base64 -d &gt;&gt;/etc/kubernetes/pki/admin-token-9dg92
</code></pre><h4 id="然后在a集群的prometheus配置如下">然后在A集群的prometheus配置如下<a href="#然后在a集群的prometheus配置如下" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>- job_name: k8s_B-kube-state-metrics
   honor_labels: true
   kubernetes_sd_configs:
     - role: endpoints
       api_server: https://172.16.xx.xx:6443
       tls_config:
         insecure_skip_verify: true
       bearer_token: 'eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwcm9tZXRoZXVzLWRldi10b2tlbi1zZ3pidiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJwcm9tZXRoZXVzLWRldiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjliODI5NTYwLTkzNGYtMTFlYS1hMGE3LTE4NjZkYWY0NjY3NCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpwcm9tZXRoZXVzLWRldiJ9.xa8gDA0lwEi_NDGzCL3JSsZUsUD7gKiF0sfFofykyAlEYcjnPmPaksduHWzRKaUJhkvgAJN5Jl3pt8-wplQUJggGAaPVdqJVTYISi4QkPcLkDInoYm8p3OeRgvNpQJJ0VID8zp0-RBWoYe8bAh-7qT6JInt308AA-21vzDKDHtj3aa8Re1nuBxB7f0omNKcAhW0R04p59jshg95HRSBXbVQe7gX6NBjgaOWqj5i0MkKL6k2hdFKdQYgjhQjRAZmXL6F0Qx197y3HAw4zmrUPG-13RcXk38X5F4K8CWtHdOvrqUZxolaWBWin8n73Sr87KyFcEu8YA2oJbzvCKzy9Kg'
       namespaces:
         names:
           - monitoring
   scrape_interval: 30s
   scrape_timeout: 30s
   tls_config:
     insecure_skip_verify: true
   bearer_token: 'eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwcm9tZXRoZXVzLWRldi10b2tlbi1zZ3pidiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJwcm9tZXRoZXVzLWRldiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjliODI5NTYwLTkzNGYtMTFlYS1hMGE3LTE4NjZkYWY0NjY3NCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpwcm9tZXRoZXVzLWRldiJ9.xa8gDA0lwEi_NDGzCL3JSsZUsUD7gKiF0sfFofykyAlEYcjnPmPaksduHWzRKaUJhkvgAJN5Jl3pt8-wplQUJggGAaPVdqJVTYISi4QkPcLkDInoYm8p3OeRgvNpQJJ0VID8zp0-RBWoYe8bAh-7qT6JInt308AA-21vzDKDHtj3aa8Re1nuBxB7f0omNKcAhW0R04p59jshg95HRSBXbVQe7gX6NBjgaOWqj5i0MkKL6k2hdFKdQYgjhQjRAZmXL6F0Qx197y3HAw4zmrUPG-13RcXk38X5F4K8CWtHdOvrqUZxolaWBWin8n73Sr87KyFcEu8YA2oJbzvCKzy9Kg'
   relabel_configs:
     - action: keep
       source_labels:
         - __meta_kubernetes_service_label_k8s_app
       regex: kube-state-metrics
     - action: keep
       source_labels:
         - __meta_kubernetes_endpoint_port_name
       regex: http-main
   metric_relabel_configs:
     - source_labels:
         - __name__
       regex: (kube_daemonset_status_number_ready|kube_daemonset_status_number_unavailable|kube_deployment_status_replicas_unavailable|kube_deployment_spec_paused|kube_deployment_spec_strategy_rollingupdate_max_surge|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_endpoint_address_available|kube_endpoint_address_not_ready|kube_node_info|kube_node_spec_unschedulable|kube_node_status_condition|kube_node_status_capacity|kube_node_status_capacity|kube_node_status_allocatable|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_status_phase|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolume_status_phase|kube_persistentvolume_info|kube_persistentvolume_capacity_bytes|kube_pod_info|kube_pod_status_phase|kube_pod_status_ready|kube_pod_container_info|kube_pod_container_status_waiting|kube_pod_container_status_waiting_reason|kube_pod_container_status_running|kube_pod_container_status_terminated_reason|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|kube_service_info|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_deployment_status_replicas_available|kube_deployment_status_replicas|kube_node_status_allocatable_memory_bytes|kube_deployment_status_replicas|kube_statefulset_replicas|kube_daemonset_status_desired_number_scheduled|kube_statefulset_status_replicas|changes|kube_job_status_failed)
       action: keep
</code></pre><center>
<img src="//zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/>
</center>
<h3 id="部署alertmanager">部署alertmanager<a href="#部署alertmanager" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<h4 id="alertmanager-configmapyml">alertmanager-configmap.yml<a href="#alertmanager-configmapyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  config.yml: |-
    global:
      # 在没有报警的情况下声明为已解决的时间
      resolve_timeout: 5m
      # 配置邮件发送信息
      smtp_smarthost: 'smtp.163.com:25'
      smtp_from: 'xxx@163.com'
      smtp_auth_username: 'xxx@163.com'
      smtp_auth_password: 'xxx'
      smtp_require_tls: false
      # 所有报警信息进入后的根路由，用来设置报警的分发策略
    route:
      # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面
      group_by: ['alertname', 'cluster']
      # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。
      group_wait: 30s
      # 当第一个报警发送后，等待'group_interval'时间来发送新的一组报警信息。
      group_interval: 1m
      # 如果一个报警信息已经发送成功了，等待'repeat_interval'时间来重新发送他们
      repeat_interval: 2h
      # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器
      receiver: default
    receivers:
    - name: 'default'
      email_configs:
      - to: 'zhangzw@xxx.com'
        send_resolved: true
</code></pre><h4 id="alertmanager-deploymentyml">alertmanager-deployment.yml<a href="#alertmanager-deploymentyml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager
        args:
          - &quot;--config.file=/etc/alertmanager/config.yml&quot;
          - &quot;--storage.path=/alertmanager&quot;
        ports:
        - name: alertmanager
          containerPort: 9093
        volumeMounts:
        - name: alertmanager-cm
          mountPath: /etc/alertmanager
      volumes:
      - name: alertmanager-cm
        configMap:
          name: alertmanager-config
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
    - port: 80
      targetPort: 9093
</code></pre><h4 id="最后配置prometheus的rule文件-prometheus-config-rulefilesyml-修改">最后配置prometheus的rule文件 prometheus-config-rulefiles.yml (修改)<a href="#最后配置prometheus的rule文件-prometheus-config-rulefilesyml-修改" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<blockquote>
<p>这里仅针对我这里prometheus部署的时候是通过configmap挂载的rule文件</p>
</blockquote>
<pre><code>
kind: ConfigMap
metadata:
  name: prometheus-rulefiles
  namespace: monitoring
apiVersion: v1
data:
  k8s.yml: |
    groups:
    - name: cpu-load-rule
      rules:
      - alert: cpu-load-high
        expr: irate(container_cpu_usage_seconds_total{image!=&quot;&quot;}[1m]) &gt; 0.1
        for: 1m
        labels:
          serverity: warning
        annotations:
          summary: &quot;{{ $labels.instance }} container_name: {{ $labels.container_name }}  pod_name: {{ $labels.pod_name }} , namespace: {{ $labels.namespace}}&quot;


</code></pre><h4 id="也贴上prometheus的statefulset-配置">也贴上prometheus的StatefulSet 配置<a href="#也贴上prometheus的statefulset-配置" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h4>
<pre><code>&gt;  prometheus-statefulset.yml 配置可以看到prometheus-rulefiles 这个configmap是 挂载到 /etc/prometheus/rules/ 目录
&gt;  prometheus-configmap.yml 配置可以看到     rule_files: - /etc/prometheus/rules/*.yml,  所以 以上prometheus-rulefiles 这个configmap 的k8s.yml就被prometheus当做rule文件了
</code></pre><ul>
<li>prometheus-statefulset.yml</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
 labels:
   app: prometheus
   prometheus: k8s
 name: prometheus
 namespace: monitoring
spec:
 replicas: 1
 volumeClaimTemplates:
 - metadata:
     name: prometheus-data
     annotations:
       volume.beta.kubernetes.io/storage-class: &quot;nfs-retain&quot; # 这里配置 上面创建的 storageclass 的名称
   spec:
     accessModes: [ &quot;ReadWriteOnce&quot; ]
     resources:
       requests:
         storage: 20Gi
 revisionHistoryLimit: 10
 selector:
   matchLabels:
     app: prometheus
     prometheus: k8s
 serviceName: prometheus
 updateStrategy:
   type: RollingUpdate
 template:
   metadata:
     creationTimestamp: null
     labels:
       app: prometheus
       prometheus: k8s
   spec:
     serviceAccount: prometheus-k8s
     containers:
       - args:
           - --web.console.templates=/etc/prometheus/consoles
           - --web.console.libraries=/etc/prometheus/console_libraries
           - --config.file=/etc/prometheus/config/prometheus.yml
           - --storage.tsdb.path=/prometheus
           - --web.enable-admin-api
           - --storage.tsdb.retention.time=20d
           - --web.enable-lifecycle
           - --storage.tsdb.no-lockfile
           - --web.external-url=http://prometheus1-dev.xxx.com/
           - --web.route-prefix=/
         image: prom/prometheus:v2.11.1
         imagePullPolicy: IfNotPresent
         livenessProbe:
           failureThreshold: 6
           httpGet:
             path: /-/healthy
             port: web
             scheme: HTTP
           periodSeconds: 5
           successThreshold: 1
           timeoutSeconds: 3
         name: prometheus
         ports:
           - containerPort: 9090
             name: web
             protocol: TCP
         readinessProbe:
           failureThreshold: 120
           httpGet:
             path: /-/ready
             port: web
             scheme: HTTP
           periodSeconds: 5
           successThreshold: 1
           timeoutSeconds: 3
         resources:
           requests:
             memory: 400Mi
         terminationMessagePath: /dev/termination-log
         terminationMessagePolicy: File
         volumeMounts:
           - mountPath: /etc/prometheus/config
             name: config
             readOnly: true
           - mountPath: /prometheus
             name: prometheus-data
             #subPath: prometheus-db
           - mountPath: /etc/prometheus/rules/
             name: prometheus-rulefiles
     volumes:
       - name: config
         configMap:
           defaultMode: 420
           name: prometheus
       - name: prometheus-rulefiles
         configMap:
           defaultMode: 420
           name: prometheus-rulefiles
</code></pre><ul>
<li>prometheus-configmap.yml</li>
</ul>
<pre><code>kind: ConfigMap
metadata:
  name: prometheus
  namespace: monitoring
apiVersion: v1
data:
  prometheus.yml: |
    global:
      evaluation_interval: 30s
      scrape_interval: 30s
      external_labels:
        prometheus: monitoring/k8s
    alerting:
      alertmanagers:
      - static_configs:
        - targets: ['alertmanager:80']
    rule_files:
    - /etc/prometheus/rules/*.yml
    scrape_configs:
    - job_name: prometheus
      honor_labels: false
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - monitoring
      scrape_interval: 30s
      relabel_configs:
      - action: keep
        source_labels:
        - __meta_kubernetes_service_label_prometheus
        regex: k8s
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: job
        replacement:
      - target_label: endpoint
        replacement: web

    - job_name: k8s-db-t-kube-apiserver
      honor_labels: false
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - default
      scrape_interval: 30s
      scheme: https
      tls_config:
        insecure_skip_verify: false
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
        - action: keep
          source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          separator: ;
          regex: default;kubernetes;https
      metric_relabel_configs:
        - source_labels:
            - __name__
          action: drop
          regex: (apiserver_storage_data_key_generation_latencies_microseconds_bucket|apiserver_admission_controller_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_summary|apiserver_request_latencies_bucket|apiserver_request_latencies_summary|apiserver_storage_data_key_generation_latencies_microseconds_bucket|rest_client_request_latency_seconds_bucket)

    - job_name: k8s-db-t-pods
      honor_labels: true
      kubernetes_sd_configs:
      - role: node
      scrape_interval: 30s
      metrics_path: /metrics/cadvisor
      scheme: https
      tls_config:
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token


    - job_name: k8s-db-t-kube-state-metrics
      honor_labels: true
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - monitoring
      scrape_interval: 30s
      scrape_timeout: 30s
      tls_config:
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
        - action: keep
          source_labels:
            - __meta_kubernetes_service_label_k8s_app
          regex: kube-state-metrics
        - action: keep
          source_labels:
            - __meta_kubernetes_endpoint_port_name
          regex: http-main
      metric_relabel_configs:
        - source_labels:
            - __name__
          regex: (kube_daemonset_status_number_ready|kube_daemonset_status_number_unavailable|kube_deployment_status_replicas_unavailable|kube_deployment_spec_paused|kube_deployment_spec_strategy_rollingupdate_max_surge|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_endpoint_address_available|kube_endpoint_address_not_ready|kube_node_info|kube_node_spec_unschedulable|kube_node_status_condition|kube_node_status_capacity|kube_node_status_capacity|kube_node_status_allocatable|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_status_phase|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolume_status_phase|kube_persistentvolume_info|kube_persistentvolume_capacity_bytes|kube_pod_info|kube_pod_status_phase|kube_pod_status_ready|kube_pod_container_info|kube_pod_container_status_waiting|kube_pod_container_status_waiting_reason|kube_pod_container_status_running|kube_pod_container_status_terminated_reason|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|kube_service_info|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_deployment_status_replicas_available|kube_deployment_status_replicas|kube_node_status_allocatable_memory_bytes|kube_deployment_status_replicas|kube_statefulset_replicas|kube_daemonset_status_desired_number_scheduled|kube_statefulset_status_replicas|changes|kube_job_status_failed)
          action: keep


</code></pre>
			</div>
   

			<hr class="post-end">
			<footer class="post-info">
				<p>
					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://www.ngirl.xyz/tags/k8s">k8s</a></span><span class="tag"><a href="https://www.ngirl.xyz/tags/promethues">promethues</a></span>
				</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>2957 Words</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2020-05-15 02:34 &#43;0800</p>
			</footer>
		</article>
		<aside id="toc" class="show-toc">
			<div class="toc-title">Table of Contents</div>
			<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#搭建prometheus">搭建prometheus</a></li>
        <li><a href="#查看etc指标页面">查看etc指标页面</a></li>
        <li><a href="#安装-prometheus">安装 Prometheus</a></li>
        <li><a href="#role-和-rolebinding">role 和 roleBinding</a></li>
        <li><a href="#问题1-k8s110报错rbac权限错误">问题1 k8s1.10报错rbac权限错误</a></li>
        <li><a href="#监控etcd">监控etcd</a></li>
        <li><a href="#监控-apiserver">监控 apiserver</a></li>
        <li><a href="#监控-pod">监控 pod</a></li>
        <li><a href="#安装-kube-state-metrics">安装 kube-state-metrics</a></li>
        <li><a href="#deployment-和-service">deployment 和 service</a></li>
        <li><a href="#配置一个grafana">配置一个grafana</a></li>
        <li><a href="#部署alertmanager">部署alertmanager</a></li>
      </ul>
    </li>
  </ul>
</nav>
		</aside>
		<div class="post-nav thin">
			<a class="next-post" href="https://www.ngirl.xyz/posts/48-k8s%E5%8D%87%E7%BA%A7-1-10-1-15/">
				<span class="post-nav-label"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>&nbsp;Newer</span><br><span>k8s升级(1.10-&gt;1.15)</span>
			</a>
			<a class="prev-post" href="https://www.ngirl.xyz/posts/46-docker%E5%AE%89%E8%A3%85nginx%E7%AC%AC%E4%B8%89%E6%96%B9%E6%A8%A1%E5%9D%97/">
				<span class="post-nav-label">Older&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg></span><br><span>docker安装nginx第三方模块</span>
			</a>
		</div>
		<div id="comments" class="thin">
						<script src="https://utteranc.es/client.js"
							repo="zhangzw001/blog-hugo"
							issue-term="pathname"
							theme="github-light"
							crossorigin="anonymous"
							async>
			</script>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2019 - 2021 <a href="https://www.ngirl.xyz">zhangzw</a> &#183; <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://www.ngirl.xyz/post/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
	</footer>


	<script src="https://www.ngirl.xyz/js/main.min.784417f5847151f848c339cf0acb13a06cbb648b1483435a28ed4556c4ead69b.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-180942795-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


</body>

</html>
