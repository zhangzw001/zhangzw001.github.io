<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[istio环境下配置nginx+php]]></title>
    <url>%2F2020%2F08%2F04%2F52-istio%E6%B5%8B%E8%AF%95nginx-php%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[将nginx+php的环境结合istio的智能路由功能做一个简单实践 istio中文官方-协议选择 istio中文官方-virtualService 安装部署请参考官方 https://istio.io/latest/zh/docs/setup/getting-started/ 一 部署nginx+php,并设置简单智能路由 环境说明 123451. k8s: 1.15.112. istio: 1.6.73. istio-alpha ns设置了自动注入: kubectl label namespace istio-alpha istio-injection=enabled kubectl get namespaces istio-alpha --show-labels 1.1 安装部署php-fpm1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798apiVersion: apps/v1kind: Deploymentmetadata: name: php-fpm-v1 namespace: istio-alpha labels: app: php-fpm version: v1spec: replicas: 1 selector: matchLabels: app: php-fpm version: v1 template: metadata: labels: app: php-fpm version: v1 spec: containers: - name: app image: hub.xxx.com/bq/php:7.0.13-fpm imagePullPolicy: Always ports: - name: tcp protocol: TCP containerPort: 9000 resources: requests: cpu: "50m" limits: cpu: "100m" volumeMounts: - name: php-fpm-v1-data mountPath: /webwww volumes: - name: php-fpm-v1-data nfs: server: x.x.x.x path: /disk/k8s-nfs-data/k8s1-t/php-fpm-7-0-13/webwww-data ---apiVersion: apps/v1kind: Deploymentmetadata: name: php-fpm-v2 namespace: istio-alpha labels: app: php-fpm version: v2spec: replicas: 1 selector: matchLabels: app: php-fpm version: v2 template: metadata: labels: app: php-fpm version: v2 spec: containers: - name: app image: hub.xxx.com/bq/php:7.0.13-fpm imagePullPolicy: Always ports: - name: tcp protocol: TCP containerPort: 9000 resources: requests: cpu: "50m" limits: cpu: "100m" volumeMounts: - name: php-fpm-v2-data mountPath: /webwww volumes: - name: php-fpm-v2-data nfs: server: x.x.x.x path: /disk/k8s-nfs-data/k8s1-t/php-fpm-7-0-13/webwww-data---apiVersion: v1kind: Servicemetadata: name: php-fpm namespace: istio-alpha labels: app: php-fpmspec: ports: - name: tcp port: 9000 protocol: TCP selector: app: php-fpm 1.2 安装部署nginx123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-v1 labels: app: nginx version: v1 namespace: istio-alphaspec: replicas: 1 selector: matchLabels: app: nginx version: v1 template: metadata: labels: app: nginx version: v1 spec: containers: - name: nginx-v1 image: hub.xxx.com/bq/nginx:1.16 ports: - containerPort: 80 name: http protocol: TCP resources: requests: cpu: "30m" limits: cpu: "100m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: nginx-v1-cm mountPath: /etc/nginx/conf.d/ volumes: - name: nginx-www-dev nfs: server: x.x.x.x path: /disk/k8s-nfs-data/k8s1-t/php-fpm-7-0-13/webwww-data - name: nginx-v1-cm configMap: name: nginx-v1-cm---kind: ConfigMapmetadata: name: nginx-v1-cm labels: app: nginx-v1 namespace: istio-alphaapiVersion: v1data: nginx.conf: | server &#123; listen 80 default_server; server_name _; root /webwww/test-v1; add_header "X" "v1"; location = /50x.html &#123; root html; &#125; location / &#123; index index.php index.html index.htm; &#125; location ~ \.php$ &#123; fastcgi_pass php-fpm:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTP_HOST $server_name; include fastcgi_params; &#125; &#125;---apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-v2 labels: app: nginx version: v2 namespace: istio-alphaspec: replicas: 1 selector: matchLabels: app: nginx version: v2 template: metadata: labels: app: nginx version: v2 spec: containers: - name: nginx-v2 image: hub.xxx.com/bq/nginx:1.16 ports: - containerPort: 80 name: http protocol: TCP resources: requests: cpu: "30m" limits: cpu: "100m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: nginx-v2-cm mountPath: /etc/nginx/conf.d/ volumes: - name: nginx-www-dev nfs: server: x.x.x.x path: /disk/k8s-nfs-data/k8s1-t/php-fpm-7-0-13/webwww-data - name: nginx-v2-cm configMap: name: nginx-v2-cm---kind: ConfigMapmetadata: name: nginx-v2-cm labels: app: nginx-v2 namespace: istio-alphaapiVersion: v1data: nginx.conf: | server &#123; listen 80 default_server; server_name _; root /webwww/test-v2; add_header "X" "v2"; location = /50x.html &#123; root html; &#125; location / &#123; index index.php index.html index.htm; &#125; location ~ \.php$ &#123; fastcgi_pass php-fpm:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTP_HOST $server_name; include fastcgi_params; &#125; &#125;---apiVersion: v1kind: Servicemetadata: name: nginx namespace: istio-alpha labels: app: nginxspec: ports: - name: http port: 80 protocol: TCP selector: app: nginx 1.3 配置默认destinationRule1234567891011121314151617181920212223242526272829apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: nginx namespace: istio-alphaspec: host: nginx subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2---apiVersion: networking.istio.io/v1alpha3kind: DestinationRulemetadata: name: php-fpm namespace: istio-alphaspec: host: php-fpm subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 1.4 配置nginx的gateway和VirtualService12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: nginx-gateway namespace: istio-alphaspec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: nginx-80 protocol: HTTP hosts: - "*"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-gateway namespace: istio-alphaspec: hosts: - "*" gateways: - nginx-gateway http: - match: - uri: exact: /phpinfo.php route: - destination: host: nginx port: number: 80 subset: v1 weight: 90 - destination: host: nginx port: number: 80 subset: v2 weight: 10 以上配置之后即可在kiali查看 graph 1.5 简单配置流量全部切到v1 或v21234567891011121314151617181920apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-gateway namespace: istio-alphaspec: hosts: - "*" gateways: - nginx-gateway http: - match: - uri: exact: /phpinfo.php route: - destination: host: nginx port: number: 80 subset: v2 当然也可以根据uri match选择不同的路由 1234567891011121314151617181920212223242526272829apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: nginx-gateway namespace: istio-alphaspec: hosts: - "*" gateways: - nginx-gateway http: - match: - uri: exact: /phpinfo.php route: - destination: host: nginx port: number: 80 subset: v2 - match: - uri: exact: /a.php route: - destination: host: nginx port: number: 80 subset: v1 1.6 配置php-fpm的virtualService 将流量全部转到 v1版本 1234567891011121314151617apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: php-fpm namespace: istio-alphaspec: hosts: - php-fpm tcp: - match: - port: 9000 route: - destination: host: php-fpm port: number: 9000 subset: v1 将流量分成1:9 123456789101112131415161718192021222324apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: php-fpm namespace: istio-alphaspec: hosts: - php-fpm tcp: - match: - port: 9000 route: - destination: host: php-fpm port: number: 9000 subset: v1 weight: 10 - destination: host: php-fpm port: number: 9000 subset: v2 weight: 90 $ 问题说明$.1 协议选择说明 通过声明一个 Service 端口，协议可以被手动指定 name: &lt;protocol&gt;[-&lt;suffix&gt;]。 下列协议是被支持的： 1234567891011grpcgrpc-webhttphttp2httpsmongomysql*redis*tcptlsudp 因此注意,我们在部署deployment和service的时候, ports.name 会被istio 认为是协议 例如下面的例子中, name: tcp, 如果配置了其他的值, 会导致nginx-&gt;php-fpm 协议异常, 可能会出现错误: upstream sent unsupported FastCGI protocol version: 72 while reading response header from upstream 1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: apps/v1kind: Deploymentmetadata: name: php-fpm-v1 namespace: istio-alpha labels: app: php-fpm version: v1spec: replicas: 1 selector: matchLabels: app: php-fpm version: v1 template: metadata: labels: app: php-fpm version: v1 spec: containers: - name: app image: php:7.0.13-fpm imagePullPolicy: Always ports: - name: tcp protocol: TCP containerPort: 9000 ...---apiVersion: v1kind: Servicemetadata: name: php-fpm namespace: istio-alpha labels: app: php-fpmspec: ports: - name: tcp port: 9000 protocol: TCP selector: app: php-fpm]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s搭建radius]]></title>
    <url>%2F2020%2F07%2F27%2F51-k8s%E6%90%AD%E5%BB%BAradius%2F</url>
    <content type="text"><![CDATA[搭建lnmp+freeradius的账号认证服务 一 记录123456docker: 17.03.2-cek8s: 1.15.11php: 7.0.13mysql: 5.6freeraidus: 3.0.21daloradius: 1.1-2 / 08 Aug 2019 二 首先搭建数据库k8s-freeradius-mysql-dev_5.6.36-configmap.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263---kind: ConfigMapmetadata: name: freeradius-mysql-dev labels: app: freeradius-mysql-dev namespace: dbapiVersion: v1data: my.cnf: | [mysqld] innodb_file_per_table=1 user=mysql datadir=/data socket=/data/mysql.sock symbolic-links=0 binlog_format=mixed #log-bin=mysql-bin log-bin=/data/mysql-bin.log #general_log = 1 #general_log_file = /data/general.log #日志 log-error=/data/mysqld.log innodb_log_file_size = 256M thread_cache_size = 50 max_allowed_packet=16M max_binlog_size=500M #old_passwords=1 character-set-server=utf8 max_connections=3000 skip-name-resolve max_allowed_packet=64M tmp_table_size=200M server-id=101 port=3306 skip_slave_start expire_logs_days=7 [mysqld_safe] log-error=/data/mysqld.log pid-file=/data/mysqld.pid [mysql] no-auto-rehash user=root default-character-set=utf8 socket=/data/mysql.sock [client] socket=/data/mysql.sock k8s-freeradius-mysql-dev_5.6.36.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465---apiVersion: apps/v1kind: Deploymentmetadata: labels: app: freeradius-mysql-dev name: freeradius-mysql-dev namespace: dbspec: replicas: 1 selector: matchLabels: app: freeradius-mysql-dev template: metadata: labels: app: freeradius-mysql-dev spec: containers: - name: freeradius-mysql-dev image: hub.xxx.com/mysql:5.6.36-Asia ports: - containerPort: 3306 name: db-port resources: requests: cpu: "50m" limits: cpu: "1" env: - name: MYSQL_ROOT_PASSWORD value: "xxx" volumeMounts: - name: freeradius-mysql-dev-data mountPath: /data - name: freeradius-mysql-dev-conf mountPath: /etc/mysql/my.cnf subPath: my.cnf volumes: - name: freeradius-mysql-dev-data nfs: server: xxx.xxx.xxx.194 path: /disk/k8s-nfs-data/k8s-db-t/freeradius-mysql-dev - name: freeradius-mysql-dev-conf configMap: name: freeradius-mysql-dev---kind: ServiceapiVersion: v1metadata: labels: app: freeradius-mysql-dev name: freeradius-mysql-dev namespace: dbspec: type: NodePort ports: - port: 3306 name: db-port targetPort: 3306 nodePort: 3326 protocol: TCP selector: app: freeradius-mysql-dev 创建库和账号123456create database radius;grant all on radius.* to radius@'%' identified by 'xxxxxxxxxxxxxxxxxx';flush privileges;# 测试登录mysql -hk8s-db-t.xxx.com -P3326 -uradius -p 三 部署freeradiusdeployment-freeradius.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100apiVersion: apps/v1kind: Deploymentmetadata: name: freeradius-dev labels: app: freeradius-dev namespace: radius-devspec: replicas: 1 selector: matchLabels: app: freeradius-dev template: metadata: labels: app: freeradius-dev spec: containers: - name: freeradius-dev image: hub.xxx.com/freeradius-server:3.0.21 ports: - containerPort: 1812 name: port-1812 protocol: UDP - containerPort: 1813 name: port-1813 protocol: UDP resources: requests: cpu: "30m" limits: cpu: "1" volumeMounts: - name: daloradius-freeradius-sql-dev mountPath: /etc/raddb/mods-available/sql subPath: sql - name: daloradius-freeradius-radiusd-dev mountPath: /etc/raddb/radiusd.conf subPath: radiusd.conf - name: daloradius-freeradius-clients-dev mountPath: /etc/raddb/clients.conf subPath: clients.conf - name: daloradius-freeradius-log mountPath: /nginx_logs volumes: - name: daloradius-freeradius-sql-dev configMap: name: daloradius-freeradius-sql-dev - name: daloradius-freeradius-radiusd-dev configMap: name: daloradius-freeradius-radiusd-dev - name: daloradius-freeradius-clients-dev configMap: name: daloradius-freeradius-clients-dev - name: daloradius-freeradius-log nfs: server: xxx.xxx.xxx.194 path: /disk/k8s-nfs-data/k8s1-t/daloradius-php/nginx_logs---apiVersion: v1kind: Servicemetadata: name: freeradius-dev namespace: radius-dev labels: app: freeradius-devspec: type: ClusterIP ports: - name: port-1812 port: 1812 protocol: UDP - name: port-1813 port: 1813 protocol: UDP selector: app: freeradius-dev---kind: ServiceapiVersion: v1metadata: name: freeradius-dev-svc namespace: radius-dev labels: app: freeradius-devspec: type: NodePort ports: - name: port-1812 port: 1812 targetPort: 1812 nodePort: 1812 protocol: UDP - name: port-1813 port: 1813 targetPort: 1813 nodePort: 1813 protocol: UDP selector: app: freeradius-dev configmap-freeradius-radiusd-conf.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127---kind: ConfigMapmetadata: name: daloradius-freeradius-radiusd-dev labels: app: daloradius-freeradius-radiusd-dev namespace: radius-devapiVersion: v1data: radiusd.conf: | prefix = /usr exec_prefix = /usr sysconfdir = /etc localstatedir = /var sbindir = $&#123;exec_prefix&#125;/sbin logdir = /nginx_logs raddbdir = /etc/freeradius radacctdir = $&#123;logdir&#125;/radacct name = freeradius confdir = $&#123;raddbdir&#125; modconfdir = $&#123;confdir&#125;/mods-config certdir = $&#123;confdir&#125;/certs cadir = $&#123;confdir&#125;/certs run_dir = $&#123;localstatedir&#125;/run/$&#123;name&#125; # Should likely be $&#123;localstatedir&#125;/lib/radiusd db_dir = $&#123;raddbdir&#125; libdir = /usr/lib/freeradius pidfile = $&#123;run_dir&#125;/$&#123;name&#125;.pid correct_escapes = true max_request_time = 30 cleanup_delay = 5 max_requests = 16384 hostname_lookups = no log &#123; destination = files colourise = yes file = $&#123;logdir&#125;/radius.log syslog_facility = daemon stripped_names = no auth = no # auth_accept = no # auth_reject = no auth_badpass = no auth_goodpass = no msg_denied = "You are already logged in - access denied" &#125; checkrad = $&#123;sbindir&#125;/checkrad ENV &#123; &#125; security &#123; user = freerad group = freerad allow_core_dumps = no max_attributes = 200 reject_delay = 1 status_server = yes &#125; proxy_requests = yes $INCLUDE proxy.conf $INCLUDE clients.conf thread pool &#123; start_servers = 5 max_servers = 32 min_spare_servers = 3 max_spare_servers = 10 max_requests_per_server = 0 auto_limit_acct = no &#125; modules &#123; $INCLUDE mods-enabled/ &#125; instantiate &#123; &#125; policy &#123; $INCLUDE policy.d/ &#125; $INCLUDE sites-enabled/ configmap-freeradius-clients-conf.yaml1234567891011121314151617181920212223242526272829303132333435363738---kind: ConfigMapmetadata: name: daloradius-freeradius-clients-dev labels: app: daloradius-freeradius-clients-dev namespace: radius-devapiVersion: v1data: clients.conf: | client k8s_pods_clients &#123; ipaddr = 192.168.0.0/16 secret = testing123-k8s-pod require_message_authenticator = no &#125; client k8s_svc_clients &#123; ipaddr = 10.96.0.0/12 secret = testing123-k8s-svc require_message_authenticator = no &#125; client localhost &#123; ipaddr = 127.0.0.1 proto = * secret = testing123 require_message_authenticator = no nas_type = other # localhost isn't usually a NAS... limit &#123; max_connections = 16 lifetime = 0 idle_timeout = 30 &#125; &#125; client localhost_ipv6 &#123; ipv6addr = ::1 secret = testing123 &#125; configmap-freeradius-sql.yaml 主要是修改 Connection info 的内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879---kind: ConfigMapmetadata: name: daloradius-freeradius-sql-dev labels: app: daloradius-freeradius-sql-dev namespace: radius-devapiVersion: v1data: sql: | sql &#123; dialect = "mysql" driver = "rlm_sql_mysql" sqlite &#123; filename = "/tmp/freeradius.db" busy_timeout = 200 bootstrap = "$&#123;modconfdir&#125;/$&#123;..:name&#125;/main/sqlite/schema.sql" &#125; mysql &#123; tls &#123; ca_file = "/etc/ssl/certs/my_ca.crt" ca_path = "/etc/ssl/certs/" certificate_file = "/etc/ssl/certs/private/client.crt" private_key_file = "/etc/ssl/certs/private/client.key" cipher = "DHE-RSA-AES256-SHA:AES128-SHA" tls_required = yes tls_check_cert = no tls_check_cert_cn = no &#125; warnings = auto &#125; postgresql &#123; send_application_name = yes &#125; mongo &#123; appname = "freeradius" tls &#123; certificate_file = /path/to/file certificate_password = "password" ca_file = /path/to/file ca_dir = /path/to/directory crl_file = /path/to/file weak_cert_validation = false allow_invalid_hostname = false &#125; &#125; server = "k8s-db-t.xxx.com" port = 3326 login = "radius" password = "xxxxxxxxxxxxxxxxxx" radius_db = "radius" acct_table1 = "radacct" acct_table2 = "radacct" postauth_table = "radpostauth" authcheck_table = "radcheck" groupcheck_table = "radgroupcheck" authreply_table = "radreply" groupreply_table = "radgroupreply" usergroup_table = "radusergroup" delete_stale_sessions = yes pool &#123; start = $&#123;thread[pool].start_servers&#125; min = $&#123;thread[pool].min_spare_servers&#125; max = $&#123;thread[pool].max_servers&#125; spare = $&#123;thread[pool].max_spare_servers&#125; uses = 0 retry_delay = 30 lifetime = 0 idle_timeout = 60 &#125; client_table = "nas" group_attribute = "SQL-Group" $INCLUDE $&#123;modconfdir&#125;/$&#123;.:name&#125;/main/$&#123;dialect&#125;/queries.conf &#125; 四 部署nignx+php7的daloradius web管理后台php7部署配置php镜像安装扩展 Dockerfile1234567from hub.xxx.com/php:developuser rootRUN pear install DB \ &amp;&amp; pear install -a Mail \ &amp;&amp; pear install -a Mail_Mime deployment-php.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990---apiVersion: apps/v1kind: Deploymentmetadata: name: daloradius-php-dev labels: app: daloradius-php-dev namespace: radius-devspec: replicas: 1 strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate selector: matchLabels: app: daloradius-php-dev template: metadata: labels: app: daloradius-php-dev spec: containers: - name: daloradius-php-dev image: hub.xxx.com/php:radius imagePullPolicy: Always ports: - containerPort: 9000 name: fpm-9000 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "600m" volumeMounts: - name: daloradius-nginx-dev mountPath: /webwww - name: daloradius-php-cfg-dev mountPath: "/usr/local/etc/php/php.ini" subPath: php.ini - name: daloradius-fpm-cfg-dev mountPath: "/usr/local/etc/php-fpm.d/www.conf" subPath: www.conf volumes: - name: daloradius-nginx-dev nfs: server: xxx.xxx.xxx.194 path: /disk/k8s-nfs-data/k8s1-t/daloradius-php - name: daloradius-php-cfg-dev configMap: name: daloradius-php-cfg-dev - name: daloradius-fpm-cfg-dev configMap: name: daloradius-fpm-cfg-dev---apiVersion: v1kind: Servicemetadata: name: daloradius-php-dev namespace: radius-dev labels: app: daloradius-php-devspec: type: ClusterIP ports: - name: fpm-9000 port: 9000 protocol: TCP selector: app: daloradius-php-dev---apiVersion: v1kind: Servicemetadata: name: daloradius-php-dev-svc namespace: radius-dev labels: app: daloradius-php-devspec: type: NodePort selector: app: daloradius-php-dev ports: - name: fpm-9000 port: 9000 targetPort: 9000 nodePort: 32101 protocol: TCP configmap-php-fpm.yaml12345678910111213141516171819---kind: ConfigMapmetadata: name: daloradius-fpm-cfg-dev labels: app: daloradius-fpm-cfg-dev namespace: radius-devapiVersion: v1data: www.conf: | [www] user = 101 group = 101 listen = 127.0.0.1:9000 pm = static pm.max_children = 20 pm.start_servers = 10 pm.min_spare_servers = 5 pm.max_spare_servers = 5 configmap-php.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149---kind: ConfigMapmetadata: name: daloradius-php-cfg-dev labels: app: daloradius-php-cfg-dev namespace: radius-devapiVersion: v1data: php.ini: | [PHP] engine = On short_open_tag = Off precision = 14 output_buffering = 4096 zlib.output_compression = Off implicit_flush = Off unserialize_callback_func = serialize_precision = 17 disable_functions = disable_classes = zend.enable_gc = On expose_php = On max_execution_time = 30 max_input_time = 60 memory_limit = 128M max_input_vars = 10000 error_reporting = E_ALL &amp; ~E_DEPRECATED &amp; ~E_STRICT display_errors = On display_startup_errors = Off log_errors = On log_errors_max_len = 1024 ignore_repeated_errors = Off ignore_repeated_source = Off report_memleaks = On track_errors = Off html_errors = On error_log = php_errors.log variables_order = "GPCS" request_order = "GP" register_argc_argv = Off auto_globals_jit = On post_max_size = 8M auto_prepend_file = auto_append_file = default_mimetype = "text/html" default_charset = "UTF-8" doc_root = user_dir = enable_dl = Off file_uploads = On upload_max_filesize = 2M max_file_uploads = 20 allow_url_fopen = On allow_url_include = Off default_socket_timeout = 60 [CLI Server] cli_server.color = On [Date] date.timezone =Asia/Shanghai [filter] [iconv] [intl] [sqlite3] [Pcre] [Pdo] [Pdo_mysql] pdo_mysql.cache_size = 2000 pdo_mysql.default_socket= [Phar] [mail function] SMTP = localhost smtp_port = 25 mail.add_x_header = On [SQL] sql.safe_mode = Off [ODBC] odbc.allow_persistent = On odbc.check_persistent = On odbc.max_persistent = -1 odbc.max_links = -1 odbc.defaultlrl = 4096 odbc.defaultbinmode = 1 [Interbase] ibase.allow_persistent = 1 ibase.max_persistent = -1 ibase.max_links = -1 ibase.timestampformat = "%Y-%m-%d %H:%M:%S" ibase.dateformat = "%Y-%m-%d" ibase.timeformat = "%H:%M:%S" [MySQLi] mysqli.max_persistent = -1 mysqli.allow_persistent = On mysqli.max_links = -1 mysqli.cache_size = 2000 mysqli.default_port = 3306 mysqli.default_socket = mysqli.default_host = mysqli.default_user = mysqli.default_pw = mysqli.reconnect = Off [mysqlnd] mysqlnd.collect_statistics = On mysqlnd.collect_memory_statistics = Off [OCI8] [PostgreSQL] pgsql.allow_persistent = On pgsql.auto_reset_persistent = Off pgsql.max_persistent = -1 pgsql.max_links = -1 pgsql.ignore_notice = 0 pgsql.log_notice = 0 [bcmath] bcmath.scale = 0 [browscap] [Session] session.save_handler = redis session.save_path = "tcp://xxx.xxx.xxx.194:6399" session.use_strict_mode = 0 session.use_cookies = 1 session.use_only_cookies = 1 session.name = PHPSESSID session.auto_start = 0 session.cookie_lifetime = 0 session.cookie_path = / session.cookie_domain = session.cookie_httponly = session.serialize_handler = php session.gc_probability = 1 session.gc_divisor = 1000 session.gc_maxlifetime = 1440 session.referer_check = session.cache_limiter = nocache session.cache_expire = 180 session.use_trans_sid = 0 session.hash_function = 0 session.hash_bits_per_character = 5 url_rewriter.tags = "a=href,area=href,frame=src,input=src,form=fakeentry" [Assertion] zend.assertions = -1 tidy.clean_output = Off [soap] soap.wsdl_cache_enabled=1 soap.wsdl_cache_dir="/tmp" soap.wsdl_cache_ttl=86400 soap.wsdl_cache_limit = 5 [sysvshm] [ldap] ldap.max_links = -1 nginx部署配置deployment-nginx.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263---apiVersion: apps/v1kind: Deploymentmetadata: name: daloradius-nginx-dev labels: app: daloradius-nginx-dev namespace: radius-devspec: replicas: 1 selector: matchLabels: app: daloradius-nginx-dev template: metadata: labels: app: daloradius-nginx-dev spec: containers: - name: daloradius-nginx-dev image: hub.xxx.com/nginx:1.16.0-develop ports: - containerPort: 80 name: nginx-80 protocol: TCP resources: requests: cpu: "30m" limits: cpu: "500m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: daloradius-nginx-dev-cm mountPath: /etc/nginx/nginx.conf subPath: nginx.conf volumes: - name: nginx-www-dev nfs: server: xxx.xxx.xxx.194 path: /disk/k8s-nfs-data/k8s1-t/daloradius-php - name: daloradius-nginx-dev-cm configMap: name: daloradius-nginx-dev-cm---kind: ServiceapiVersion: v1metadata: labels: app: daloradius-nginx-dev name: daloradius-nginx-dev namespace: radius-devspec: type: NodePort ports: - name: nginx-80 port: 80 targetPort: 80 nodePort: 32100 protocol: TCP selector: app: daloradius-nginx-dev--- configmap-php-nginx.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162---kind: ConfigMapmetadata: name: daloradius-nginx-dev-cm labels: app: daloradius-nginx-dev namespace: radius-devapiVersion: v1data: nginx.conf: | user www-data; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events &#123; worker_connections 1024; &#125; http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; server &#123; listen 80 default_server; server_name radius.xxx.com ; root /webwww; location = /50x.html &#123; root html; &#125; location / &#123; index index.php index.html index.htm; &#125; location ~ \.php$ &#123; fastcgi_pass daloradius-php-dev:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTP_HOST $server_name; include fastcgi_params; &#125; &#125; &#125; 五 测试通过前端nginx代理转发到nodeport端口1234567891011121314151617181920212223242526server &#123; listen 80; charset utf-8; listen 443 ssl http2; server_name radius.xxx.com; ssl_certificate /etc/nginx/server.pem; ssl_certificate_key /etc/nginx/server.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers AESGCM:ALL:!DH:!EXPORT:!RC4:+HIGH:!MEDIUM:!LOW:!aNULL:!eNULL; ssl_prefer_server_ciphers on; #转向预发布环境 location / &#123; proxy_pass http://xxx.xxx.xxx.221:32100/; proxy_redirect http://$host:32100 http://$host; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 配置k8s的ingress12345678910111213141516171819---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: daloradius-nginx-dev labels: app: daloradius-nginx-dev namespace: radius-dev annotations: kubernetes.io/ingress.class: "traefik"spec: rules: - host: radius.xxx.com http: paths: - path: / backend: serviceName: daloradius-nginx-dev servicePort: 80 登录测试12登录地址: radius.xxx.com账号密码: administrator/radius 通过后台新建账号之后, 测试账号连通性命令行12345678910111213# 在freeradius 容器内支线radtest xxx xxx 127.0.0.1 0 testing123# 在其他客户端执行(configmap-freeradius-clients-conf.yaml 配置允许的ip)radtest xxx xxx k8s1-t.xxx.com 0 testing123-k8s-podSending Access-Request of id 237 to xxx.xxx.xxx.120 port 1812 User-Name = "xxx" User-Password = "xxx" NAS-IP-Address = 192.168.0.172 NAS-Port = 0 Message-Authenticator = 0x00000000000000000000000000000000rad_recv: Access-Reject packet from host xxx.xxx.xxx.120 port 1812, id=237, length=20 通过radius.xxx.com 管理页面测试连通性]]></content>
      <categories>
        <category>k8s</category>
        <category>radius</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>freeradius</tag>
        <tag>daloradius</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次windows安装OpenSSH问题]]></title>
    <url>%2F2020%2F06%2F12%2F50-%E8%AE%B0%E4%B8%80%E6%AC%A1windows%E5%AE%89%E8%A3%85OpenSSH%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[windows一次安装openssh server问题 首先是安装官方教程官方releases版本 这里直接下载最新 : OpenSSH-Win64.zip, Symbols应该是ddl SSH-2.0-OpenSSH_for_Windows_8.1 解压包放在目录: C:\Program Files\OpenSSH12345# 进入到解压目录cd C:\Program Files\OpenSSH# 执行安装命令powershell.exe -ExecutionPolicy Bypass -File install-sshd.ps1 配置sshd_config123456789101112131415# 首先创建authorized_keys文件cd C:\ProgramData\sshecho "&lt;you ssh public key&gt;" &gt; authorized_keys# 使用icacls命令修改权限(仅允许SYSTEM和Administrators组有权限写,其他用户只读)icacls authorized_keys /inheritance:ricacls authorized_keys /grant SYSTEM:(F)icacls authorized_keys /grant BUILTIN\Administrators:(F)# sshd_config修改开启密钥访问：PubkeyAuthentication yes禁用密码访问：PasswordAuthentication no禁用空密码：PermitEmptyPasswords no关闭DNS查找: UseDNS no设置authorized_keys路径: AuthorizedKeysFile C:/ProgramData/ssh/authorized_keys 这里配置AuthorizedKeysFile 为全路径, 之前配置变量导致报错 启动12net start sshdnet stop sshd 卸载1powershell.exe -ExecutionPolicy Bypass -File uninstall-sshd.ps1 错误1 Read from socket failed: Connection reset by peer1AuthorizedKeysFile 的路径配置的变量, 之后改成绝对路径缺成功了]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go简单记录]]></title>
    <url>%2F2020%2F05%2F27%2F49-go%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[go的一些简单记录 1. go中’…’的用法 第一个用法: 用于函数有多个不定参数的情况,可以接受多个不确定个数参数, 将多个参数作为slice传递了 第二个用法: 用于slice打散进行传递 例一1234567891011121314151617func sum(nums ...int) &#123; fmt.Println(nums) totols :=0 for i, num := range nums&#123; totols +=num &#125; fmt.Println(totols)&#125;func main() &#123; slice1 := []int&#123;1,3,4&#125; //切片被打散传入 sum(slice1...) slice2 := make([]int,3) //元素被打散一个个append slice2 = append(slice2,slice1...)&#125; 2. golang中引用类型 slice(切片)、map(字典)、channel(管道) int类型的零值是0,string类型的零值是””，引用类型的零值是nil 值类型 这里a数组是值类型, 赋值给b,修改b不会导致a改变 12345a := [3]int&#123;1,2,3&#125;b := afmt.Println(a,b)b[2] = 4fmt.Println(a,b) 引用类型 这里a是个slice引用类型,修改b会改变a, slice的复制是引用类型 1234567a := []int&#123;1,2,3&#125;b := afmt.Printf("%v:\t%p,%v:\t%p\n",a,a,b,b) //[1 2 3]: 0xc000090020,[1 2 3]: 0xc000090020b[2] = 4fmt.Printf("%v:\t%p,%v:\t%p\n",a,a,b,b) //[1 2 4]: 0xc000090020,[1 2 4]: 0xc000090020c := a[1:]fmt.Printf("%v:\t%p,%v:\t%p\n",a,a,c,c) //[1 2 4]: 0xc000090020,[2 4]: 0xc000090028 3. golang查看变量类型 第一种123arr := [3]int&#123;1,2,3&#125;sli := []int&#123;1,2,3&#125;fmt.Printf("%T,%T\n",arr,sli) //[3]int,[]int 显示的结果可以看到, 一个是数组, 一个是切片 第二种1234arr := [3]int&#123;1,2,3&#125;sli := []int&#123;1,2,3&#125;fmt.Println("type:",reflect.TypeOf(arr) ) //type: [3]intfmt.Println("type:",reflect.TypeOf(sli) ) //type: []int 4. go中内置分配内存函数new和make的用法原文: go语言值类型与引用类型理解 make 仅用于 slice, map, channel, 返回类型是本身 1func make(t Type, size ...IntegerType) Type new 返回类型是指针(引用类型) 12345// The new built-in function allocates memory. The first argument is a type,// not a value, and the value returned is a pointer to a newly// allocated zero value of that type.func new(Type) *Type//它只接受一个参数，这个参数是一个类型，分配好内存后，返回一个指向该类型内存地址的指针。同时请注意它同时把分配的内存置为零，也就是类型的零值。 5. 查看slice的长度和容量12345// 初始化一个3个0的slice,分配内容的容量是10个ints3 := make([]int,3,10)fmt.Println(s3)fmt.Println(len(s3))fmt.Println(cap(s3)) 6. 闭包的说明原文: 闭包与匿名函数 闭包与外部函数的生命周期 内函数对外函数的修改 是对变量的引用, 所以内函数结束后变量不会释放, 变量的内存地址不变 1234567891011121314151617181920212223242526func A(n int) func() &#123; n++ return func() &#123; fmt.Println(n) &#125;&#125;func B(n int) func() &#123; return func() &#123; n++ fmt.Println(n) &#125;&#125;func main() &#123; A1:=A(20) fmt.Println(A1) //0x48e3d0 在这儿已经定义了n=20 ，然后执行++ 操作，所以是21 。 A1() //21 后面对闭包的调用，没有对n执行加一操作，所以一直是21 A1() //21 B1:=B(10) fmt.Println(B1) //0x48e340 这儿定义了n 为10 B1() //11 后面对闭包的调用，每次都对n进行加1操作。 B1() //12&#125; 7. 反引号 反引号会原样输出, 不能使用转义, \n也会原样输出 \r 类似python里面的 \r ，每次都覆盖上一次输出的内容 123456789func spinner(delay time.Duration) &#123; for &#123; //for _, r := range `-\|/` &#123; for _, r := range "-\\|/" &#123; fmt.Printf("\r%c",r) time.Sleep(delay) &#125; &#125;&#125; 8. 编译不同平台可执行文件1、Mac下编译Linux, Windows平台的64位可执行程序：12CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build test.goCGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build test.go 2、Linux下编译Mac, Windows平台的64位可执行程序：12CGO_ENABLED=0 GOOS=darwin GOARCH=amd64 go build test.goCGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build test.go 3、Windows下编译Mac, Linux平台的64位可执行程序：12SET CGO_ENABLED=0SET GOOS=darwin3 SET GOARCH=amd64 go build test.goSET CGO_ENABLED=0 SET GOOS=linux SET GOARCH=amd64 go build test.go 9. 斐波那契数列1 递归写法(n 越大 就贼慢)1234func Fib(n int ) int &#123; if n &lt;=1 &#123; return 1 &#125; return fib(n-1)+fib(n-2)&#125; 2 循环的写法12345678910func Fib2(n int ) int &#123; f := [3]int&#123;0,1,0&#125; for i := 2 ; i &lt; n ; i++ &#123; f[2] = f[0] + f[1] f[0] = f[1] f[1] = f[2] &#125; return f[2]&#125; 3 匿名函数写法12345678910func Fib3() func() int &#123; var n,m int return func() int &#123; if n &lt;= 1 &#123; n = 1 &#125; n,m = n+m,n return m &#125;&#125; BenchmarkFibX 测试如下1234567891011121314151617181920212223242526272829303132func BenchmarkFib(b *testing.B) &#123; for i:=0 ; i &lt; b.N ; i++ &#123; for i:= 0 ; i &lt; 30 ; i++ &#123; Fib(i) &#125; &#125;&#125;func BenchmarkFib2(b *testing.B) &#123; for i:=0 ; i &lt; b.N ; i++ &#123; for i:= 0 ; i &lt; 20000 ; i++ &#123; Fib2(i) &#125; &#125;&#125;func BenchmarkFib3(b *testing.B) &#123; for i:=0 ; i &lt; b.N ; i++ &#123; for i:= 0 ; i &lt; 20000 ; i++ &#123; Fib3() &#125; &#125;&#125;//结果goos: darwingoarch: amd64pkg: github.com/zhangzw001/go-example/go圣经/05/5.6/fibBenchmarkFib-4 100 10680039 ns/opBenchmarkFib2-4 3 481973439 ns/opBenchmarkFib3-4 750 1567117 ns/opPASS]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s升级(1.10->1.15)]]></title>
    <url>%2F2020%2F05%2F18%2F48-k8s%E5%8D%87%E7%BA%A7-1-10-1-15%2F</url>
    <content type="text"><![CDATA[k8s升级一般不能跨版本升级, 所以这里间断介绍升级过程, 每次升级一个大版本 k8s升级kubernetes集群版本升级攻略kuboard网址k8s升级攻略官方kubeadm升级文档官方k8s版本changelog 版本关系Kubernetes 1.18.0 –&gt;Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09, 19.03Kubernetes 1.17.0 –&gt;Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09, 19.03Kubernetes 1.16.0 –&gt;Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09Kubernetes 1.15.0 –&gt;Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09Kubernetes 1.14.0 –&gt;Docker版本1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09Kubernetes 1.13.0 –&gt;Docker版本1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06Kubernetes 1.12.0 –&gt;Docker版本1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06Kubernetes 1.11.* –&gt;Docker版本1.11.2 to 1.13.1 and 17.03.x (ref)Kubernetes 1.10.* –&gt;Docker版本1.11.2 to 1.13.1 and 17.03.x (ref) k8s1.10 -> k8s1.11 k8s1.10 -&gt; k8s1.11 注意我这里是单机,所以没有禁止调度,集群在升级前需要禁止调度: kubectl taint node master-01 node-role.kubernetes.io/master=””:NoExecutekubectl taint node –all node-role.kubernetes.io/master- 注意备份etcd 12345export ETCDCTL_API=3# 备份etcdctl snapshot save backup-$(date +%Y%m%d_%H).db# 恢复etcdctl snapshot restore backup-xxxx.db --data-dir=/data/etcd/data 更稳妥的方法应该是先 k8s1.10 -&gt; k8s1.10.13(小版本最后一个版本) -&gt; k8s1.11.0 kubeadm升级准备12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 保存配置export k8s_old_version=1.10.0export k8s_version=1.11.0export kubeadm_config=kubeadmin-$&#123;k8s_old_version&#125;-view.confkubeadm config view &gt; $&#123;kubeadm_config&#125;# k8s1.10版本要求cni必须是低于kubernetes-cni-0.6.0-0版本yum makecache allversion=$(yum list kubeadm --showduplicates | sort -r|grep $&#123;k8s_version&#125;|awk '&#123;print $2&#125;')echo $version# 这里执行不会升级kubeletyum install kubeadm-$&#123;version&#125; --disableexcludes=kubernetes -y# 查看是否可以更新kubeadm upgrade plan[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'I0520 09:32:03.264259 30047 feature_gate.go:230] feature gates: &amp;&#123;map[]&#125;[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.10.0[upgrade/versions] kubeadm version: v1.11.0[upgrade/versions] WARNING: Couldn't fetch latest stable version from the internet: unable to get URL "https://dl.k8s.io/release/stable.txt": Get https://storage.googleapis.com/kubernetes-release/release/stable.txt: dial tcp 34.64.4.80:443: i/o timeout[upgrade/versions] WARNING: Falling back to current kubeadm version as latest stable version[upgrade/versions] Latest version in the v1.10 series: v1.10.13External components that should be upgraded manually before you upgrade the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEEtcd 3.3.11 3.1.12Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEKubelet 1 x v1.10.1 v1.10.13Upgrade to the latest version in the v1.10 series:COMPONENT CURRENT AVAILABLEAPI Server v1.10.0 v1.10.13Controller Manager v1.10.0 v1.10.13Scheduler v1.10.0 v1.10.13Kube Proxy v1.10.0 v1.10.13CoreDNS 1.0.6 1.1.3You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.10.13_____________________________________________________________________External components that should be upgraded manually before you upgrade the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEEtcd 3.3.11 3.2.18Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEKubelet 1 x v1.10.1 v1.11.0Upgrade to the latest stable version:COMPONENT CURRENT AVAILABLEAPI Server v1.10.0 v1.11.0Controller Manager v1.10.0 v1.11.0Scheduler v1.10.0 v1.11.0Kube Proxy v1.10.0 v1.11.0CoreDNS 1.0.6 1.1.3You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.11.0_____________________________________________________________________ 执行升级操作12345678910111213141516kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; --dry-run# 修改下kubeadmin-10-view.conf配置 imageRepository: registry.cn-hangzhou.aliyuncs.com/k8sthvim $&#123;kubeadm_config&#125;imageRepository: registry.cn-hangzhou.aliyuncs.com/k8sthdocker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-proxy-amd64:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-controller-manager-amd64:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-scheduler-amd64:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/kube-apiserver-amd64:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/k8sth/coredns:1.1.3kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; 升级kubelet kubectl123456789101112yum install -y kubectl-$&#123;version&#125; kubelet-$&#123;version&#125; --disableexcludes=kubernetes# 修改下kubelet配置(否则集群会报错)vim /var/lib/kubelet/kubeadm-flags.env增加--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1systemctl daemon-reloadsystemctl stop kubeletsystemctl start kubeletkubectl versionkubelet --versionkubectl get nodes 这里执行完, 我的pod mysql和redis等服务器并没有被重启 k8s1.11 -> k8s1.12 k8s1.11 -&gt; k8s1.121234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 在这之前先停一下指标 metrics-server 和 prometheus-adapter (我这里会导致api-server无法启动成功,找不到metrics.k8s.io/v1beta1 等)kubectl delete -f /data/k8s-config/prometheus/prometheus-adapter kubectl delete -f /data/k8s-config/metrics-server/metrics-server-0.3.2/deploy/1.8+/# 保存配置export k8s_old_version=1.11.0export k8s_version=1.12.0export kubeadm_config=kubeadmin-$&#123;k8s_old_version&#125;-view.confkubeadm config view &gt; $&#123;kubeadm_config&#125;# 或者通过configmap (注意是 ClusterConfiguration)kubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.MasterConfiguration&#125; &gt; $&#123;kubeadm_config&#125;yum makecache allversion=$(yum list kubeadm --showduplicates | sort -r|grep $&#123;k8s_version&#125;|awk '&#123;print $2&#125;')echo $versionyum install -y kubeadm-$&#123;version&#125; --disableexcludes=kuberneteskubeadm upgrade planCOMPONENT CURRENT AVAILABLEEtcd 3.3.11 3.2.24Upgrade to the latest version in the v1.11 series:COMPONENT CURRENT AVAILABLEAPI Server v1.11.0 v1.12.0Controller Manager v1.11.0 v1.12.0Scheduler v1.11.0 v1.12.0Kube Proxy v1.11.0 v1.12.0CoreDNS 1.1.3 1.2.2#修改image地址(之前是registry.cn-hangzhou.aliyuncs.com/k8sth)vim $&#123;kubeadm_config&#125;imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerskubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; --dry-rundocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.2docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.2.24kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125;yum install -y kubelet-$&#123;version&#125; kubectl-$&#123;version&#125; --disableexcludes=kubernetessystemctl daemon-reloadsystemctl restart kubeletkubectl versionkubelet --versionkubectl get nodes k8s1.12 -> k8s1.13 k8s1.12 -&gt; k8s1.131234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 保存配置export k8s_old_version=1.12.0export k8s_version=1.13.0export kubeadm_config=kubeadmin-$&#123;k8s_old_version&#125;-view.conf# 通过configmapkubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.ClusterConfiguration&#125; &gt; $&#123;kubeadm_config&#125;yum makecache allversion=$(yum list kubeadm --showduplicates | sort -r|grep $&#123;k8s_version&#125;|awk '&#123;print $2&#125;')echo $versionyum install -y kubeadm-$&#123;version&#125; --disableexcludes=kuberneteskubeadm upgrade planExternal components that should be upgraded manually before you upgrade the control plane with 'kubeadm upgrade apply':COMPONENT CURRENT AVAILABLEEtcd 3.3.11 3.2.24Upgrade to the latest stable version:COMPONENT CURRENT AVAILABLEAPI Server v1.12.0 v1.13.0Controller Manager v1.12.0 v1.13.0Scheduler v1.12.0 v1.13.0Kube Proxy v1.12.0 v1.13.0CoreDNS 1.2.2 1.2.6kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; --dry-rundocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125;yum install -y kubelet-$&#123;version&#125; kubectl-$&#123;version&#125; --disableexcludes=kubernetessystemctl daemon-reloadsystemctl restart kubeletkubectl versionkubelet --versionkubectl get nodes k8s1.13 -> k8s1.14 k8s1.13 -&gt; k8s1.1412345678910111213141516171819202122232425262728293031323334353637383940# 保存配置export k8s_old_version=1.13.0export k8s_version=1.14.0export kubeadm_config=kubeadmin-$&#123;k8s_old_version&#125;-view.conf# config view或者通过configmap(2选1)kubeadm config view &gt; $&#123;kubeadm_config&#125;kubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.ClusterConfiguration&#125; &gt; $&#123;kubeadm_config&#125;yum makecache allversion=$(yum list kubeadm --showduplicates | sort -r|grep $&#123;k8s_version&#125;|awk '&#123;print $2&#125;')echo $version# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) # (这里1.14会依赖kubernetes-cni:0.7.5-0)yum install -y kubeadm-$&#123;version&#125; kubelet-$&#123;version&#125; kubectl-$&#123;version&#125; --disableexcludes=kuberneteskubeadm upgrade plankubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; --dry-rundocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125;systemctl daemon-reloadsystemctl restart kubeletkubectl versionkubelet --versionkubectl get nodes k8s1.14 -> k8s1.15 k8s1.14 -&gt; k8s1.151234567891011121314151617181920212223242526272829303132333435# 保存配置export k8s_old_version=1.14.0export k8s_version=1.15.0export kubeadm_config=kubeadmin-$&#123;k8s_old_version&#125;-view.conf# config view或者通过configmap(2选1)kubeadm config view &gt; $&#123;kubeadm_config&#125;kubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.ClusterConfiguration&#125; &gt; $&#123;kubeadm_config&#125;yum makecache allversion=$(yum list kubeadm --showduplicates | sort -r|grep $&#123;k8s_version&#125;|awk '&#123;print $2&#125;')# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) yum install -y kubeadm-$&#123;version&#125; kubelet-$&#123;version&#125; kubectl-$&#123;version&#125; --disableexcludes=kuberneteskubeadm upgrade plankubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; --dry-rundocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125;systemctl daemon-reloadsystemctl restart kubeletkubectl versionkubelet --versionkubectl get nodes k8s1.15.0 -> k8s1.15.11 k8s1.15.0 -&gt; k8s1.15.11 (选择11是因为google_containers/kube-proxy:v1.15.12 not found)123456789101112131415161718192021222324252627282930313233343536373839404142434445# 保存配置export k8s_old_version=1.15.0export k8s_version=1.15.11export kubeadm_config=kubeadmin-$&#123;k8s_old_version&#125;-view.conf# config view或者通过configmap(2选1)kubeadm config view &gt; $&#123;kubeadm_config&#125;kubectl get configmap -n kube-system kubeadm-config -o jsonpath=&#123;.data.ClusterConfiguration&#125; &gt; $&#123;kubeadm_config&#125;yum makecache allversion=$(yum list kubeadm --showduplicates | sort -r|grep $&#123;k8s_version&#125;|awk '&#123;print $2&#125;')# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) yum install -y kubeadm-$&#123;version&#125; --disableexcludes=kuberneteskubeadm upgrade planCOMPONENT CURRENT AVAILABLEEtcd 3.3.11 3.3.10COMPONENT CURRENT AVAILABLEAPI Server v1.15.0 v1.15.11Controller Manager v1.15.0 v1.15.11Scheduler v1.15.0 v1.15.11Kube Proxy v1.15.0 v1.15.11CoreDNS 1.3.1 1.3.1kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125; --dry-rundocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v$&#123;k8s_version&#125;docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1kubeadm upgrade apply v$&#123;k8s_version&#125; --config $&#123;kubeadm_config&#125;yum install -y kubelet-$&#123;version&#125; kubectl-$&#123;version&#125; --disableexcludes=kubernetessystemctl daemon-reloadsystemctl restart kubeletkubectl versionkubelet --versionkubectl get nodes 升级docker 升级docker (慎用)1234yum install docker-ce-18.09.9 docker-ce-cli-18.09.9# 重启docker, k8s集群会间断(如果是多台, 将重启的节点taint剔除集群重启即可)service docker restart 升级单机版rancher 升级单机版rancher 这里之前是2.1.9版本, 更新为stable(2.4.3) ,我这里rancher只是用导入方式,rancher宕机不影响k8s集群 首先将swarm启动的rancher停用:docker service rm rancher_rancher 其次将data目录备份cp -ra /data/rancher /data/rancher_2.1.9_2020-05-20_bak 修改docker-compose.yml重新启动docker stack deploy -c docker-compose.yml rancher 查看日志docker logs -f $(docker ps|grep rancher_rancher|awk ‘{print $1}’) 恢复(停止服务,将备份的目录还原重新启动即可) docker-compose.yml 123456789101112131415161718version: '3'services: rancher: hostname: rancher image: rancher/rancher:stable deploy: replicas: 1 restart_policy: condition: on-failure volumes: - /data/rancher/data/:/var/lib/rancher/ ports: - 10080:80 - 10443:443 networks: - rancher_netnetworks: rancher_net: 问题 问题1. 版本选择的image配置问题1开始选择升级到k8s1.11.10版本, 但是镜像版本只有registry.cn-hangzhou.aliyuncs.com/k8sth/kube-proxy-amd64:v1.11.0 , 没有找到v1.11.10 2. 1.12版本开始阿里云镜像地址变化12registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.12.0registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 3. 如果执行升级中断或停止, 可以1kubeadm upgrade --force。 4. 如果跨版本升级12- Specified version to upgrade to "v1.16.2" is too high; kubeadm can upgrade only 1 minor version at a time- Specified version to upgrade to "v1.16.2" is at least one minor release higher than the kubeadm minor release (16 &gt; 11). Such an upgrade is not supported 5. k8s1.11 开启使用了pause:3.1123k8s1.11开始kubelet的配置文件修改为:vim /var/lib/kubelet/kubeadm-flags.env增加--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 6. 在1.12-&gt;1.13升级完成后报错: OpenAPI spec for “v1beta1.metrics.k8s.io” failed with: OpenAPI spec does not exists1234567891011121314# 1.11 -&gt; 1.12 k8s_kube-apiserver日志E0520 06:21:09.369346 1 memcache.go:134] couldn't get resource list for crd.projectcalico.org/v1: the server could not find the requested resourceE0520 06:21:09.369725 1 memcache.go:134] couldn't get resource list for authentication.istio.io/v1alpha1: the server could not find the requested resourceE0520 06:21:09.370842 1 memcache.go:134] couldn't get resource list for certmanager.k8s.io/v1alpha1: the server could not find the requested resourceE0520 06:21:09.371925 1 memcache.go:134] couldn't get resource list for rbac.istio.io/v1alpha1: the server could not find the requested resourceE0520 06:21:09.373016 1 memcache.go:134] couldn't get resource list for config.istio.io/v1alpha2: the server could not find the requested resourceE0520 06:21:09.374083 1 memcache.go:134] couldn't get resource list for networking.istio.io/v1alpha3: the server could not find the requested resourceE0520 06:21:09.375179 1 memcache.go:134] couldn't get resource list for metrics.k8s.io/v1beta1: the server could not find the requested resource# 1.12-&gt;1.13 k8s_kube-apiserver日志OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: OpenAPI spec does not exists原因是我这边metrics-server 和prometheus-adapter 部署的时候用到了metrics.k8s.io/v1beta1但是升级之后这个api-version丢失了, 所以导致apiserver一直报错, 无法启动成功 7. 单机k8s升级会造成服务中断1234567891011# 1.10 ~ 1.13升级会导致核心组件apiserver,scheduler,controller,coredns,网络calico等由于版本升级都需要重新启动了一次, 部分其他业务pod也会重启其中nodejs服务会pm2重启,mysql等也会重启 ,会导致服务器压力上升这里检测到有状态statefulset的服务并没有重启, 而无状态的deployment会重启, 由于单机1个pod的服务重启会导致中断所以避免业务中断必须是多主机集群, 在升级之前先taint机器升级# 1.13 -&gt; 1.14这期间有状态statefulset的服务也重启了# 1.14 ~ 1.15.11这期间有状态statefulset的服务没有重启 8. 注意1.13-&gt;1.14的时候安装kubeadm可能会依赖kubelet自动安装了最新版本123# 一起安装是因为kubeadm安装时会依赖kubelet, kubelet会安装最新版本(1.18.2) # (这里1.14会依赖kubernetes-cni:0.7.5-0)yum install -y kubeadm-$&#123;version&#125; kubelet-$&#123;version&#125; kubectl-$&#123;version&#125; --disableexcludes=kubernetes]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s安装promethues+alertmanager+grafana]]></title>
    <url>%2F2020%2F05%2F14%2F47-k8s%E5%AE%89%E8%A3%85promethues-alertmanager-grafana%2F</url>
    <content type="text"><![CDATA[本文主要是针对prometheus 简单部署 , 考虑到测试资源有限,为更精简自定义按照监控,减少多余资源占用, 也同时更了解prometheus的更多细节, 这里没有使用prometheus-operator 1 K8s上部署原生的prometheus 2 prometheus-operator 方式部署 3 docker-compose快速搭建 Prometheus+Grafana监控系统 4 一套prometheus监控多个k8s集群,详细讲解配置 5 使用prometheus监控traefik、redis、k8s集群各节点、各节点kubelet 6 grafana 监控模板下载 搭建prometheus创建ns123456789tee ns.yml &lt;&lt;- EOFapiVersion: v1kind: Namespacemetadata: name: monitoringEOFkubectl apply -f ns.yml prometheus-clusterRole.yml1234567891011121314151617tee prometheus-clusterRole.yml &lt;&lt;- EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheus-k8srules: - apiGroups: - "" resources: - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - getEOF prometheus-clusterRoleBinding.yml1234567891011121314tee prometheus-clusterRoleBinding.yml &lt;&lt;- EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheus-k8sroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus-k8ssubjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoringEOF prometheus-serviceAccount.yml1234567tee prometheus-serviceAccount.yml &lt;&lt;- EOFapiVersion: v1kind: ServiceAccountmetadata: name: prometheus-k8s namespace: monitoringEOF 创建角色123kubectl apply -f prometheus-clusterRole.ymlkubectl apply -f prometheus-clusterRoleBinding.ymlkubectl apply -f prometheus-serviceAccount.yml 这样我们就创建了一个 ServiceAccount，名为 prometheus-k8s，这个 ServiceAccount 不仅现在可以用来获取 kubelet 的监控指标，后续 Prometheus 也会使用这个 serviceAccount 启动。 创建完成后，会自动在生成一个 secret，里面包含了 token：123kubectl get secret -n monitoringNAME TYPE DATA AGEprometheus-k8s-token-6v9m9 kubernetes.io/service-account-token 3 13s 获取token12token=$(kubectl get secret prometheus-k8s-token-6v9m9 -n monitoring -o json|jq -r '.data.token'|base64 -d)token=$(kubectl get secret prometheus-k8s-token-pl2wx -n monitoring -o json|jq -r '.data.token'|base64 -d) 通过token查看metrics1curl https://127.0.0.1:10250/metrics/cadvisor -k -H "Authorization: Bearer $token" kubelet 除了 /metrics/cadvisor 这个 url 之外，还有一个 /metrics，这是它本身的监控指标而非 pod 的 查看etc指标页面etcd 的指标页面的 url 也是 /metrics，但是你想要访问它需要提供证书，因为它会验证客户端证书。当然你可以在它的启动参数中通过 –listen-metrics-urls http://ip:port 让监控指标页使用 http 而非 https，这样就不用提供证书了。etcd 虽然部署在容器中，但是由于使用了 hostNetwork，所以我们可以通过直接访问 master 的 2379 端口访问它。默认它会采用了 https，因此我们需要提供它的 peer 证书。如果 k8s 是使用 kubeadm 安装的，etcd 的证书在 /etc/kubernetes/pki/etcd/ 目录下。因此访问 etcd 的命令为：curl https://127.0.0.1:2379/metrics –cacert /etc/kubernetes/pki/etcd/ca.crt –cert /etc/kubernetes/pki/etcd/healthcheck-client.crt –key /etc/kubernetes/pki/etcd/healthcheck-client.key复制代码后面我们需要将这三个文件挂载到 Prometheus 容器中，以便它能收集 etcd 监控数据。 如果你并非容器部署的etcd,请使用你的etcd端口访问 安装 Prometheus我们先创建两个 configmap，一个是 Prometheus 的配置文件，另一个是告警的规则文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152tee prometheus-configmap.yml &lt;&lt;- EOFapiVersion: v1data: prometheus.yml: | global: evaluation_interval: 30s scrape_interval: 30s external_labels: prometheus: monitoring/k8s rule_files: - /etc/prometheus/rules/*.yml scrape_configs: - job_name: prometheus honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - monitoring scrape_interval: 30s relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_prometheus regex: k8s - source_labels: - __meta_kubernetes_endpoint_address_target_kind - __meta_kubernetes_endpoint_address_target_name separator: ; regex: Pod;(.*) replacement: $&#123;1&#125; target_label: pod - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_pod_name target_label: pod - source_labels: - __meta_kubernetes_service_name target_label: job replacement: $&#123;1&#125; - target_label: endpoint replacement: webkind: ConfigMapmetadata: name: prometheus namespace: monitoringEOF 首先看这段配置 123456kubernetes_sd_configs:- role: endpoints namespaces: names: - monitoringscrape_interval: 30s 这段配置使用的是 endpoint 的方式对 Prometheus 本身进行自动发现，你可以有疑问了，为什么不直接对自身的 127.0.0.1:9090 进行采集呢？因为考虑到 Prometheus 可能会有多台，这样即使有多台，它们也都在一个 job 下面。 kubernetes_sd_configs 配置可以自动发现 k8s 中 node、service、pod、endpoint、ingress，并为其添加监控, 详见: kubernetes_sd_configs官方文档 再看下面这段配置 1234- action: keep source_labels: - __meta_kubernetes_service_label_prometheus regex: k8s 表示并非所有的endpoint 都会被抓取, 这里只抓取label 是 prometheus=k8s的标签, 所以只会监控prometheus的endpoint 默认不指定url 就是/metrics 接着看这段配置 1234567- source_labels: - __meta_kubernetes_endpoint_address_target_kind - __meta_kubernetes_endpoint_address_target_name separator: ; regex: Pod;(.*) replacement: $&#123;1&#125; target_label: pod 如果 meta_kubernetes_endpoint_address_target_kind 的值为 Pod，meta_kubernetes_endpoint_address_target_name 的值为 prometheus-0，在它们之间加上一个 ; 之后，它们合起来就是 Pod;prometheus-0。使用正则表达式 Pod;(.*) 对其进行匹配，那么 ${1} 就是取第一个分组，它值就是 prometheus-0，最后将这个值交给 pod 这个标签。因此这一段配置就是为所有采集到的监控指标增加一个 pod=prometheus-0 的标签。 以上配置其实可以去掉, 这里prometheus=k8s的匹配条件以及唯一,这段kind和name配置去掉影响不大 创建configmap1kubectl apply -f prometheus-configmap.yml Prometheus 规则文件 (后面会更新)123456789101112tee prometheus-config-rulefiles.yml &lt;&lt;- EOFapiVersion: v1data: k8s.yml: ""kind: ConfigMapmetadata: name: prometheus-rulefiles namespace: monitoringEOFkubectl apply -f prometheus-config-rulefiles.yml role 和 roleBinding因为 Prometheus 会使用之前创建的 sa（serviceAccount）prometheus-k8s 运行，那么光现在 prometheus-k8s 这个 sa 的权限是没有办法查看 service 以及 endpoint 的。我们使用 kubernetes_sd_config 主要会使用 endpoint 进行发现，因此 prometheus-k8s 必须具备更多的权限。我们需要创建更多的 role，并通过 roleBinding 将这些权限绑定到 prometheus-k8s 这个 sa 上，之所以不使用 clusterRole 是为了权限最小化。这里会创建 prometheus-roleConfig.yml、prometheus-roleBindingConfig.yml、prometheus-roleSpecificNamespaces.yml、prometheus-roleBindingSpecificNamespaces.yml 这四个文件，它们的内容如下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130tee prometheus-roleConfig.yml &lt;&lt;- EOFapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: prometheus-k8s-config namespace: monitoringrules: - apiGroups: - "" resources: - configmaps verbs: - getEOFtee prometheus-roleBindingConfig.yml &lt;&lt;- EOFapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: prometheus-k8s-config namespace: monitoringroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s-configsubjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoringEOFtee prometheus-roleSpecificNamespaces.yml &lt;&lt;- EOFapiVersion: rbac.authorization.k8s.io/v1kind: RoleListitems: - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: default rules: - apiGroups: - "" resources: - services - endpoints - pods verbs: - get - list - watch - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: kube-system rules: - apiGroups: - "" resources: - services - endpoints - pods verbs: - get - list - watch - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: prometheus-k8s namespace: monitoring rules: - apiGroups: - "" resources: - services - endpoints - pods verbs: - get - list - watchEOFtee prometheus-roleBindingSpecificNamespaces.yml &lt;&lt;- EOFapiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingListitems: - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: prometheus-k8s namespace: monitoring roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoringEOF 上面的权限中，config 是用来读 configmap 的，后面的就是 Prometheus 用来进行 k8s 发现时必须要的权限了，最后使用 rulebinding 将这些所有的权限都绑定到 prometheus-k8s 这个 sa 上。 12345678910kubectl delete -f prometheus-roleBindingConfig.ymlkubectl delete -f prometheus-roleBindingSpecificNamespaces.ymlkubectl delete -f prometheus-roleConfig.ymlkubectl delete -f prometheus-roleSpecificNamespaces.ymlkubectl apply -f prometheus-roleBindingConfig.ymlkubectl apply -f prometheus-roleBindingSpecificNamespaces.ymlkubectl apply -f prometheus-roleConfig.ymlkubectl apply -f prometheus-roleSpecificNamespaces.yml 手动创建pv (我这里没用上, 用的是storageclass)1234567891011121314151617tee prometheus-pv.yml &lt;&lt;- EOFapiVersion: v1kind: PersistentVolumemetadata: name: prometheus labels: name: prometheusspec: nfs: path: /disk/k8s-nfs-data/k8s-db-t/prometheus server: 172.16.xx.xx accessModes: ["ReadWriteMany", "ReadWriteOnce"] capacity: storage: 50GiEOFkubectl apply -f prometheus-pv.yml 创建 service12345678910111213141516171819202122tee prometheus-service.yml &lt;&lt;- EOFapiVersion: v1kind: Servicemetadata: name: prometheus namespace: monitoring labels: prometheus: k8sspec: clusterIP: None ports: - name: web port: 9090 protocol: TCP targetPort: web selector: app: prometheus type: ClusterIPEOFkubectl apply -f prometheus-service.yml 这里service定义了 app=prometheus 这样的标签选择器，因此 Prometheus 容器StatefulSet的时候必须存在这个标签。 部署 Prometheus123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108tee prometheus-statefulset.yml &lt;&lt;- EOFapiVersion: apps/v1kind: StatefulSetmetadata: labels: app: prometheus prometheus: k8s name: prometheus namespace: monitoringspec: replicas: 1 volumeClaimTemplates: - metadata: name: prometheus-data annotations: volume.beta.kubernetes.io/storage-class: "nfs-retain" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 20Gi revisionHistoryLimit: 10 selector: matchLabels: app: prometheus prometheus: k8s serviceName: prometheus updateStrategy: type: RollingUpdate template: metadata: creationTimestamp: null labels: app: prometheus prometheus: k8s spec: serviceAccount: prometheus-k8s containers: - args: - --web.console.templates=/etc/prometheus/consoles - --web.console.libraries=/etc/prometheus/console_libraries - --config.file=/etc/prometheus/config/prometheus.yml - --storage.tsdb.path=/prometheus - --web.enable-admin-api - --storage.tsdb.retention.time=20d - --web.enable-lifecycle - --storage.tsdb.no-lockfile - --web.external-url=http://promethes-dev.k1s.club/ - --web.route-prefix=/ image: prom/prometheus:v2.11.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 6 httpGet: path: /-/healthy port: web scheme: HTTP periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 name: prometheus ports: - containerPort: 9090 name: web protocol: TCP readinessProbe: failureThreshold: 120 httpGet: path: /-/ready port: web scheme: HTTP periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 resources: requests: memory: 400Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/prometheus/config name: config readOnly: true - mountPath: /prometheus name: prometheus-data #subPath: prometheus-db - mountPath: /etc/prometheus/rules/ name: prometheus-rulefiles - mountPath: /etc/prometheus/secrets/etcd-client-cert name: secret-etcd-client-cert readOnly: true volumes: - name: config configMap: defaultMode: 420 name: prometheus - name: prometheus-rulefiles configMap: defaultMode: 420 name: prometheus-rulefiles - name: secret-etcd-client-cert secret: defaultMode: 420 secretName: etcd-client-certEOFkubectl apply -f prometheus-statefulset.yml 注意下这里如果你并非容器安装的etcd, 则mount的secret-etcd-client-cert可能不存在, 请自行挂载正确的目录, 如果etcd是http访问, 则不需要证书挂载 基础的 statfulset 相关的知识我就不多提了，说几个重点吧： 1 –storage.tsdb.retention.time=20d 这个启动选项表示 Prometheus 所收集的监控数据只保留 20 天，这个值最好不要太大。如果历史数据保存很久，建议写到持久存储中，比如 VictoriaMetrics、thanos、influxdb、opentsdb 等；2 –web.enable-admin-api 这个启动选项表示启动管理员 api，你可以通过 api 对监控数据进行删除等；3 serviceAccount 它的值必须是 prometheus-k8s，不然前面的赋权操作都白干了；4 pod 必须存在 app: prometheus 这个标签，不然无法被前面创建的 service 选择到；5 挂载了两个 configmap、一个 secret 还有一个存储卷。(我这里是采用storageclass, 当然你也可以用上面的手动创建pv) 创建一个ingress12345678910111213141516171819tee prometheus-ingress.yml &lt;&lt;- EOFapiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus namespace: monitoringspec: rules: - host: prometheus-dev.k1s.club http: paths: - path: / backend: serviceName: prometheus servicePort: 9090EOFkubectl apply -f prometheus-ingress.yml 至此, 我们就监控了prometheus本身 问题1 k8s1.10报错rbac权限错误 level=error ts=2020-05-11T10:00:03.091Z caller=klog.go:94 component=k8s_client_runtime func=ErrorDepth msg=”/app/discovery/kubernetes/kubernetes.go:335: Failed to list *v1.Node: nodes is forbidden: User &quot;system:serviceaccount:monitoring:prometheus-k8s&quot; cannot list nodes at the cluster scope” 按照以下rbac增加了权限,则正常 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556prometheus-rbac.ymlapiVersion: v1kind: ServiceAccountmetadata: name: prometheus-k8s namespace: monitoring labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheus-k8s labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilerules: - apiGroups: - "" resources: - nodes - nodes/metrics - services - endpoints - pods verbs: - get - list - watch - apiGroups: - "" resources: - configmaps verbs: - get - nonResourceURLs: - "/metrics" verbs: - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheus-k8s labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus-k8ssubjects:- kind: ServiceAccount name: prometheus-k8s namespace: monitoring 监控etcd如果是容器安装的etcd集群1234567891011121314151617181920tee kube-etcd-service.yml &lt;&lt;- EOFapiVersion: v1kind: Servicemetadata: name: kube-etcd labels: k8s-app: kube-etcd namespace: kube-systemspec: clusterIP: None ports: - name: http-metrics port: 2379 protocol: TCP targetPort: 2379 selector: component: etcd type: ClusterIPEOF 由于 etcd 处于 kube-system 名称空间，所以这里的 namespace 也应该是 kube-system； 因为 etcd pod 本身会存在 component=etcd 这个标签，所以这里的选择器使用的就是这个。 12kubectl apply -f kube-etcd-service.ymlkubectl -n kube-system get endpoints kube-etcd 现在通过这个 endpoint 就能够访问到后面三台 etcd，现在只需要在 Prometheus 中添加对应的配置即可，配置内容如下。 12345678910111213141516171819202122232425262728293031323334- job_name: kube-etcd honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - kube-system scheme: https tls_config: insecure_skip_verify: false ca_file: /etc/prometheus/secrets/etcd-client-cert/ca.crt cert_file: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.crt key_file: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.key relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: kube-etcd - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_pod_name target_label: pod - target_label: endpoint replacement: http-metrics metric_relabel_configs: - action: drop regex: (etcd_debugging|etcd_disk|etcd_request|etcd_server|grpc_server).* source_labels: - __name__ 我这里有一个环境是http访问的, 则直接通过http://172.16.xx.xx:2379/metrics 监控即可123456789- job_name: 'etcd' scrape_interval: 60s static_configs: - targets: ['172.16.xx.xx:2379'] metric_relabel_configs: - action: drop regex: (etcd_debugging|etcd_disk|etcd_request|etcd_server|grpc_server).* source_labels: - __name__ 然后重新加载configmap 1234kubectl apply -f prometheus-configmap.yml# 该配置不用重启prometheus即可重新加载配置curl -XPOST prometheus-dev.k1s.club/-/reload 监控 apiserverapiserver 的监控方式更简单，因为它的 service 已经自动创建了。但你需要注意的是，它的 service 创建在 default 名称空间，名为 kubernetes。 1234567891011121314151617181920212223- job_name: kube-apiserver honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - default scrape_interval: 30s scheme: https tls_config: insecure_skip_verify: false ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: keep source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] separator: ; regex: default;kubernetes;https metric_relabel_configs: - source_labels: - __name__ action: drop regex: (apiserver_storage_data_key_generation_latencies_microseconds_bucket|apiserver_admission_controller_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_summary|apiserver_request_latencies_bucket|apiserver_request_latencies_summary|apiserver_storage_data_key_generation_latencies_microseconds_bucket|rest_client_request_latency_seconds_bucket) 监控 podpod 的监控指标是 kubelet 提供的，前面也已经使用 curl 命令看到了，因此这里也是直接干。prometheus-operator 使用的同样是 endpoints 发现的方式，但是 kubelet 是操作系统的进程，并不是 pod，因此通过创建 service 的方式是不可能创建对应的 endpoint 的，也不知道它为啥可以做到。为了更通用，我们这里是通过 node 发现的方式进行的。使用 node 发现，你无法指定端口，prometheus 会自动访问发现 node 的 10250 端口。 12345678910- job_name: pods honor_labels: true kubernetes_sd_configs: - role: node scrape_interval: 30s metrics_path: /metrics/cadvisor scheme: https tls_config: insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token k8s 的其他组件我就不继续监控了，包括 kubelet、controller manager、coredns 等，它们监控的手段和之前的几个组件都差不多 安装 kube-state-metrics 常见应用使用kube-state-metrics后的常用场景有： 存在执行失败的Job: kube_job_status_failed{job=”kubernetes-service-endpoints”,k8s_app=”kube-state-metrics”}==1 集群节点状态错误: kube_node_status_condition{condition=”Ready”,status!=”true”}==1 集群中存在启动失败的Pod：kube_pod_status_phase{phase=~”Failed|Unknown”}==1 最近30分钟内有Pod容器重启: changes(kube_pod_container_status_restarts[30m])&gt;0配合报警可以更好地监控集群的运行 RBAC 权限 因为它要访问集群内的所有资源，才能将它们的信息提供出去，因此部署它之前，先为它创建一些权限。这些权限都会绑定到一个 serviceAccount 上，然后我们用这个 sa 运行 kube-state-metrics 就行 kube-state-metrics-clusterRole.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: kube-state-metricsrules: - apiGroups: - "" resources: - configmaps - secrets - nodes - pods - services - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: - list - watch - apiGroups: - extensions resources: - daemonsets - deployments - replicasets - ingresses verbs: - list - watch - apiGroups: - apps resources: - statefulsets - daemonsets - deployments - replicasets verbs: - list - watch - apiGroups: - batch resources: - cronjobs - jobs verbs: - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - list - watch - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - certificates.k8s.io resources: - certificatesigningrequests verbs: - list - watch kube-state-metrics-clusterRoleBinding.yml12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kube-state-metricsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metricssubjects: - kind: ServiceAccount name: kube-state-metrics namespace: monitoring kube-state-metrics-role.yml123456789101112131415161718192021222324252627282930apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: kube-state-metrics namespace: monitoringrules: - apiGroups: - "" resources: - pods verbs: - get - apiGroups: - extensions resourceNames: - kube-state-metrics resources: - deployments verbs: - get - update - apiGroups: - apps resourceNames: - kube-state-metrics resources: - deployments verbs: - get - update kube-state-metrics-roleBinding.yml12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kube-state-metrics namespace: monitoringroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kube-state-metricssubjects: - kind: ServiceAccount name: kube-state-metrics kube-state-metrics-serviceAccount.yml123456apiVersion: v1kind: ServiceAccountmetadata: name: kube-state-metrics namespace: monitoring 12345kubectl apply -f kube-state-metrics-clusterRole.ymlkubectl apply -f kube-state-metrics-clusterRoleBinding.ymlkubectl apply -f kube-state-metrics-role.ymlkubectl apply -f kube-state-metrics-roleBinding.ymlkubectl apply -f kube-state-metrics-serviceAccount.yml deployment 和 service kube-state-metrics 会提供两个指标页面，一个是暴露集群内资源的，另一个是它自身的，它自身的可以选择性的关注 kube-state-metrics-deployment.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: apps/v1kind: Deploymentmetadata: labels: app: kube-state-metrics name: kube-state-metrics namespace: monitoringspec: replicas: 1 selector: matchLabels: app: kube-state-metrics template: metadata: labels: app: kube-state-metrics spec: containers: - args: - --port=10000 - --telemetry-port=10001 image: quay.io/coreos/kube-state-metrics:v1.6.0 name: kube-state-metrics resources: limits: cpu: 100m memory: 150Mi requests: cpu: 100m memory: 150Mi - command: - /pod_nanny - --container=kube-state-metrics - --cpu=100m - --extra-cpu=2m - --memory=150Mi - --extra-memory=30Mi - --threshold=5 - --deployment=kube-state-metrics env: - name: MY_POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: k8s.gcr.io/addon-resizer:1.8.4 name: addon-resizer resources: limits: cpu: 50m memory: 30Mi requests: cpu: 10m memory: 30Mi nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 65534 serviceAccountName: kube-state-metrics 指定了两个启动参数，也就是两个端口，其中 10000 是暴露集群资源指标的端口，10001 就是它自身了。除了 kube-state-metrics 之外，还启动了 addon-resizer 这个容器 最后是 service 文件 kube-state-metrics-service.yml123456789101112131415161718apiVersion: v1kind: Servicemetadata: labels: k8s-app: kube-state-metrics name: kube-state-metrics namespace: monitoringspec: clusterIP: None ports: - name: http-main port: 10000 targetPort: 10000 - name: http-self port: 10001 targetPort: 10001 selector: app: kube-state-metrics 12docker pull registry.cn-beijing.aliyuncs.com/minminmsn/addon-resizer:1.8.4docker tag registry.cn-beijing.aliyuncs.com/minminmsn/addon-resizer:1.8.4 k8s.gcr.io/addon-resizer:1.8.4 12kubectl apply -f kube-state-metrics-deployment.ymlkubectl apply -f kube-state-metrics-service.yml 两个端口都暴露出来，你可以都收集或者只收集 10000 端口。如果只收集 10000，你可以只暴露一个端口，也可以两个都暴露，然后在 Prometheus 配置中过滤掉一个端口即可。 收集监控数据将上面所有的文件都 apply 之后，就可以直接配置 Prometheus 进行收集了。在此之前，你可以使用 curl 命令访问它的指标页面，看看里面都有啥： 123456kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools -n monitoring# 首先看一下健康情况curl kube-state-metrics:10000/healthz# 在看看指标 (这里有非常多指标)curl kube-state-metrics:10000/metrics 修改下prometheus-configmap.yml 文件1234567891011121314151617181920212223242526- job_name: kube-state-metrics honor_labels: true kubernetes_sd_configs: - role: endpoints namespaces: names: - monitoring scrape_interval: 30s scrape_timeout: 30s tls_config: insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: kube-state-metrics - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http-main metric_relabel_configs: - source_labels: - __name__ regex: (kube_daemonset_status_number_ready|kube_daemonset_status_number_unavailable|kube_deployment_status_replicas_unavailable|kube_deployment_spec_paused|kube_deployment_spec_strategy_rollingupdate_max_surge|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_endpoint_address_available|kube_endpoint_address_not_ready|kube_node_info|kube_node_spec_unschedulable|kube_node_status_condition|kube_node_status_capacity|kube_node_status_capacity|kube_node_status_allocatable|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_status_phase|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolume_status_phase|kube_persistentvolume_info|kube_persistentvolume_capacity_bytes|kube_pod_info|kube_pod_status_phase|kube_pod_status_ready|kube_pod_container_info|kube_pod_container_status_waiting|kube_pod_container_status_waiting_reason|kube_pod_container_status_running|kube_pod_container_status_terminated_reason|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|kube_service_info|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready) action: keep 这里是只关注匹配到的指标, 其他指标忽略(白名单) 123kubectl apply -f prometheus-configmap.yml# 该配置不用重启prometheus即可重新加载配置curl -XPOST prometheus-dev.k1s.club/-/reload 配置一个grafana首先查看一下你是否配置了storageclass1234kubectl get storageclassNAME PROVISIONER AGEnfs (default) nfs.com/nfs-ssd 248dnfs-retain nfs.com/nfs-ssd 248d k8s-StatefulSet_grafana.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273---apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: grafana-dev namespace: monitoringspec: serviceName: "grafana-dev" updateStrategy: type: RollingUpdate replicas: 1 volumeClaimTemplates: - metadata: name: grafana-data annotations: volume.beta.kubernetes.io/storage-class: "nfs-retain" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 5Gi template: metadata: labels: app: grafana-dev spec: containers: - name: grafana-dev image: grafana/grafana ports: - containerPort: 3000 name: grafana-port resources: requests: cpu: "50m" limits: cpu: "512m" volumeMounts: - mountPath: /var/lib/grafana name: grafana-data---kind: ServiceapiVersion: v1metadata: labels: app: grafana-dev name: grafana-dev-service namespace: monitoringspec: clusterIP: None type: ClusterIP ports: - port: 3000 name: grafana-port protocol: TCP targetPort: 3000 selector: app: grafana-dev---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: grafana-dev namespace: monitoringspec: rules: - host: grafana-dev.k1s.club http: paths: - path: / backend: serviceName: grafana-dev-service servicePort: 3000 通过 k8s grafana dashboard模板 监控效果如下 通过以上的经验, 如果我希望通过A集群的Prometheus 来监控 B集群, 所以一些指标的可通过token访问B api12345678910111213141516# 查看B集群的apikubectl cluster-infokubectl create serviceaccount --namespace=monitoring prometheus-devkubectl create clusterrolebinding prometheus-k8s --clusterrole=cluster-admin --serviceaccount=monitoring:prometheus-dev# 创建一个叫admin的serviceaccountkubectl -n kube-system create serviceaccount admin# 给这个admin的serviceaccount绑上cluser-admin的clusterrolekubectl -n kube-system create clusterrolebinding sa-cluster-admin --serviceaccount kube-system:admin --clusterrole cluser-admin# 查询admin的secretkubectl -n kube-system get serviceaccounts admin -o json|jq -r '.secrets[0].name'admin-token-h4zz4# 查看tokenkubectl get secret admin-token-h4zz4 -n kube-system -o json|jq -r ".data.token"|base64 -d &gt;&gt;/etc/kubernetes/pki/admin-token-9dg92 然后在A集群的prometheus配置如下123456789101112131415161718192021222324252627282930- job_name: k8s_B-kube-state-metrics honor_labels: true kubernetes_sd_configs: - role: endpoints api_server: https://172.16.xx.xx:6443 tls_config: insecure_skip_verify: true bearer_token: 'eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwcm9tZXRoZXVzLWRldi10b2tlbi1zZ3pidiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJwcm9tZXRoZXVzLWRldiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjliODI5NTYwLTkzNGYtMTFlYS1hMGE3LTE4NjZkYWY0NjY3NCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpwcm9tZXRoZXVzLWRldiJ9.xa8gDA0lwEi_NDGzCL3JSsZUsUD7gKiF0sfFofykyAlEYcjnPmPaksduHWzRKaUJhkvgAJN5Jl3pt8-wplQUJggGAaPVdqJVTYISi4QkPcLkDInoYm8p3OeRgvNpQJJ0VID8zp0-RBWoYe8bAh-7qT6JInt308AA-21vzDKDHtj3aa8Re1nuBxB7f0omNKcAhW0R04p59jshg95HRSBXbVQe7gX6NBjgaOWqj5i0MkKL6k2hdFKdQYgjhQjRAZmXL6F0Qx197y3HAw4zmrUPG-13RcXk38X5F4K8CWtHdOvrqUZxolaWBWin8n73Sr87KyFcEu8YA2oJbzvCKzy9Kg' namespaces: names: - monitoring scrape_interval: 30s scrape_timeout: 30s tls_config: insecure_skip_verify: true bearer_token: 'eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwcm9tZXRoZXVzLWRldi10b2tlbi1zZ3pidiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJwcm9tZXRoZXVzLWRldiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjliODI5NTYwLTkzNGYtMTFlYS1hMGE3LTE4NjZkYWY0NjY3NCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpwcm9tZXRoZXVzLWRldiJ9.xa8gDA0lwEi_NDGzCL3JSsZUsUD7gKiF0sfFofykyAlEYcjnPmPaksduHWzRKaUJhkvgAJN5Jl3pt8-wplQUJggGAaPVdqJVTYISi4QkPcLkDInoYm8p3OeRgvNpQJJ0VID8zp0-RBWoYe8bAh-7qT6JInt308AA-21vzDKDHtj3aa8Re1nuBxB7f0omNKcAhW0R04p59jshg95HRSBXbVQe7gX6NBjgaOWqj5i0MkKL6k2hdFKdQYgjhQjRAZmXL6F0Qx197y3HAw4zmrUPG-13RcXk38X5F4K8CWtHdOvrqUZxolaWBWin8n73Sr87KyFcEu8YA2oJbzvCKzy9Kg' relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: kube-state-metrics - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http-main metric_relabel_configs: - source_labels: - __name__ regex: (kube_daemonset_status_number_ready|kube_daemonset_status_number_unavailable|kube_deployment_status_replicas_unavailable|kube_deployment_spec_paused|kube_deployment_spec_strategy_rollingupdate_max_surge|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_endpoint_address_available|kube_endpoint_address_not_ready|kube_node_info|kube_node_spec_unschedulable|kube_node_status_condition|kube_node_status_capacity|kube_node_status_capacity|kube_node_status_allocatable|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_status_phase|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolume_status_phase|kube_persistentvolume_info|kube_persistentvolume_capacity_bytes|kube_pod_info|kube_pod_status_phase|kube_pod_status_ready|kube_pod_container_info|kube_pod_container_status_waiting|kube_pod_container_status_waiting_reason|kube_pod_container_status_running|kube_pod_container_status_terminated_reason|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|kube_service_info|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_deployment_status_replicas_available|kube_deployment_status_replicas|kube_node_status_allocatable_memory_bytes|kube_deployment_status_replicas|kube_statefulset_replicas|kube_daemonset_status_desired_number_scheduled|kube_statefulset_status_replicas|changes|kube_job_status_failed) action: keep 部署alertmanageralertmanager-configmap.yml123456789101112131415161718192021222324252627282930313233apiVersion: v1kind: ConfigMapmetadata: name: alertmanager-config namespace: monitoringdata: config.yml: |- global: # 在没有报警的情况下声明为已解决的时间 resolve_timeout: 5m # 配置邮件发送信息 smtp_smarthost: 'smtp.163.com:25' smtp_from: 'xxx@163.com' smtp_auth_username: 'xxx@163.com' smtp_auth_password: 'xxx' smtp_require_tls: false # 所有报警信息进入后的根路由，用来设置报警的分发策略 route: # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster=A 和 alertname=LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: ['alertname', 'cluster'] # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。 group_wait: 30s # 当第一个报警发送后，等待'group_interval'时间来发送新的一组报警信息。 group_interval: 1m # 如果一个报警信息已经发送成功了，等待'repeat_interval'时间来重新发送他们 repeat_interval: 2h # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器 receiver: default receivers: - name: 'default' email_configs: - to: 'zhangzw@xxx.com' send_resolved: true alertmanager-deployment.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546apiVersion: apps/v1kind: Deploymentmetadata: name: alertmanager namespace: monitoringspec: replicas: 1 selector: matchLabels: app: alertmanager selector: matchLabels: app: alertmanager template: metadata: labels: app: alertmanager spec: containers: - name: alertmanager image: prom/alertmanager args: - "--config.file=/etc/alertmanager/config.yml" - "--storage.path=/alertmanager" ports: - name: alertmanager containerPort: 9093 volumeMounts: - name: alertmanager-cm mountPath: /etc/alertmanager volumes: - name: alertmanager-cm configMap: name: alertmanager-config---apiVersion: v1kind: Servicemetadata: name: alertmanager namespace: monitoringspec: selector: app: alertmanager ports: - port: 80 targetPort: 9093 最后配置prometheus的rule文件 prometheus-config-rulefiles.yml (修改) 这里仅针对我这里prometheus部署的时候是通过configmap挂载的rule文件 123456789101112131415161718kind: ConfigMapmetadata: name: prometheus-rulefiles namespace: monitoringapiVersion: v1data: k8s.yml: | groups: - name: cpu-load-rule rules: - alert: cpu-load-high expr: irate(container_cpu_usage_seconds_total&#123;image!=""&#125;[1m]) &gt; 0.1 for: 1m labels: serverity: warning annotations: summary: "&#123;&#123; $labels.instance &#125;&#125; container_name: &#123;&#123; $labels.container_name &#125;&#125; pod_name: &#123;&#123; $labels.pod_name &#125;&#125; , namespace: &#123;&#123; $labels.namespace&#125;&#125;" 也贴上prometheus的StatefulSet 配置12&gt; prometheus-statefulset.yml 配置可以看到prometheus-rulefiles 这个configmap是 挂载到 /etc/prometheus/rules/ 目录&gt; prometheus-configmap.yml 配置可以看到 rule_files: - /etc/prometheus/rules/*.yml, 所以 以上prometheus-rulefiles 这个configmap 的k8s.yml就被prometheus当做rule文件了 prometheus-statefulset.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596apiVersion: apps/v1kind: StatefulSetmetadata: labels: app: prometheus prometheus: k8s name: prometheus namespace: monitoringspec: replicas: 1 volumeClaimTemplates: - metadata: name: prometheus-data annotations: volume.beta.kubernetes.io/storage-class: "nfs-retain" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 20Gi revisionHistoryLimit: 10 selector: matchLabels: app: prometheus prometheus: k8s serviceName: prometheus updateStrategy: type: RollingUpdate template: metadata: creationTimestamp: null labels: app: prometheus prometheus: k8s spec: serviceAccount: prometheus-k8s containers: - args: - --web.console.templates=/etc/prometheus/consoles - --web.console.libraries=/etc/prometheus/console_libraries - --config.file=/etc/prometheus/config/prometheus.yml - --storage.tsdb.path=/prometheus - --web.enable-admin-api - --storage.tsdb.retention.time=20d - --web.enable-lifecycle - --storage.tsdb.no-lockfile - --web.external-url=http://prometheus1-dev.xxx.com/ - --web.route-prefix=/ image: prom/prometheus:v2.11.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 6 httpGet: path: /-/healthy port: web scheme: HTTP periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 name: prometheus ports: - containerPort: 9090 name: web protocol: TCP readinessProbe: failureThreshold: 120 httpGet: path: /-/ready port: web scheme: HTTP periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 resources: requests: memory: 400Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/prometheus/config name: config readOnly: true - mountPath: /prometheus name: prometheus-data #subPath: prometheus-db - mountPath: /etc/prometheus/rules/ name: prometheus-rulefiles volumes: - name: config configMap: defaultMode: 420 name: prometheus - name: prometheus-rulefiles configMap: defaultMode: 420 name: prometheus-rulefiles prometheus-configmap.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110kind: ConfigMapmetadata: name: prometheus namespace: monitoringapiVersion: v1data: prometheus.yml: | global: evaluation_interval: 30s scrape_interval: 30s external_labels: prometheus: monitoring/k8s alerting: alertmanagers: - static_configs: - targets: ['alertmanager:80'] rule_files: - /etc/prometheus/rules/*.yml scrape_configs: - job_name: prometheus honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - monitoring scrape_interval: 30s relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_prometheus regex: k8s - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_pod_name target_label: pod - source_labels: - __meta_kubernetes_service_name target_label: job replacement: - target_label: endpoint replacement: web - job_name: k8s-db-t-kube-apiserver honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - default scrape_interval: 30s scheme: https tls_config: insecure_skip_verify: false ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: keep source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] separator: ; regex: default;kubernetes;https metric_relabel_configs: - source_labels: - __name__ action: drop regex: (apiserver_storage_data_key_generation_latencies_microseconds_bucket|apiserver_admission_controller_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_bucket|apiserver_admission_step_admission_latencies_milliseconds_summary|apiserver_request_latencies_bucket|apiserver_request_latencies_summary|apiserver_storage_data_key_generation_latencies_microseconds_bucket|rest_client_request_latency_seconds_bucket) - job_name: k8s-db-t-pods honor_labels: true kubernetes_sd_configs: - role: node scrape_interval: 30s metrics_path: /metrics/cadvisor scheme: https tls_config: insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token - job_name: k8s-db-t-kube-state-metrics honor_labels: true kubernetes_sd_configs: - role: endpoints namespaces: names: - monitoring scrape_interval: 30s scrape_timeout: 30s tls_config: insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: kube-state-metrics - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http-main metric_relabel_configs: - source_labels: - __name__ regex: (kube_daemonset_status_number_ready|kube_daemonset_status_number_unavailable|kube_deployment_status_replicas_unavailable|kube_deployment_spec_paused|kube_deployment_spec_strategy_rollingupdate_max_surge|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_endpoint_address_available|kube_endpoint_address_not_ready|kube_node_info|kube_node_spec_unschedulable|kube_node_status_condition|kube_node_status_capacity|kube_node_status_capacity|kube_node_status_allocatable|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_status_phase|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolume_status_phase|kube_persistentvolume_info|kube_persistentvolume_capacity_bytes|kube_pod_info|kube_pod_status_phase|kube_pod_status_ready|kube_pod_container_info|kube_pod_container_status_waiting|kube_pod_container_status_waiting_reason|kube_pod_container_status_running|kube_pod_container_status_terminated_reason|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_resource_limits|kube_service_info|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_deployment_status_replicas_available|kube_deployment_status_replicas|kube_node_status_allocatable_memory_bytes|kube_deployment_status_replicas|kube_statefulset_replicas|kube_daemonset_status_desired_number_scheduled|kube_statefulset_status_replicas|changes|kube_job_status_failed) action: keep]]></content>
      <categories>
        <category>k8s</category>
        <category>prometheus</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>promethues</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker安装nginx第三方模块]]></title>
    <url>%2F2020%2F05%2F13%2F46-docker%E5%AE%89%E8%A3%85nginx%E7%AC%AC%E4%B8%89%E6%96%B9%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[由于hub.docker.com 官方的nginx 并不会包括第三方包, 这里简要说明如何安装nginx_upstream_check_module模块 健康检查 nginx_upstream_check_module github 地址 docker安装nginx 如果使用官方的nginx镜像, 这里无法安装第三方模块,并没有像php的docker-php-ext-install 工具, 因此这里采用源码安装 这里镜像的大小也控制的还可以 12345# 自己源码安装的镜像hub.xxx.com/nginx 1.16.1-debian-buster-slim 5b5884f7927e 34 seconds ago 120MB# 官方镜像nginx 1.16 588bb5d559c2 6 weeks ago 127MB dockerfile123456789101112131415161718192021222324252627282930FROM debian:buster-slimLABEL maintainer="zhangzw zhangzw@xxx.com"ENV NGINX_VERSION 1.16.1workdir /optRUN apt-get update \ &amp;&amp; apt-get install wget unzip gcc make openssl libssl-dev libpcre3 libpcre3-dev zlib1g-dev net-tools patch -y\ &amp;&amp; wget http://nginx.org/download/nginx-$&#123;NGINX_VERSION&#125;.tar.gz \ &amp;&amp; wget https://codeload.github.com/yaoweibin/nginx_upstream_check_module/zip/master \ &amp;&amp; tar -xvf nginx-$&#123;NGINX_VERSION&#125;.tar.gz \ &amp;&amp; unzip master \ &amp;&amp; cd nginx-$&#123;NGINX_VERSION&#125; \ &amp;&amp; patch -p1 &lt; ../nginx_upstream_check_module-master/check_1.16.1+.patch \ &amp;&amp; ./configure --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --add-module=../nginx_upstream_check_module-master/ &amp;&amp; make &amp;&amp; make install \ &amp;&amp; rm -rf /opt/* \ &amp;&amp; apt-get remove --purge -y wget unzip gcc make patch \ &amp;&amp; apt-get autoremove -y \ &amp;&amp; apt-get clean \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; useradd nginxrun ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \ &amp;&amp; echo 'Asia/Shanghai' &gt;/etc/timezoneexpose 80CMD ["/usr/sbin/nginx", "-c", "/etc/nginx/nginx.conf", "-g", "daemon off;"]]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci与k8s结合]]></title>
    <url>%2F2020%2F04%2F23%2F45-gitlab-ci%E4%B8%8Ek8s%E7%BB%93%E5%90%88%2F</url>
    <content type="text"><![CDATA[本文减少如何通过gitlab-ci整合k8s实现流水线部署 https://www.cnblogs.com/Sinte-Beuve/p/11739196.html https://www.qikqiak.com/post/gitlab-ci-k8s-cluster-feature/ 环境版本统计123451 gitlab/gitlab-runner 0.15.02 helm 2.163 k8s 1.16.44 gitlab 11.55 CentOS Linux release 7.7 /kernel 5.2 小节 1231. gitlab 通过admin管理页面的runner配置, 安装gitlab-runner, 安装方式可以是二进制, docker 或k8s (这里是k8s)2. gitlab 项目目录的 Operations -&gt; kubernetes -&gt; Add Kubernetes Cluster -&gt; Add existing cluster 是结合k8s, 每个项目都需要设置一个k8s集群,k8s集群需要配置rbac权限3. ci 在提交到私有harbor上是需要验证账号密码, 私有仓库拉取也需要验证 helm2.16安装12345678wget https://get.helm.sh/helm-v2.16.2-linux-amd64.tar.gztar -xvf helm-v2.16.2-linux-amd64.tar.gzmv linux-amd64/helm /usr/local/bin/helmkubectl create serviceaccount --namespace=kube-system tillerkubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tillerhelm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account tiller 从官方拉取helm2 配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344# 添加源helm repo add gitlab https://charts.gitlab.io# 查看helm search runnergitlab/gitlab-runner 0.15.0 12.9.0 GitLab Runner# 下载charts压缩包gitlab-runner-0.15.0.tgzhelm fetch gitlab/gitlab-runner# 创建gitlab-runner的rbac账号kubectl create serviceaccount gitlab-cluster-adminkubectl create clusterrolebinding gitlab-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts --namespace=default# 然后修改 values.yamlgitlabUrl: http://gitlab.zhangzw.com #gitlab服务器上管理页面上的URLrunnerRegistrationToken: #gitlab服务器管理页面的tokenserviceAccountName: gitlab-cluster-admin# 修改 templates/configmap.yaml (如果后面配置dind采用tcp://0.0.0.0:2375的方式应该不用挂载sock文件) # if ! sh /scripts/register-the-runner; then exit 1 fi # add new config start cat &gt;&gt;/home/gitlab-runner/.gitlab-runner/config.toml &lt;&lt;EOF [[runners.kubernetes.volumes.host_path]] name = "docker" mount_path = "/var/run/docker.sock" read_only = false host_path = "/var/run/docker.sock" EOF # add new config end # Start the runner exec /entrypoint run --user=gitlab-runner \ --working-directory=/home/gitlab-runner# helm启动,更新,删除gitlab-runnerhelm install --name gitlab-runner .helm upgrade gitlab-runner .helm delete --purge gitlab-runner 登录到gitlab-runner 镜像register注册 以下大部分都是回车, 只有token需要手动输入(输入values.yaml中的runnerRegistrationToken即可) 123456789101112131415161718192021bash-5.0$ gitlab-runner registerRuntime platform arch=amd64 os=linux pid=4257 revision=4c96e5ad version=12.9.0WARNING: Running in user-mode. WARNING: The user-mode requires you to manually start builds processing:WARNING: $ gitlab-runner run WARNING: Use sudo for system-mode: WARNING: $ sudo gitlab-runner... Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.zhangzw.com/):[http://gitlab.zhangzw.com]:Please enter the gitlab-ci token for this runner:djs47LiKFy-64FxAACp5Please enter the gitlab-ci description for this runner:[gitlab-runner-gitlab-runner-6cf8c6bff4-rhhs5]:Please enter the gitlab-ci tags for this runner (comma separated):Registering runner... succeeded runner=djs47LiKPlease enter the executor: docker+machine, docker-ssh+machine, kubernetes, docker, docker-ssh, shell, ssh, virtualbox, custom, parallels:[kubernetes]:Runner registered successfully. Feel free to start it, but if its running already the config should be automatically reloaded!bash-5.0$ 在项目的页面点击 Add Kubernetes Cluster -&gt; Add existing cluster API URL 是你的集群的apiserver的地址， 一般可以通过输入kubectl cluster-info获取 12345kubectl cluster-infoKubernetes master is running at https://master.k8s.io:16443KubeDNS is running at https://master.k8s.io:16443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyMetrics-server is running at https://master.k8s.io:16443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy CA证书 由于我们在部署阶段需要去创建、删除一些资源对象，所以我们也需要对象的 RBAC 权限，这里为了简单，我们直接新建一个 ServiceAccount，绑定上一个cluster-admin的权限(gitlab-serviceAccount.yaml) 123456789101112131415161718192021---apiVersion: v1kind: ServiceAccountmetadata: name: gitlab namespace: gitlab---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: gitlab namespace: gitlabsubjects: - kind: ServiceAccount name: gitlab namespace: gitlabroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin 1234567891011#可以通过上面创建的 ServiceAccount 获取 CA 证书和 Token：kubectl get serviceaccount gitlab -n gitlab -o json | jq -r '.secrets[0].name'gitlab-token-9jlpr# 然后根据上面的Secret找到CA证书kubectl get secret gitlab-token-9jlpr -n gitlab -o json | jq -r '.data["ca.crt"]' | base64 -dxxxxxCA证书内容xxxxx# 当然要找到对应的 Token 也很简单kubectl get secret gitlab-token-9jlpr -n gitlab -o json | jq -r '.data.token' | base64 -dxxxxxxtoken值xxxx 然后复制对应的值贴到项目的k8s配置中即可 简单测试123456789101112131415161718#.gitlab-ci.ymlimage: busyboxstages: - build - deployJob1: stage: build script: - echo "go go go !!!~" only: - masterdeploy: stage: deploy script: - echo "部署开始" only: - master .gitlab-ci.yaml配置 以下一些配置写到gitlab项目页面-&gt; Settings -&gt; CI/CD -&gt; Variables其实也可以写到.gitlab-ca.yaml中, 我在deployment.yaml也无法用如下变量 1234CI_REGISTRY_USER: adminCI_REGISTRY_PASSWORD: xxxxxCI_REGISTRY: hub.xxx.comCI_REGISTRY_IMAGE: hub.xxx.com/public/nginx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293variables: GIT_CURL_VERBOSE: 1 GIT_TRACE: 1stages: - release - review - deploybuildPushImage: stage: release image: docker:latest services: - name: docker:dind command: ["--insecure-registry=hub.zhangzw.com"] variables: DOCKER_DRIVER: overlay2 DOCKER_HOST: tcp://192.168.0.136:2375 script: - docker login -u "$&#123;CI_REGISTRY_USER&#125;" -p "$&#123;CI_REGISTRY_PASSWORD&#125;" "$&#123;CI_REGISTRY&#125;" - docker build -t "$&#123;CI_REGISTRY_IMAGE&#125;:$&#123;CI_COMMIT_REF_NAME&#125;" . - docker push "$&#123;CI_REGISTRY_IMAGE&#125;:$&#123;CI_COMMIT_REF_NAME&#125;"deploy_review: image: bitnami/kubectl:1.16.3 stage: review only: - branches except: - tags environment: name: dev url: http://dev-gitlab-k8s-demo.zhangzw.com on_stop: stop_review script: - kubectl version - cd deploy/ - sed -i "s/__CI_ENVIRONMENT_SLUG__/$&#123;CI_ENVIRONMENT_SLUG&#125;/" deployment.yaml ingress.yaml service.yaml - sed -i "s/__VERSION__/$&#123;CI_COMMIT_REF_NAME&#125;/" deployment.yaml ingress.yaml service.yaml - | if kubectl apply -f deployment.yaml | grep -q unchanged; then echo "=&gt; Patching deployment to force image update." kubectl patch -f deployment.yaml -p "&#123;\"spec\":&#123;\"template\":&#123;\"metadata\":&#123;\"annotations\":&#123;\"ci-last-updated\":\"$(date +'%s')\"&#125;&#125;&#125;&#125;&#125;" else echo "=&gt; Deployment apply has changed the object, no need to force image update." fi - kubectl apply -f service.yaml || true - kubectl apply -f ingress.yaml - kubectl rollout status -f deployment.yaml - kubectl get all,ing -l ref=$&#123;CI_ENVIRONMENT_SLUG&#125;stop_review: image: bitnami/kubectl:1.16.3 stage: review variables: GIT_STRATEGY: none when: manual only: - branches except: - master - tags environment: name: dev action: stop script: - kubectl version - kubectl delete ing -l ref=$&#123;CI_ENVIRONMENT_SLUG&#125; - kubectl delete all -l ref=$&#123;CI_ENVIRONMENT_SLUG&#125;deploy_live: image: bitnami/kubectl:1.16.3 stage: deploy environment: name: live url: http://live-gitlab-k8s-demo.zhangzw.com script: - echo "部署开始" - cd deploy - sed -i "s/__CI_ENVIRONMENT_SLUG__/$&#123;CI_ENVIRONMENT_SLUG&#125;/" deployment.yaml ingress.yaml service.yaml - sed -i "s/__VERSION__/$&#123;CI_COMMIT_REF_NAME&#125;/" deployment.yaml ingress.yaml service.yaml - kubectl apply -f deployment.yaml - kubectl apply -f service.yaml - kubectl apply -f ingress.yaml - kubectl rollout status -f deployment.yaml - kubectl get all,ing -l ref=$&#123;CI_ENVIRONMENT_SLUG&#125; only: - tags when: manual deploy/deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344---apiVersion: apps/v1kind: Deploymentmetadata: name: gitlab-k8s-demo-__CI_ENVIRONMENT_SLUG__ labels: app: gitlab-k8s-demo ref: __CI_ENVIRONMENT_SLUG__ track: stablespec: replicas: 2 selector: matchLabels: app: gitlab-k8s-demo ref: __CI_ENVIRONMENT_SLUG__ template: metadata: labels: app: gitlab-k8s-demo ref: __CI_ENVIRONMENT_SLUG__ track: stable spec: imagePullSecrets: - name: myregistry containers: - name: app image: hub.zhangzw.com/bq/nginx:__VERSION__ imagePullPolicy: Always ports: - name: http-metrics protocol: TCP containerPort: 80 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 timeoutSeconds: 2 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 3 timeoutSeconds: 2 如果我们harbor中设置的是私有镜像, 则需要设置imagePullSecret 才能拉取镜像1kubectl create secret docker-registry gitlab-hub-secret --docker-server=hub.zhangzw.com --docker-username=xxx --docker-password=xxx --docker-email=zhangzw@zhangzw.com deploy/service.yaml12345678910111213141516171819202122---apiVersion: v1kind: Servicemetadata: name: gitlab-k8s-demo-__CI_ENVIRONMENT_SLUG__ labels: app: gitlab-k8s-demo ref: __CI_ENVIRONMENT_SLUG__ annotations: prometheus.io/scrape: "true" prometheus.io/port: "80" prometheus.io/scheme: "http" prometheus.io/path: "/"spec: type: ClusterIP ports: - name: http-metrics port: 80 protocol: TCP selector: app: gitlab-k8s-demo ref: __CI_ENVIRONMENT_SLUG__ deploy/ingress.yaml12345678910111213141516171819---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: gitlab-k8s-demo-__CI_ENVIRONMENT_SLUG__ labels: app: gitlab-k8s-demo ref: __CI_ENVIRONMENT_SLUG__ annotations: kubernetes.io/ingress.class: "traefik"spec: rules: - host: __CI_ENVIRONMENT_SLUG__-gitlab-k8s-demo.zhangzw.com http: paths: - path: / backend: serviceName: gitlab-k8s-demo-__CI_ENVIRONMENT_SLUG__ servicePort: 80 遇到的问题err1. 流水线打包的时候提示没有权限12345ERROR: Job failed (system failure): pods is forbidden: User "system:serviceaccount:default:default" cannot create resource "pods" in API group "" in the namespace "default"kubectl create serviceaccount gitlab-cluster-adminkubectl create clusterrolebinding gitlab-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts --namespace=default err2. 流水线打包提示无法拉取项目123fatal: unable to access 'http://gitlab.zhangzw.com/k8s/rancher-dev-php.git/': Failed to connect to gitlab.zhangzw.com port 80: Connection refusedUploading artifacts for failed jobERROR: Job failed: command terminated with exit code 1 难道是因为我们的项目是私有项目? 显然我们在官网 https://docs.gitlab.zhangzw.com/runner/configuration/advanced-configuration.html 这里看到下面一句话 1234Only if the clone_url is set, the runner will construct a clone URL in the form of http://gitlab-ci-token:s3cr3tt0k3n@192.168.1.23/namespace/project.git.# 尝试修改 values.yamlcloneUrl: http://gitlab.zhangzw.com 之后还是出现了该问题 所以这应该是gitlab-runner register的问题 但是这里诡异的是, 我三个步骤,前两个都是正常, 第三个一直是报错connection refused, 但是我重试了三遍又正常了, 这么不稳定的吗? 看日志错误有点像是这么回事, 我的nginx反向代理了gitlab.zhangzw.com proxy_pass的是ip:10080, 而日志中看起来报错提示是链接到ip:80 我这里试着将helm安装gitlab-runner的配置文件values.yaml修改如下: 12gitlabUrl: http://192.168.0.65:10080 cloneUrl: http://192.168.0.65:10080 测试之后还是会出现无法连接, 所以也不是这个问题, 这里git clone http 方式经过nginx代理是没问题的 如果是项目权限问题, 那我新建一个public项目应该不会报错 测试之后还是会出现无法连接, 看起问题还是网络方面问题, 为什么会网络无法连接呢? 嗯??? 我查看了下 gitlab admin -&gt; runners -&gt; 点击Runner token 进入 -&gt; Assigned projects 这里我之前是加过的, 但是helm 安装的gitlab-runner upgrade之后就好像丢失了, 可能是我这边没有mount数据存储到nfs, 不过这里可以不用按照分配的方式, 就设置为share即可,如果有多个runner,需要更细致的权限部署划分,可以设置 所以现在这个问题就变成, 有可能第一次会成功, 有可能会失败, 有可能第二个步骤失败 , 然后retry 2次?3次或6次又成功了, 蛋疼中… 2020-04-24 15:58:40 补: 我这边测试的gitlab-runner安装的网段和gitlab的网段之间网络问题, 现在新搭建的gitlab-runner和gitlab在同一网段就不在报错了... err3. k8s1.16安装helm2.14的有报错: Error: error installing: the server could not find the requested resource, 这是由于 extensions/v1beta1 已经被 apps/v1 替代。相信在2.15 或者 3 版本发布之后, 应该就不会遇到这个问题了。还是生态比较慢的原因。1helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3 --stable-repo-url http://mirror.azure.cn/kubernetes/charts/ --service-account tiller --override spec.selector.matchLabels.'name'='tiller',spec.selector.matchLabels.'app'='helm' --output yaml | sed 's@apiVersion: extensions/v1beta1@apiVersion: apps/v1@' | kubectl apply -f - err4. Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?1不确定为什么会报这个错误, 第二次启动又没问题了 err5. time=”2020-04-22T02:39:12Z” level=error msg=”failed to dial gRPC: cannot connect to the Docker daemon. Is ‘docker daemon’ running on this host?: dial tcp 127.0.0.1:2375: connect: connection refused”123这种情况需要docker或者k8s的docker启动方式添加tcp方式vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock -H fd:// --containerd=/run/containerd/containerd.sock]]></content>
      <categories>
        <category>k8s</category>
        <category>gitlab-ci</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>gitlab-runner</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s部署fluentd+kafka+logstash+es]]></title>
    <url>%2F2020%2F04%2F09%2F44-k8s%E9%83%A8%E7%BD%B2fluentd-kafka-logstash-es%2F</url>
    <content type="text"><![CDATA[客户端采集数据的软件比较多, 有logstash,flume,fluentd/fluent-bit,filebeat等,这里在k8s集群中部署fluentd开启UDP端口接收代码写入的json日志,并写入到kafka中 1. 一些服务版本123docker镜像: docker pull fluentd:v1.9.1-1.0kafka: kafka-server-0.10.0+kafka2.1.0-1.2.1.0.p0.63.el6.noarchfluent-plugin-kafka: 0.5.7 2. fluentd 镜像安装kafka扩展 Dockerfile 由于fluent-plugin-kafka版本要求我们的kafka是0.10, 所以高版本有问题, 安装了fluent-plugin-kafka 0.5.7 则正常 官方文档: https://rubygems.org/gems/fluent-plugin-kafka/versions/0.5.7 123456789from fluentd:v1.9.1-1.0MAINTAINER zhangzw &lt;zhangzw@xxx.com&gt;USER rootRUN fluent-gem install fluent-plugin-kafka -v 0.5.7USER fluent 3. fluentd配置文件 fluent-udp-to-kafka.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;source&gt; @type udp @label @mainstream tag udplog # required &lt;parse&gt; @type regexp expression /^(?&lt;message&gt;.*)$/ &lt;/parse&gt; port 12301 # optional. 5160 by default bind 0.0.0.0 # optional. 0.0.0.0 by default message_length_limit 1MB # optional. 4096 bytes by default&lt;/source&gt;&lt;filter **&gt; @type stdout&lt;/filter&gt;&lt;label @mainstream&gt; &lt;match **&gt; @type kafka2 # list of seed brokers，这个地方可以通过逗号写多个地址比如 host1:9092,host2:9092 brokers 192.168.xxx.142:9092 use_event_time true # buffer settings &lt;buffer topic&gt; @type file # 下面的path可能需要手动创建目录，并给写入权限，我直接给了777 path /fluentd/log/td-agent/buffer/td flush_interval 3s &lt;/buffer&gt; # data type settings &lt;format&gt; @type json &lt;/format&gt; # kafka中创建的topic topic_key udplog # 默认topic default_topic udplog get_kafka_client_log true # producer settings required_acks -1 compression_codec gzip &lt;/match&gt;&lt;/label&gt; 4. k8s部署 开启input udp 12301接收数据, 并output给kafka123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263k8s-fluentd-udplog-udp-to-kafka.yml---kind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: fluentd-udplog name: fluentd-udplog namespace: ns-elastic7spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: fluentd-udplog template: metadata: labels: elastic-app: fluentd-udplog spec: containers: - name: fluentd-udplog image: hub.xxx.com/bq/fluentd:v1.9.1-1.0-kafka-0.10 ports: - containerPort: 12301 name: port12301 protocol: UDP resources: requests: cpu: "50m" limits: cpu: "500m" volumeMounts: - name: fluentd-udplog-logs mountPath: /fluentd/log - name: fluentd-udplog-cfg mountPath: /fluentd/etc/fluent.conf volumes: - name: fluentd-udplog-logs hostPath: path: /data/k8s-container/elk-7.2.0/fluentd/logs/ - name: fluentd-udplog-cfg hostPath: path: /data/k8s-container/elk-7.2.0/fluentd/fluent-udp-to-kafka.conf---kind: ServiceapiVersion: v1metadata: labels: elastic-app: fluentd-udplog name: fluentd-udplog-service-nodeport namespace: ns-elastic7spec: type: NodePort ports: - name: port12301 port: 12301 targetPort: 12301 nodePort: 12301 protocol: UDP selector: elastic-app: fluentd-udplog 5. 修改logstash的配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576config/pipeline/logstash.confinput &#123; kafka&#123; type =&gt;"php-mysql-dev-252-log" bootstrap_servers =&gt; "192.168.xxx.142:9092" topics =&gt; "php-mysql-dev-0-slowlog" &#125; kafka&#123; type =&gt;"udplog" bootstrap_servers =&gt; "192.168.xxx.142:9092" topics =&gt; "udplog" &#125;&#125;filter&#123;############### if [type] == "php-mysql-dev-252-log" &#123; json &#123; source =&gt; "message" &#125; mutate &#123; gsub =&gt; [ "message", "\n", "" ] &#125; grok &#123; match =&gt; ["message","(?m)^# User@Host: %&#123;USER:user&#125;\[[^\]]+\] @ \[%&#123;IP:clientip&#125;\]# Query_time: %&#123;NUMBER:query_time:float&#125;\s+Lock_time: %&#123;NUMBER:lock_time:float&#125;\s+Rows_sent: %&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined: %&#123;NUMBER:rows_examined:int&#125;(?&lt;dbnameall&gt;.*)SET\s+timestamp=%&#123;NUMBER:timestamp_mysql:int&#125;;(?&lt;query&gt;.*)"] &#125; date &#123; match =&gt; ["timestamp_mysql", "UNIX"] target =&gt; "@timestamp" &#125; &#125;###### if [type] == "udplog" &#123; grok &#123; match =&gt; &#123; "message" =&gt; "&lt;%&#123;NUMBER:id:int&#125;&gt;%&#123;NUMBER:id_N:int&#125; (?&lt;http_time&gt;\S+) %&#123;DATA:hostname&#125; %&#123;DATA:ident&#125; %&#123;NUMBER:pid:int&#125; - - %&#123;DATA:logLevel&#125;: X-Request-Id:%&#123;DATA:Request_Id&#125; module:%&#123;DATA:moduleName&#125; act:%&#123;DATA:Act&#125; sql:(?&lt;sql&gt;(.*)) cost:%&#123;NUMBER:sqlDuring:int&#125;ms \[\] \[\]" &#125; &#125; grok &#123; match =&gt; &#123;"sql" =&gt;" %&#123;DATA:operation&#125; "&#125; &#125; if "_grokparsefailure" not in [tags] &#123; if [sqlDuring] &lt; 5 &#123; drop &#123;&#125; &#125; &#125; else &#123; drop &#123;&#125; &#125; &#125;&#125;output &#123;### if [type] == "php-mysql-dev-252-log" &#123; elasticsearch &#123; hosts =&gt; [ "http://192.168.xxx.120:19230" ] index =&gt; "php-mysql-dev-252-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;### if [type] == "udplog" &#123; elasticsearch &#123; hosts =&gt; [ "http://192.168.xxx.120:19230" ] index =&gt; "udplog-%&#123;+YYYY.MM.dd&#125;" &#125; &#125;&#125; 6. 部署k8s logstash 分析后写入到es中 k8s-logstash-7.2.0-kafka-to-es.yml12345678910111213141516171819202122232425262728293031323334353637---kind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: slowlog-logstash-kafka-to-es name: slowlog-logstash-kafka-to-es namespace: ns-elastic7spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: slowlog-logstash-kafka-to-es template: metadata: labels: elastic-app: slowlog-logstash-kafka-to-es spec: containers: - name: slowlog-logstash-kafka-to-es image: hub.xxx.com/bq/logstash:7.2.0 resources: requests: cpu: "50m" limits: cpu: "500m" volumeMounts: - name: slowlog-toes-cfg mountPath: /usr/share/logstash/config volumes: - name: slowlog-toes-cfg hostPath: path: /data/k8s-container/elk-7.2.0/mysqlslowlog-logstash-7.2.0/config tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule]]></content>
      <categories>
        <category>elk7</category>
        <category>fluentd</category>
      </categories>
      <tags>
        <tag>fluentd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s的yaml配置名词解释(模板)]]></title>
    <url>%2F2020%2F03%2F31%2F43-k8s%E7%9A%84yaml%E9%85%8D%E7%BD%AE%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[针对Deployment的yaml配置解释说明 原文: K8s Deployment YAML 名词解释 Deployment API 版本对照表123456Kubernetes 版本 Deployment 版本v1.5-v1.15 extensions/v1beta1v1.7-v1.15 apps/v1beta1v1.8-v1.15 apps/v1beta2v1.9+ apps/v1 Deployment yaml 名词解释：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384apiVersion: apps/v1 # 指定api版本，此值必须在kubectl api-versions中 kind: Deployment # 指定创建资源的角色/类型 metadata: # 资源的元数据/属性 name: demo # 资源的名字，在同一个namespace中必须唯一 namespace: default # 部署在哪个namespace中 labels: # 设定资源的标签 app: demo version: stablespec: # 资源规范字段 replicas: 1 # 声明副本数目 revisionHistoryLimit: 3 # 保留历史版本 selector: # 选择器 matchLabels: # 匹配标签 app: demo version: stable strategy: # 策略 rollingUpdate: # 滚动更新 maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 示在更新过程中能够进入不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 type: RollingUpdate # 滚动更新策略 template: # 模版 metadata: # 资源的元数据/属性 annotations: # 自定义注解列表 sidecar.istio.io/inject: "false" # 自定义注解名字 labels: # 设定资源的标签 app: demo version: stable spec: # 资源规范字段 containers: - name: demo # 容器的名字 image: demo:v1 # 容器使用的镜像地址 imagePullPolicy: IfNotPresent # 每次Pod启动拉取镜像策略，三个选择 Always、Never、IfNotPresent # Always，每次都检查；Never，每次都不检查（不管本地是否有）；IfNotPresent，如果本地有就不检查，如果没有就拉取 resources: # 资源管理 limits: # 最大使用 cpu: 300m # CPU，1核心 = 1000m memory: 500Mi # 内存，1G = 1000Mi requests: # 容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 100m memory: 100Mi livenessProbe: # pod 内部健康检查的设置 httpGet: # 通过httpget检查健康，返回200-399之间，则认为容器正常 path: /healthCheck # URI地址 port: 8080 # 端口 scheme: HTTP # 协议 # host: 127.0.0.1 # 主机地址 initialDelaySeconds: 30 # 表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 # 检测的超时时间 periodSeconds: 30 # 检查间隔时间 successThreshold: 1 # 成功门槛 failureThreshold: 5 # 失败门槛，连接失败5次，pod杀掉，重启一个新的pod readinessProbe: # Pod 准备服务健康检查设置 httpGet: path: /healthCheck port: 8080 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 5 periodSeconds: 10 successThreshold: 1 failureThreshold: 5 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: # 通过tcpSocket检查健康 # port: number ports: - name: http # 名称 containerPort: 8080 # 容器开发对外的端口 protocol: TCP # 协议 imagePullSecrets: # 镜像仓库拉取密钥 - name: harbor-certification affinity: # 亲和性调试 nodeAffinity: # 节点亲和力 requiredDuringSchedulingIgnoredDuringExecution: # pod 必须部署到满足条件的节点上 nodeSelectorTerms: # 节点满足任何一个条件就可以 - matchExpressions: # 有多个选项，则只有同时满足这些逻辑选项的节点才能运行 pod - key: beta.kubernetes.io/arch operator: In values: - amd64 Service yaml 名词解释：12345678910111213141516apiVersion: v1 # 指定api版本，此值必须在kubectl api-versions中 kind: Service # 指定创建资源的角色/类型 metadata: # 资源的元数据/属性 name: demo # 资源的名字，在同一个namespace中必须唯一 namespace: default # 部署在哪个namespace中 labels: # 设定资源的标签 app: demospec: # 资源规范字段 type: ClusterIP # ClusterIP 类型 ports: - port: 8080 # service 端口 targetPort: http # 容器暴露的端口 protocol: TCP # 协议 name: http # 端口名称 selector: # 选择器 app: demo]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm安装高可用k8s集群]]></title>
    <url>%2F2020%2F03%2F24%2F42-kubeadm%E5%AE%89%E8%A3%85%E9%AB%98%E5%8F%AF%E7%94%A8k8s%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[简单记录kubeadm方式安装k8s1.16.4高可用集群,haproxy通过keepalived绑定的vip来负载均衡到三台master, 其中keepalived则反之单机故障,haproxy则让三台master可以同时提供服务. Centos7.6部署k8s v1.16.4高可用集群(主备模式)使用kubeadm搭建高可用k8s v1.16.3集群(keepalived+haproxy) 一、 安装准备 1.1 主机名 1234192.168.53.106 master01.k8s.io192.168.53.107 master02.k8s.io192.168.53.108 master03.k8s.io192.168.53.137 master.k8s.io 1.2 同步时间, 设置时区 12345* * * * * /usr/sbin/ntpdate time.nist.govtimedatectl set-timezone Asia/Shanghai或者ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 1.3 关闭SElinux 12345setenforce 0sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/configsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/selinux/config 1.4 关闭swap(否则kubeadm init或join会报错) 1234567&gt; swapoff -a &amp;&amp; sysctl -w vm.swappiness=0vm.swappiness = 0或 swapoff -a#/etc/fstab也要注解掉SWAP挂载。sed -i.$(date +%F).bak '/swap/s/^/#/' /etc/fstab#sed -i 's/.*swap.*/#&amp;/' /etc/fstab 1.5 配置系统内核参数 1234567使流过网桥的流量也进入iptables/netfilter框架中，在/etc/sysctl.conf中添加以下配置cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1EOFsysctl -p /etc/sysctl.d/k8s.conf 如果出现报错 12sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directorysysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory 报错解决: 1234# 执行以下命令1 modprobe br_netfilter2 ls /proc/sys/net/bridge3 sysctl -p /etc/sysctl.d/k8s.conf 1.6 设置k8s源 123456789101112cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum clean allyum makecache -y 1234567[] 中括号中的是repository id，唯一，用来标识不同仓库name 仓库名称，自定义baseurl 仓库地址enable 是否启用该仓库，默认为1表示启用gpgcheck 是否验证从该仓库获得程序包的合法性，1为验证repo_gpgcheck 是否验证元数据的合法性 元数据就是程序包列表，1为验证gpgkey=URL 数字签名的公钥文件所在位置，如果gpgcheck值为1，此处就需要指定gpgkey文件的位置，如果gpgcheck值为0就不需要此项了 1.7 免密登录配置 略 二、 docker版本安装 2.1 配置源 123456yum install -y yum-utils device-mapper-persistent-data lvm2 bash-completionyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install docker-ce-18.09.9 docker-ce-cli-18.09.9 containerd.io -y# 高版本降级yum downgrade --setopt=obsoletes=0 -y docker-ce-18.09.9 docker-ce-cli-18.09.9 2.2 配置阿里云镜像加速器 登陆地址为：https://cr.console.aliyun.com ,未注册的可以先注册阿里云账户 123456mkdir -p /etc/dockertee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://0aqwccdy.mirror.aliyuncs.com"]&#125;EOF 2.3 启动docker 12systemctl restart dockersystemctl enable docker 2.4 修改Cgroup Driver 修改daemon.json，新增‘”exec-opts”: [“native.cgroupdriver=systemd”’ 12345cat /etc/docker/daemon.json&#123; "registry-mirrors": ["https://0aqwccdy.mirror.aliyuncs.com"], "exec-opts": ["native.cgroupdriver=systemd"]&#125; 重新加载docker 12systemctl restart dockersystemctl enable docker 修改cgroupdriver是为了消除告警： 1[WARNING IsDockerSystemdCheck]: detected “cgroupfs” as the Docker cgroup driver. The recommended driver is “systemd”. Please follow the guide at https://kubernetes.io/docs/setup/cri/ 三、 keepalived安装 3.1 安装 1yum -y install keepalived 3.2 master01.k8s.io上配置 1234567891011121314151617181920212223242526272829303132tee /etc/keepalived/keepalived.conf &lt;&lt;- 'EOF'! Configuration File for keepalivedglobal_defs &#123; router_id master01.k8s.io&#125;vrrp_script check_haproxy &#123; script "killall -0 haproxy" interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface enp0s3 virtual_router_id 51 priority 250 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.53.137 &#125; track_script &#123; check_haproxy &#125;&#125;EOF 3.3 master02.k8s.io,master03.k8s.io上配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566tee /etc/keepalived/keepalived.conf &lt;&lt;- 'EOF'! Configuration File for keepalivedglobal_defs &#123; router_id master02.k8s.io&#125;vrrp_script check_haproxy &#123; script "killall -0 haproxy" interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface enp0s3 virtual_router_id 51 priority 200 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.53.137 &#125; track_script &#123; check_haproxy &#125;&#125;EOFtee /etc/keepalived/keepalived.conf &lt;&lt;- 'EOF'! Configuration File for keepalivedglobal_defs &#123; router_id master03.k8s.io&#125;vrrp_script check_haproxy &#123; script "killall -0 haproxy" interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface enp0s3 virtual_router_id 51 priority 150 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.53.137 &#125; track_script &#123; check_haproxy &#125;&#125;EOF 3.4 master02.k8s.io,master03.k8s.io上启动keepalived 12service keepalived startsystemctl enable keepalived 3.5 测试 1234567# 首先 ip a查看ip否则绑定成功# ping 192.168.53.137 是否正常# 在master01.k8s.io上 停止服务 service keepalived stop# 在master02.k8s.io或master03.k8s.io上查看ip a是否存在192.168.53.137, 检查ping 192.168.53.137 是否正常 四、 haproxy安装 4.1 安装 1yum install -y haproxy 4.2 配置 三台master节点的配置均相同，配置中声明了后端代理的三个master节点服务器，指定了haproxy运行的端口为16443等，因此16443端口为集群的入口，其他的配置不做赘述。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475tee /etc/haproxy/haproxy.cfg &lt;&lt;- 'EOF'#---------------------------------------------------------------------# Global settings#---------------------------------------------------------------------global # to have these messages end up in /var/log/haproxy.log you will # need to: # 1) configure syslog to accept network log events. This is done # by adding the '-r' option to the SYSLOGD_OPTIONS in # /etc/sysconfig/syslog # 2) configure local2 events to go to the /var/log/haproxy.log # file. A line like the following can be added to # /etc/sysconfig/syslog # # local2.* /var/log/haproxy.log # log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats#---------------------------------------------------------------------# common defaults that all the 'listen' and 'backend' sections will# use if not designated in their block#--------------------------------------------------------------------- defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000#---------------------------------------------------------------------# kubernetes apiserver frontend which proxys to the backends#---------------------------------------------------------------------frontend kubernetes-apiserver mode tcp bind *:16443 option tcplog default_backend kubernetes-apiserver #---------------------------------------------------------------------# round robin balancing between the various backends#---------------------------------------------------------------------backend kubernetes-apiserver mode tcp balance roundrobin server master01.k8s.io 192.168.53.106:6443 check server master02.k8s.io 192.168.53.107:6443 check server master03.k8s.io 192.168.53.108:6443 check#---------------------------------------------------------------------# collection haproxy statistics message#---------------------------------------------------------------------listen stats bind *:1080 stats auth admin:awesomePassword stats refresh 5s stats realm HAProxy\ Statistics stats uri /admin?statsEOF 4.3 启动 1234systemctl enable haproxysystemctl start haproxysystemctl status haproxynetstat -lnptu|grep haproxy 五、 k8s安装 5.1 版本查看 1yum list kubelet --showduplicates | sort -r 本文安装的kubelet版本是1.16.4，该版本支持的docker版本为1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09。 5.2 安装kubelet、kubeadm和kubectl 1yum install -y kubelet-1.16.4 kubeadm-1.16.4 kubectl-1.16.4 kubelet 运行在集群所有节点上，用于启动Pod和容器等对象的工具kubeadm 用于初始化集群，启动集群的命令工具kubectl 用于和集群通信的命令行，通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件 5.3 启动kubelet 12systemctl enable kubeletsystemctl start kubelet 5.4 kubectl命令补全 123456# bashecho "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bash_profilesource ~/.bash_profile# zshecho "source &lt;(kubectl completion zsh)" &gt;&gt; ~/.zshrcsource ~/.zshrc 5.5 下载镜像 外网的慢, 从阿里云下载后打个官方tag即可 1234567891011121314151617181920212223242526tee /root/image.sh &lt;&lt;- 'EOF'#!/bin/bash#url=registry.cn-hangzhou.aliyuncs.com/loong576url=registry.aliyuncs.com/google_containersversion=v1.16.4images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '&#123;print $2&#125;'`)for imagename in $&#123;images[@]&#125; ; do docker pull $url/$imagename docker tag $url/$imagename k8s.gcr.io/$imagename docker rmi -f $url/$imagenamedoneEOF# 下载sh /root/image.sh# 验证docker images|grep 1.16.4k8s.gcr.io/kube-apiserver v1.16.4 3722a80984a0 3 months ago 217MBk8s.gcr.io/kube-controller-manager v1.16.4 fb4cca6b4e4c 3 months ago 163MBk8s.gcr.io/kube-proxy v1.16.4 091df896d78f 3 months ago 86.1MBk8s.gcr.io/kube-scheduler v1.16.4 2984964036c8 3 months ago 87.3MBk8s.gcr.io/metrics-server-amd64 v0.3.5 abf04c0f54ff 6 months ago 39.9MBk8s.gcr.io/etcd 3.3.15-0 b2756210eeab 6 months ago 247MBk8s.gcr.io/coredns 1.6.2 bf261d157914 7 months ago 44.1MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 2 years ago 742kB 六、初始化master 6.1 kubeadm.1.16.4.conf 在具有vip的master上操作，这里为master01.k8s.io 123456789101112131415161718192021tee /data/k8s-config/kubeadm.1.16.4.conf &lt;&lt;- 'EOF'apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: v1.16.4apiServer: certSANs: #填写所有kube-apiserver节点的hostname、IP、VIP - master01.k8s.io - master02.k8s.io - master03.k8s.io - master.k8s.io - dk-node1 - 192.168.53.106 - 192.168.53.107 - 192.168.53.108 - 192.168.53.137 - 192.168.0.136 - 127.0.0.1controlPlaneEndpoint: "master.k8s.io:16443"networking: podSubnet: "10.244.0.0/16"EOF 6.2 master初始化 1kubeadm init --config=kubeadm.1.16.4.conf 1234567891011You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root: kubeadm join master.k8s.io:16443 --token ynaob5.49rz8ofxavp6hzes \ --discovery-token-ca-cert-hash sha256:6e7859f3b9d8ede08e2202d3cd63c42f56c7d2503dc8c6fb9dc5f050b5c17bac \ --control-planeThen you can join any number of worker nodes by running the following on each as root:kubeadm join master.k8s.io:16443 --token ynaob5.49rz8ofxavp6hzes \ --discovery-token-ca-cert-hash sha256:6e7859f3b9d8ede08e2202d3cd63c42f56c7d2503dc8c6fb9dc5f050b5c17bac 6.3 加载环境变量 12echo "export KUBECONFIG=/etc/kubernetes/admin.conf" &gt;&gt; ~/.zshrcsource ~/.zshrc 本文所有操作都在root用户下执行，若为非root用户，则执行如下操作： 123mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 6.4 安装flannel网络 123456789101112kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.ymlpodsecuritypolicy.policy/psp.flannel.unprivileged createdclusterrole.rbac.authorization.k8s.io/flannel createdclusterrolebinding.rbac.authorization.k8s.io/flannel createdserviceaccount/flannel createdconfigmap/kube-flannel-cfg createddaemonset.apps/kube-flannel-ds-amd64 createddaemonset.apps/kube-flannel-ds-arm64 createddaemonset.apps/kube-flannel-ds-arm createddaemonset.apps/kube-flannel-ds-ppc64le createddaemonset.apps/kube-flannel-ds-s390x created 七、control plane节点加入集群 7.1 证书分发 在master01.k8s.io上运行脚本cert-main-master.sh，将证书分发至master02.k8s.io 1234567891011121314151617181920tee /root/cert-main-master.sh &lt;&lt;- 'EOF'USER=root # customizableCONTROL_PLANE_IPS="192.168.53.107 192.168.53.108"CONTROL_PLANE_pkidir="/etc/kubernetes/pki"for host in $&#123;CONTROL_PLANE_IPS&#125;; do ssh root@$&#123;host&#125; "mkdir -p $&#123;CONTROL_PLANE_pkidir&#125;/etcd" scp /etc/kubernetes/pki/ca.crt "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/ scp /etc/kubernetes/pki/ca.key "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/ scp /etc/kubernetes/pki/sa.key "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/ scp /etc/kubernetes/pki/sa.pub "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/ scp /etc/kubernetes/pki/front-proxy-ca.crt "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/ scp /etc/kubernetes/pki/front-proxy-ca.key "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/ scp /etc/kubernetes/pki/etcd/ca.crt "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/etcd/ca.crt # Quote this line if you are using external etcd scp /etc/kubernetes/pki/etcd/ca.key "$&#123;USER&#125;"@$host:$&#123;CONTROL_PLANE_pkidir&#125;/etcd/ca.keydoneEOFsh /root/cert-main-master.sh 7.2 master02.k8s.io,master03.k8s.io加入集群 123kubeadm join master.k8s.io:16443 --token ynaob5.49rz8ofxavp6hzes \ --discovery-token-ca-cert-hash sha256:6e7859f3b9d8ede08e2202d3cd63c42f56c7d2503dc8c6fb9dc5f050b5c17bac \ --control-plane 6.3 master02.k8s.io,master03.k8s.io加载环境变量 12echo "export KUBECONFIG=/etc/kubernetes/admin.conf" &gt;&gt; ~/.zshrcsource ~/.zshrc 本文所有操作都在root用户下执行，若为非root用户，则执行如下操作： 123mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 7.4 集群节点查看 1234567891011121314151617181920212223242526272829303132333435363738# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster01.k8s.io Ready master 6m v1.16.4master02.k8s.io Ready master 99s v1.16.4master03.k8s.io Ready master 46s v1.16.4# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-5644d7b6d9-fz77l 1/1 Running 0 4hcoredns-5644d7b6d9-qvh6b 1/1 Running 0 4hetcd-master01.k8s.io 1/1 Running 1 4h15metcd-master02.k8s.io 1/1 Running 0 4h12metcd-master03.k8s.io 1/1 Running 0 4h11mkube-apiserver-master01.k8s.io 1/1 Running 1 4h15mkube-apiserver-master02.k8s.io 1/1 Running 0 4h12mkube-apiserver-master03.k8s.io 1/1 Running 0 4h10mkube-controller-manager-master01.k8s.io 1/1 Running 2 4h15mkube-controller-manager-master02.k8s.io 1/1 Running 1 4h12mkube-controller-manager-master03.k8s.io 1/1 Running 0 4h10mkube-flannel-ds-amd64-84b6w 1/1 Running 1 4h15mkube-flannel-ds-amd64-df99l 1/1 Running 0 4h11mkube-flannel-ds-amd64-jzt62 1/1 Running 1 4h12mkube-flannel-ds-amd64-lwd8m 1/1 Running 0 12mkube-proxy-fgcmg 1/1 Running 0 4h11mkube-proxy-mss74 1/1 Running 0 12mkube-proxy-r9rz2 1/1 Running 1 4h16mkube-proxy-s47gj 1/1 Running 0 4h12mkube-scheduler-master01.k8s.io 1/1 Running 2 4h15mkube-scheduler-master02.k8s.io 1/1 Running 1 4h12mkube-scheduler-master03.k8s.io 1/1 Running 0 4h10m# kubectl get csNAME AGEscheduler &lt;unknown&gt;controller-manager &lt;unknown&gt;etcd-0 &lt;unknown&gt; 执行kubectl get cs显示&lt;unknown&gt;是一个1.16版本已知的bug，后续官方应该会解决处理，有大佬分析了源码并且提交了pr，可点此参考 7.5 测试集群 12345678910111213141516171819# 1 查看leader# kubectl get endpoints kube-controller-manager -n kube-system -o yaml |grep holderIdentitycontrol-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"master01.k8s.io_4b4f63f3-551e-4514-8aa9-a8fdbb13f1b4","leaseDurationSeconds":15,"acquireTime":"2020-03-24T02:40:32Z","renewTime":"2020-03-24T02:45:47Z","leaderTransitions":1&#125;'# 2 在master01.k8s.io 上执行 init 0 关机 模拟宕机# 3 controller-manager和scheduler也发生了迁移# kubectl get endpoints kube-controller-manager -n kube-system -o yaml |grep holderIdentitycontrol-plane.alpha.kubernetes.io/leader: '&#123;"holderIdentity":"master02.k8s.io_457a8d6d-d0e4-4a8e-afbe-0c37f0dadf8d","leaseDurationSeconds":15,"acquireTime":"2020-03-24T02:46:03Z","renewTime":"2020-03-24T02:50:50Z","leaderTransitions":2&#125;'# 4 集群此时还是能正常操作# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster01.k8s.io NotReady master 17m v1.16.4master02.k8s.io Ready master 12m v1.16.4master03.k8s.io Ready master 11m v1.16.4 7.6 导入集群到rancher 12# 这里请自行在rancher界面生成(我这里是rancherv2.3.5)curl --insecure -sfL https://rancher-dev.xxx.com/v3/import/68nzw8nlch92gshktcx2v5d8xvlvlk57nfgffz9jr7hxwfkwcbbtpz.yaml | kubectl apply -f -]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次es集群内存溢出的问题]]></title>
    <url>%2F2020%2F03%2F19%2F41-%E8%AE%B0%E4%B8%80%E6%AC%A1es%E9%9B%86%E7%BE%A4%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[es机器报警磁盘 / 空间不足,查看是生成了 .hprof 文件, 内存溢出的典型特征 以上问题主要是两点 由于elasticsearch用户家目录是/home/elasticsearch, 所以内存溢出时 写的.hprof文件会生成到家目录, 并且大小有6G+, 这会导致/目录磁盘空间不足报警, 是否可以设置该日志目录? 或者取巧设置elasticsearch家目录到/data挂载盘上? 内存溢出的问题, 是否可以优化并解决 问题1 我这里并未找到设置.hprof文件的生成目录路径设置, 所以我就将根目录做了一个链接12345mv /home/elasticsearch /data/ln -s /data/elasticsearch /home/elasticsearch或者修改elasticsearch用户的家目录(不过需要用户没有在login中)lsof |grep elasticsearchusermod -d /data/elasticsearch elasticsearch 问题2 内存溢出的问题,我们设置 indices.fielddata.cache.size: 20% elasticsearch2.x 限制内存使用 indices.fielddata.cache.size 控制为 fielddata 分配的堆空间大小。 当你发起一个查询，分析字符串的聚合将会被加载到 fielddata，如果这些字符串之前没有被加载过。如果结果中 fielddata 大小超过了指定 大小 ，其他的值将会被回收从而获得空间。 默认情况下，设置都是 unbounded ，Elasticsearch 永远都不会从 fielddata 中回收数据。这个默认设置是刻意选择的：fielddata 不是临时缓存。它是驻留内存里的数据结构，必须可以快速执行访问，而且构建它的代价十分高昂。如果每个请求都重载数据，性能会十分糟糕。 监控fielddata 按索引使用 indices-stats API ： 1GET /_stats/fielddata?fields=* 按节点使用 nodes-stats API ： 1GET /_nodes/stats/indices/fielddata?fields=* 按索引节点： 1GET /_nodes/stats/indices/fielddata?level=indices&amp;fields=* 使用设置 ?fields=* ，可以将内存使用分配到每个字段。 断路器机敏的读者可能已经发现 fielddata 大小设置的一个问题。fielddata 大小是在数据加载 之后 检查的。 如果一个查询试图加载比可用内存更多的信息到 fielddata 中会发生什么？答案很丑陋：我们会碰到 OutOfMemoryException 。 Elasticsearch 包括一个 fielddata 断熔器 ，这个设计就是为了处理上述情况。 断熔器通过内部检查（字段的类型、基数、大小等等）来估算一个查询需要的内存。它然后检查要求加载的 fielddata 是否会导致 fielddata 的总量超过堆的配置比例。 如果估算查询的大小超出限制，就会 触发 断路器，查询会被中止并返回异常。这都发生在数据加载 之前 ，也就意味着不会引起 OutOfMemoryException 。 12345678910可用的断路器（Available Circuit Breakers）Elasticsearch 有一系列的断路器，它们都能保证内存不会超出限制：indices.breaker.fielddata.limitfielddata 断路器默认设置堆的 60% 作为 fielddata 大小的上限。indices.breaker.request.limitrequest 断路器估算需要完成其他请求部分的结构大小，例如创建一个聚合桶，默认限制是堆内存的 40%。indices.breaker.total.limittotal 揉合 request 和 fielddata 断路器保证两者组合起来不会使用超过堆内存的 70%。 断路器的限制可以在文件 config/elasticsearch.yml 中指定，可以动态更新一个正在运行的集群： 123456PUT /_cluster/settings&#123; "persistent" : &#123; "indices.breaker.fielddata.limit" : "40%" &#125;&#125; 最好为断路器设置一个相对保守点的值。 记住 fielddata 需要与 request 断路器共享堆内存、索引缓冲内存和过滤器缓存。Lucene 的数据被用来构造索引，以及各种其他临时的数据结构。 正因如此，它默认值非常保守，只有 60% 。过于乐观的设置可能会引起潜在的堆栈溢出（OOM）异常，这会使整个节点宕掉。 在 Fielddata的大小 中，我们提过关于给 fielddata 的大小加一个限制，从而确保旧的无用 fielddata 被回收的方法。 indices.fielddata.cache.size 和 indices.breaker.fielddata.limit 之间的关系非常重要。 如果断路器的限制低于缓存大小，没有数据会被回收。为了能正常工作，断路器的限制 必须 要比缓存大小要高。 在设置 Elasticsearch 堆大小时需要通过 $ES_HEAP_SIZE 环境变量应用两个规则：1 不要超过可用 RAM 的 50%Lucene 能很好利用文件系统的缓存，它是通过系统内核管理的。如果没有足够的文件系统缓存空间，性能会受到影响。 此外，专用于堆的内存越多意味着其他所有使用 doc values 的字段内存越少。2 不要超过 32 GB 如果堆大小小于 32 GB，JVM 可以利用指针压缩，这可以大大降低内存的使用：每个指针 4 字节而不是 8 字节。]]></content>
      <categories>
        <category>elk</category>
        <category>elasticsearch5</category>
      </categories>
      <tags>
        <tag>elasticsearch5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次跨域的nginx配置问题]]></title>
    <url>%2F2020%2F03%2F18%2F40-%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%B7%A8%E5%9F%9F%E7%9A%84nginx%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[nginx跨域的Access-Control-Allow-Origin的配置 和多域名配置的问题 简单配置 1. nginx 配置单个域名123add_header Access-Control-Allow-Origin "a.test.com";add_header Access-Control-Allow-Methods GET,HEAD,PUT,PATCH,POST,DELETE;add_header Access-Control-Allow-Headers authorization,sign,vary-client; 2. nginx 配置所有域名1234add_header Access-Control-Allow-Origin "*";add_header Access-Control-Allow-Credentials true;add_header Access-Control-Allow-Methods GET,HEAD,PUT,PATCH,POST,DELETE;add_header Access-Control-Allow-Headers authorization,sign,vary-client; 3. nginx 配置多域名 一开始我是这样配置的: 123456789101112 ###################这里是配置多域名跨域配置set $F_Allow_Origin "127.0.0.1"; #如果是允许的域名则设置Access-Control-Allow-Origin 为该$http_origin if ( "$http_origin" ~ "[a-z]+.zhangzw.com" ) &#123; set $F_Allow_Origin "$http_origin"; &#125; add_header F_Allow_Origin "$http_origin"; add_header Access-Control-Allow-Origin "$http_origin"; add_header Access-Control-Allow-Credentials true; add_header Access-Control-Allow-Methods GET,HEAD,PUT,PATCH,POST,DELETE; add_header Access-Control-Allow-Headers authorization,sign,vary-client; ###################这里是配置多域名跨域配置 测试之后发现页面还是报没有Access-Control-Allow-Origin 头, 原因是我这边由b.test.com -&gt; a.test.com, F_Allow_Origin自定义头并没有向下传递. 123456789101112###################这里是配置多域名跨域配置#如果是允许的域名则设置Access-Control-Allow-Origin 为该$http_origin#if ( "$http_origin" !~ "[a-z]+.zhangzw.com" ) &#123;# return 403;#&#125;add_header Bq_F_Allow_Origin "$http_origin";#add_header Access-Control-Allow-Origin "$http_origin";add_header Access-Control-Allow-Origin "*";add_header Access-Control-Allow-Credentials true;add_header Access-Control-Allow-Methods GET,HEAD,PUT,PATCH,POST,DELETE;add_header Access-Control-Allow-Headers authorization,sign,vary-client;###################这里是配置多域名跨域配置]]></content>
      <categories>
        <category>技术文档</category>
        <category>nginx</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次nginx的request_time 和upstream_response_time差值很大问题]]></title>
    <url>%2F2020%2F03%2F18%2F39-%E8%AE%B0%E4%B8%80%E6%AC%A1nginx%E7%9A%84request-time-%E5%92%8Cupstream-response-time%E5%B7%AE%E5%80%BC%E5%BE%88%E5%A4%A7%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[遇到一个接口, 经过了nginx反向代理,request_time时间是60s+, upstream_response_time 在0.5s左右 首先问题描述我们发现后端响应时间没问题, 从前端和后端的日志都发现响应状态是200, 说明请求都是正常的 那为啥会响应时间这么长呢? request_time: 指的是 (Nginx 建立连接 到 接收完数据并关闭连接)从代理nginx到后端(这里是php)建立连接到接受完数据然后关闭连接为止的时间 upstream_response_time: 指的是 (接受用户请求的第一个字节 到 发送完响应数据)从接受用户请求的第一个字节 到发送完响应数据的时间(包括接受请求数据时间,程序响应时间,输出响应时间) 通过查看日志发现响应返回的字节量在 300k左右, 于是去看了下前端nginx的带宽, 并没有发现超过100%, 而且日志的同一时间的并不是所有请求都超过60s+ 因此看起来服务端也正常, 应该是客户端问题 通过询问开发, 发现是测试在本机疯狂的点击,导致并发高, 而测试的网络环境是限速5m, 显然客户端带宽接收数据限制导致了服务端发送延迟. 显然我们知道request_time: 指的是 从接受用户请求的第一个字节 到发送完响应数据的时间(包括接受请求数据时间,程序响应时间,输出响应时间)]]></content>
      <categories>
        <category>技术文档</category>
        <category>nginx</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es集群节点出现overhead脱机的问题]]></title>
    <url>%2F2020%2F03%2F12%2F38-es%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E5%87%BA%E7%8E%B0overhead%E8%84%B1%E6%9C%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[elasticsearch 日志提示 overhead, 导致集群出现问题 问题说明 elasticsearch 日志提示 overhead123456[2020-03-12T14:38:03,565][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][old][3008939][256208] duration [18.4s], collections [1]/[18.9s], total [18.4s]/[5.7h], memory [7.3gb]-&gt;[7.3gb]/[7.9gb], all_pools &#123;[young] [17.5mb]-&gt;[3.3mb]/[532.5mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[66.5mb]&#125;&#123;[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]&#125;[2020-03-12T14:38:03,593][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][3008939] overhead, spent [18.4s] collecting in the last [18.9s][2020-03-12T14:37:44,632][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][old][3008938][256207] duration [24.8s], collections [1]/[25.5s], total [24.8s]/[5.7h], memory [7.3gb]-&gt;[7.3gb]/[7.9gb], all_pools &#123;[young] [8.5mb]-&gt;[17.5mb]/[532.5mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[66.5mb]&#125;&#123;[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]&#125;[2020-03-12T14:37:44,632][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][3008938] overhead, spent [24.8s] collecting in the last [25.5s] 查看elasticsearch 配置 heap size 是8G ES 内存使用和GC指标——默认情况下，主节点每30秒会去检查其他节点的状态，如果任何节点的垃圾回收时间超过30秒（Garbage collection duration），则会导致主节点任务该节点脱离集群。 设置过大的heap会导致GC时间过长，这些长时间的停顿（stop-the-world）会让集群错误的认为该节点已经脱离。 所以通过增加ping_timeout的时间，和增加ping_retries的次数来防止节点错误的脱离集群，可以使节点有充足的时间进行full GC。 问题解决 这里将默认的超时时间增加, 增加重试次数, 增加间隔时间1234#超时时间设为5分钟，超过6次心跳没有回应，则认为该节点脱离master，每隔60s发送一次心跳。 discovery.zen.fd.ping_timeout: 300s discovery.zen.fd.ping_retries: 6 discovery.zen.fd.ping_interval: 60s gc 垃圾回收算法 摘自原文: https://www.jianshu.com/p/1f450826f62e 标记-清除 算法(Mark Sweep)该算法很简单，使用通过可达性分析分析方法标记出垃圾，然后直接回收掉垃圾区域。它的一个显著问题是一段时间后，内存会出现大量碎片，导致虽然碎片总和很大，但无法满足一个大对象的内存申请，从而导致 OOM，而过多的内存碎片（需要类似链表的数据结构维护），也会导致标记和清除的操作成本高，效率低下，如下图所示： 复制算法(Copying)有人提出了复制算法。它将可用内存一分为二，每次只用一块，当这一块内存不够用时，便触发 GC，将当前存活对象复制(Copy)到另一块上，以此往复。这种算法高效的原因在于分配内存时只需要将指针后移，不需要维护链表等。但它最大的问题是对内存的浪费，使用率只有 50% 标记-整理算法(Mark Compact)该算法解决了第1中算法的内存碎片问题，它会在回收阶段将所有内存做整理 分代收集算法(Generation Collection)既然大部分 Java 对象是朝生夕死的，那么我们将内存按照 Java 生存时间分为 新生代(Young) 和 老年代(Old)，前者存放短命僧，后者存放长寿佛，当然长寿佛也是由短命僧升级上来的。然后针对两者可以采用不同的回收算法，比如对于新生代采用复制算法会比较高效，而对老年代可以采用标记-清除或者标记-整理算法。这种算法也是最常用的。JVM Heap 分代后的划分一般如下所示，新生代一般会分为 Eden、Survivor0、Survivor1区，便于使用复制算法。 将内存分代后的 GC 过程一般类似下图所示： 1 对象一般都是先在 Eden区创建2 当Eden区满，触发 Young GC，此时将 Eden中还存活的对象复制到 S0中，并清空 Eden区后继续为新的对象分配内存3 当Eden区再次满后，触发又一次的 Young GC，此时会将 Eden和S0中存活的对象复制到 S1中，然后清空Eden和S0后继续为新的对象分配内存4 每经过一次 Young GC，存活下来的对象都会将自己存活次数加1，当达到一定次数后，会随着一次 Young GC 晋升到 Old区5 Old区也会在合适的时机进行自己的 GC elasticsearch gc说明 Elasticsearch 默认的 GC 配置是CMS GC ，其 Young 区用 ParNew，Old 区用CMS，大家可以在 config/jvm.options中看到如下的配置： 1234## GC configuration-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSInitiatingOccupancyOnly 何时进行回收1231 Young 区的GC 都是在 Eden 区满时触发2 Serial Old 和 Parallel Old 在 Old 区是在 Young GC 时预测Old 区是否可以为 young 区 promote 到 old 区 的 object 分配空间，如果不可用则触发 Old GC。这个也可以理解为是 Old区满时。3 CMS GC 是在 Old 区大小超过一定比例后触发，而不是 Old 区满。这个原因在于 CMS GC 是并发的算法，也就是说在 GC 线程收集垃圾的时候，用户线程也在运行，因此需要预留一些 Heap 空间给用户线程使用，防止由于无法分配空间而导致 Full GC 发生。 gc 日志说明123[2020-03-12T14:38:03,565][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][old][3008939][256208] duration [18.4s], collections [1]/[18.9s], total [18.4s]/[5.7h], memory [7.3gb]-&gt;[7.3gb]/[7.9gb], all_pools &#123;[young] [17.5mb]-&gt;[3.3mb]/[532.5mb]&#125;&#123;[survivor] [0b]-&gt;[0b]/[66.5mb]&#125;&#123;[old] [7.3gb]-&gt;[7.3gb]/[7.3gb]&#125;[2020-03-12T14:38:03,593][WARN ][o.e.m.j.JvmGcMonitorService] [es7-u] [gc][3008939] overhead, spent [18.4s] collecting in the last [18.9s] 本次是old gc, 这是第3008939次GC检查, 从java启动至今这是第256208次 gc 共花18.4s, [从上次检查至今共发生一次gc][从上次检查至今已经过去18.9s],[本次gc18.4s]/[从 JVM 启动至今发生的 GC 总耗时为5.7h], [ GC 前 Heap memory 空间]-&gt;[GC 后 Heap memory 空间]/[Heap memory 总空间] {[young 区][GC 前 Memory ]-&gt;[GC后 Memory]/[young区 Memory 总大小] } {[survivor 区][GC 前 Memory ]-&gt;[GC后 Memory]/[survivor区 Memory 总大小] }{[old 区][GC 前 Memory ]-&gt;[GC后 Memory]/[old区 Memory 总大小] }]]></content>
      <categories>
        <category>elk</category>
        <category>elasticsearch5</category>
      </categories>
      <tags>
        <tag>elasticsearch5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac一些常用命令]]></title>
    <url>%2F2020%2F03%2F10%2F37-mac%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[记录一些常用的mac工具和方法 1. Item2 1.1 同时按住 option(alt) 键，可以以列选中，类似于 sublime3中 按住 alt的列选中 。Command + option(alt) 1.2 剪贴板历史记录Command + Shift + h 1.3 将文本内容复制到剪切板pbcopy &lt; a.txt]]></content>
      <categories>
        <category>技术文档</category>
        <category>mac</category>
        <category>item2</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos单网卡配置多ip的几种方法]]></title>
    <url>%2F2020%2F02%2F27%2F36-centos%E5%8D%95%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE%E5%A4%9Aip%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[centos单网卡配置多ip的几种方法 方法一 新建IP别名 临时设置, 不需要重启 12ifconfig enp0s3:1 192.168.53.109/24ifconfig enp0s3:1 down 配置文件设置, 需要重启 12345678910#cat ifcfg-enp0s3:1DEVICE=enp0s3IPADDR=192.168.53.109NETMASK=255.255.255.0# 重启网络service network restart# 查看(ifconfig 也可以查看)ip a 或ifconfig 方法二 临时设置, 不需要重启1ip addr add 192.168.53.110/24 dev enp0s3 label enp0s3:2 方法三 临时设置, 不需要重启1ifconfig enp0s3:3 192.168.53.111 netmask 255.255.255.0 方法四 同一个配置文件设置, 需要重启。IP地址没有别名不好进行管理。1234567891011121314#cat ifcfg-enp0s3DEVICE=enp0s3IPADDR=192.168.53.106IPADDR1=192.168.53.112IPADDR2=192.168.53.113PREFIX=24PREFIX1=24PREFIX2=24# 重启网络service network restart# 查看(ifconfig 不可以查看)ip a 注:这里奇怪的是, 实际配置中,出现个别ip使用方法二,三时仅部分内网可以联通,例如10.10.76.1 通过方法二配置, 从10.10.76.2上可以ping通, 但是从10.10.53.1上无法ping通(10.10.53.1和10.10.76.2是可以ping通)但是通过方法四配置就正常… 目前没有找到原因… 或与公司路由器有关]]></content>
      <categories>
        <category>技术文档</category>
        <category>linux</category>
        <category>网卡</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raid1盘数据迁移]]></title>
    <url>%2F2020%2F02%2F27%2F35-raid1%E7%9B%98%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[dell PowerEdge 1950 服务器两块盘做raid1的linux操作系统, 开机后无限重启的一次数据迁移 考虑到raid1数据是互为备份,直接取一块盘应该能够拿到所有数据 首先对dell PowerEdge 1950 服务器 开机, 在提示ctrl +c的页面上进入sas页面, 进入选中磁盘后回车, 然后选择 SAS Topology页面, 可以看到是两块盘做的raid1 raid1 信息确认完毕 关闭1950服务器, 取下其中一块盘, 这里看到硬盘是sata盘 考虑到该盘不确定是否支持热插拔, 这里是将sata盘放入usb盘接到某台Linux服务器, 然后挂载, 挂载注意fdisk -l 看下具体分区, 我这里是/dev/sdb3 mount /data /dev/sdb3 进入/data, 就会看到raid1硬盘中保留的所有数据]]></content>
      <categories>
        <category>技术文档</category>
        <category>raid</category>
      </categories>
      <tags>
        <tag>raid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s一些命令总结]]></title>
    <url>%2F2019%2F12%2F05%2F34-k8s%E4%B8%80%E4%BA%9B%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[记录一些kubectl命令 kubectl命令表 常用命令1234567891011121314# 让内网可以访问 k8s proxy(k8smaster是:192.168.1.111kubectl proxy --address='192.168.1.111' -p 10000 --accept-hosts='^172.*$'# 查看api类型kubectl api-versions # 让master也运行pod（默认master不运行pod,单机会用到）kubectl taint nodes --all node-role.kubernetes.io/master-# patch补丁, 强制更新kubectl patch -f deployment.yaml -p "&#123;\"spec\":&#123;\"template\":&#123;\"metadata\":&#123;\"annotations\":&#123;\"ci-last-updated\":\"$(date +'%s')\"&#125;&#125;&#125;&#125;&#125;"# 端口转发kubectl -n default port-forward service/prometheus-server 30080:80 scale 使用123# 通过将rc的副本数重新设置为0后，再将副本数设置为2，达到重启nginx的效果。kubectl scale deployment bq-front1 --replicas=0 -n webkubectl scale deployment bq-front1 --replicas=2 -n web metrics 相关123456789# 查看node 资源kubectl top nodes# 查看pods 资源kubectl top pods -n php-dev# 获取metrics接口所有数据kubectl get --raw /metrics# patch强制更新(慎用)kubectl patch -f deployment.yaml -p "&#123;\"spec\":&#123;\"template\":&#123;\"metadata\":&#123;\"annotations\":&#123;\"ci-last-updated\":\"$(date +'%s')\"&#125;&#125;&#125;&#125;&#125;" 根据版本缩放123456789101112#查看Deployment的变更信息（以下信息得以保存，是创建时候加的“--record”这个选项起的作用）：kubectl rollout history deployment/bq-nginx-php7 -n webkubectl rollout undo deployment/bq-nginx-php7 # 回退到上一版本kubectl rollout undo deployment/bq-nginx-php7 --to-revision=2 # 回退到指定版本kubectl describe deployments/bq-nginx-php7 -n web #查询详细信息，获取升级进度kubectl rollout pause deployment/bq-nginx-php7 -n web #暂停升级kubectl rollout resume deployment/bq-nginx-php7 -n web #继续升级kubectl rollout undo deployment/bq-nginx-php7 -n web #升级回滚kubectl scale deployment bq-nginx-php7 --replicas 2 -n web #弹性伸缩Pod数量kubectl get ns --show-labels # 查看标签,除了ns, 也可以是node,pod等]]></content>
      <categories>
        <category>k8s</category>
        <category>kubectl</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubectl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k3s安装配置]]></title>
    <url>%2F2019%2F12%2F03%2F29-k3s%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[体验轻量级k8s集群,适用于低配个人开发测试使用 k3s, 5 less than k8s 详情参考官方: k3s github地址 准备 1 selinux 关闭 123456getenforce# 本次关闭setenforce 0# 重启后关闭sed -i '/SELINUX=enforcing/s/enforcing/disabled/' /etc/sysconfig/selinux 2 关闭swap(可选) 1234# 本次关闭swapoff on# 重启后关闭sed -i '/swap/s@^/@#/@' /etc/fstab 3 关闭firewalld(必须) 12systemctl stop firewalld.servicesystemctl disable firewalld.service 4 在内核3.10,4.16,5.2,5.3 都正常运行 Step 1: 安装K3S集群1234567891011121314151617181920212223# 下载k3s 二进制文件打开各版本点击详情可以查询k3s版本对应的k8s版本(https://github.com/rancher/k3s/releases)wget https://github.com/rancher/k3s/releases/download/v1.0.0/k3sk3s v1.0.0 -&gt; k8s1.16.3# https://github.com/rancher/k3s/tagsk3s v0.9.0 -&gt; k8s1.15.4k3s v0.10.0 -&gt; k8s1.16.2我这里下载最新的k3s v1.0.0, 但是由于metrics-server好像对k8s1.16.3最新有点问题, 还是先等待metrics-server更新把测试了k3s v0.10.0 测试了下, 但遗憾的是默认好像没有安装metrics-server...mv k3s /usr/local/bin/k3schmod +x /usr/local/bin/k3s#k3s --versionk3s version v1.0.0 (18bd921c)# 下载pause镜像(这里举1,其他国内地址参考官方)docker pull registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1docker tag registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1 k8s.gcr.io/pause:3.1# 验证一下docker images | grep "k8s.gcr.io/pause" Step 2: 安装k3s server12345678910111213141516# centos官方安装curl -sfL https://get.k3s.io | sh -# 至此server已经安装完了,但由于k8s默认是用Containerd, 并非docker, 所以需要手工修改配置(当然如果你熟悉ctr 操作Containerd也没问题)# 修改ExecStart内容# 1: --docker 表示k3s server使用docker引擎# 2: --no-deploy traefik 表示不安装traefikvim /etc/systemd/system/multi-user.target.wants/k3s.serviceExecStart=/usr/local/bin/k3s server --docker --no-deploy traefik# 启动服务systemctl daemon-reloadservice k3s restart# 验证k3s kubectl get node 想去掉k3s命令? kubectl命令管理k3s 1234567# 简单做一个aliasalias kubectl='k3s kubectl'# 或者rm -rf ~/.kubemkdir -p ~/.kubecp /etc/rancher/k3s/k3s.yaml ~/.kube/config Step 3: 客户端安装参考官方文档 安装和配置选项 1234# 同样下载二进制包wget https://github.com/rancher/k3s/releases/download/v1.0.0/k3smv k3s /usr/local/bin/k3schmod +x /usr/local/bin/k3s 加入到server有两种 手动加入 (其实上面我们已经拉取了image, 并且tag成官方地址了,所以这里也可以不用指定) 12nohup k3s agent --docker --pause-image registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1 --server https://k3s-server:6443 --token $&#123;NODE_TOKEN&#125; &amp;nohup k3s agent --docker --server https://k3s-server:6443 --token $&#123;NODE_TOKEN&#125; &amp; 脚本加入 12345curl -sfL https://get.k3s.io | K3S_URL=https://k3s-server:6443 K3S_TOKEN=$&#123;NODE_TOKEN&#125; INSTALL_K3S_EXEC="agent --docker --pause-image registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1" sh -s -curl -sfL https://get.k3s.io | K3S_URL=https://k3s-server:6443 K3S_TOKEN=$&#123;NODE_TOKEN&#125; INSTALL_K3S_EXEC="agent --docker" sh -s -# ps aux|grep k3s/usr/local/bin/k3s agent --docker --pause-image registry.cn-beijing.aliyuncs.com/ilemonrain/pause-amd64:3.1 当然如下差别不大, 都是会启动一个k3s的进程 rancher import1curl --insecure -sfL https://rancher-dev.xxx.com/v3/import/x8jc277zmjkxjgcmc9f67pzn9f7ffsjpszlv9dxc79vhmndqcms4nr.yaml | k3s kubectl apply -f - 卸载1sh /usr/local/bin/k3s-uninstall.sh]]></content>
      <categories>
        <category>技术文档</category>
        <category>k3s</category>
      </categories>
      <tags>
        <tag>k3s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k3s1.16部署nginx+php5.2.17]]></title>
    <url>%2F2019%2F12%2F03%2F33-k3s1.16%E9%83%A8%E7%BD%B2nginx%2Bphp5.2.17%2F</url>
    <content type="text"><![CDATA[老项目是用php5.2.17的,自己编译打包镜像简单部署 开始部署 准备dockerfile Dockerfile 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798FROM centos:6.9MAINTAINER zhangzw zhangzw@zhangzw.comENV PHP_DIR /usr/local/phpENV WORK_DIR_tar /usr/loca/src/ENV PHP_VERSION 5.2.17ENV PHP_EXT_CURL curl-7.20.0# php 及扩展 包,包括以下内容# php-5.2.17-patch-fpm.tar.gz curl-7.20.0.tar.gz freetype-2.4.0.tar.gz ImageMagick-6.9.0-4.tar.gz imagick-3.0.1.tgz zendopcache-7.0.5.tgz phpredis-2.2.2.zip # php-fpm.conf php.ini copy tar $&#123;WORK_DIR_tar&#125;run yum install -y wget \ &amp;&amp; wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo \ &amp;&amp; rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 \ &amp;&amp; yum install -y epel-release \ &amp;&amp; yum install -y freetype freetype-devel gcc make cmake ncurses-devel gcc-c++ autoconf automake zlib-devel dos2unix nc lrzsz openssl-devel pcre-devel libxml2 libxml2-devel libcurl libcurl-devel libpng-devel bzip2-devel libjpeg libjpeg-turbo-devel libmcrypt-devel mhash-devel mysql-devel libtool-ltdl libtool-ltdl-devel git bzip2-devel git supervisor autoconf automake xz unzip \ &amp;&amp; yum clean all &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; ls *gz|xargs -i tar -xf &#123;&#125; \ &amp;&amp; cd $&#123;PHP_EXT_CURL&#125; \ &amp;&amp; ./configure --prefix=/usr/local/curl \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd php-$&#123;PHP_VERSION&#125; \ &amp;&amp; ln -s /usr/lib64/libpng.so /usr/lib/ \ &amp;&amp; ln -s /usr/lib64/libjpeg.so /usr/lib/ \ &amp;&amp; ln -s /usr/lib64/mysql/libmysqlclient.so.16.0.0 /usr/lib/libmysqlclient.so \ &amp;&amp; ./configure \ --prefix=$&#123;PHP_DIR&#125; \ --with-config-file-path=$&#123;PHP_DIR&#125;/etc \ --with-mysql \ --with-mysqli \ --with-openssl \ --enable-fastcgi \ --enable-fpm \ --enable-mbstring \ --enable-bcmath \ --with-freetype-dir \ --with-jpeg-dir \ --with-png-dir \ --with-zlib-dir \ --with-libxml-dir=/usr \ --enable-xml \ --with-mhash \ --with-mcrypt \ --enable-pcntl \ --enable-sockets \ --with-bz2 \ --with-curl=/usr/local/curl \ --with-curlwrappers \ --enable-mbregex \ --with-gd \ --enable-gd-native-ttf \ --enable-zip \ --enable-soap \ --with-iconv \ --enable-pdo \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd ImageMagick-6.9.0-4 \ &amp;&amp; ./configure --prefix=/usr/local/imagemagick \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd imagick-3.0.1 \ &amp;&amp; ln -s /usr/local/imagemagick/include/ImageMagick-6 /usr/local/imagemagick/include/ImageMagick \ &amp;&amp; $&#123;PHP_DIR&#125;/bin/phpize \ &amp;&amp; ./configure --with-php-config=$&#123;PHP_DIR&#125;/bin/php-config --with-imagick=/usr/local/imagemagick \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; unzip phpredis-2.2.2.zip \ &amp;&amp; cd phpredis-2.2.2 \ &amp;&amp; $&#123;PHP_DIR&#125;/bin/phpize \ &amp;&amp; ./configure --with-php-config=$&#123;PHP_DIR&#125;/bin/php-config \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; cd zendopcache-7.0.5 \ &amp;&amp; $&#123;PHP_DIR&#125;/bin/phpize \ &amp;&amp; ./configure --with-php-config=$&#123;PHP_DIR&#125;/bin/php-config \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; groupadd -r www \ &amp;&amp; useradd -M -s /sbin/nologin -r -g www www \ &amp;&amp; cd $&#123;WORK_DIR_tar&#125; \ &amp;&amp; \cp -r $&#123;WORK_DIR_tar&#125;/php-fpm.conf $&#123;PHP_DIR&#125;/etc/ \ &amp;&amp; \cp -r $&#123;WORK_DIR_tar&#125;/php.ini $&#123;PHP_DIR&#125;/etc/ \ &amp;&amp; rm -rf $&#123;WORK_DIR_tar&#125;copy supervisord-fpm.conf /etc/supervisord.confcopy start.sh /root/start.shENTRYPOINT ["/bin/sh", "/root/start.sh"] build 打包 123# 配置自己的私有仓库地址docker build -t xxx.com/centos-php:5.2.17 .docker push xxx.com/centos-php:5.2.17 可参考k3s安装教程: k3s安装配置 官方教程 在k3s中启动(这里本地挂载方式,单节点) nginx.conf 部分配置 1234567891011121314151617181920212223server &#123; listen 80 default_server; server_name _; access_log /webwww/nginx_logs/test_access.log main; error_log /webwww/nginx_logs/test_error.log debug; root /webwww/test; location = /50x.html &#123; root html; &#125; location / &#123; index index.php index.html index.htm; &#125; location ~ \.php$ &#123; fastcgi_pass php-fpm-dev:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param HTTP_HOST $server_name; include fastcgi_params; &#125;&#125; nginx部署 nginx/php-nginx-dev.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758---apiVersion: apps/v1kind: Deploymentmetadata: name: php-nginx-dev namespace: php-devspec: replicas: 1 selector: matchLabels: app: php-nginx-dev template: metadata: labels: app: php-nginx-dev spec: containers: - name: php-nginx-dev image: hub.zhangzw.com/bq/nginx:1.15.12 ports: - containerPort: 80 name: nginx-80 protocol: TCP resources: requests: cpu: "10m" limits: cpu: "500m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: nginx-cfg-dev mountPath: "/etc/nginx/nginx.conf" volumes: - name: nginx-www-dev hostPath: path: /data/k8s-container/php-5.2.17/webwww-data - name: nginx-cfg-dev hostPath: path: /data/k8s-container/php-5.2.17/nginx/nginx.conf---kind: ServiceapiVersion: v1metadata: labels: app: php-nginx-dev name: php-nginx-dev-service namespace: php-devspec: type: NodePort ports: - name: nginx-80 port: 80 targetPort: 80 nodePort: 32001 protocol: TCP selector: app: php-nginx-dev fpm 部署配置 php-fpm/php-fpm-dev.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061---apiVersion: apps/v1kind: Deploymentmetadata: name: php-fpm-dev namespace: php-devspec: replicas: 1 selector: matchLabels: app: php-fpm-dev template: metadata: labels: app: php-fpm-dev spec: containers: - name: php-fpm-dev image: hub.zhangzw.com/bq/centos-php:5.2.17 ports: - containerPort: 9000 name: fpm-9000 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "1500m" volumeMounts: - name: nginx-www-dev mountPath: /webwww - name: php-cfg-dev mountPath: "/usr/local/php/etc/php.ini" - name: fpm-cfg-dev mountPath: "/usr/local/php/etc/php-fpm.conf" volumes: - name: nginx-www-dev hostPath: path: /data/k8s-container/php-5.2.17/webwww-data - name: php-cfg-dev hostPath: path: /data/k8s-container/php-5.2.17/php-fpm/php.ini - name: fpm-cfg-dev hostPath: path: /data/k8s-container/php-5.2.17/php-fpm/php-fpm.conf---apiVersion: v1kind: Servicemetadata: name: php-fpm-dev namespace: php-devspec: clusterIP: None selector: app: php-fpm-dev ports: - name: fpm-9000 port: 9000--- 部署命令123# 先启动fpm,否则nginx会报错找不到 php-fpm-devkubectl apply -f php-fpm/php-fpm-dev.ymlkubectl apply -f nginx/php-nginx-dev.yml]]></content>
      <categories>
        <category>技术文档</category>
        <category>k3s</category>
        <category>lnmp</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>k8s</tag>
        <tag>k3s</tag>
        <tag>php5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php错误502问题总结]]></title>
    <url>%2F2019%2F12%2F02%2F32-php%E9%94%99%E8%AF%AF502%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[近期一次 502报错, 但是没有达到timeout的值,特此记录 查看error.log 内容: recv() failed (104: Connection reset by peer) while reading response header from upstream12345678910一般来说502 主要是fpm超时进程中止了 或者就是 内存不足导致 fpm中止这里查看了fpm配置 request_terminate_timeout值, 发现并不是该原因通过 top 查看了fpm内存, 其中单个内存已经达到了700M这里fpm配置的pm 是static, 由于该服务并发不是很高, 可以适当减少max_children的值, 或者采用dynamic 动态方式这里设置的max_requests = 5500, 可以考虑减小该值 问题解决1231 这里对fpm reload 即可2 改动dynamic 方式3 合理配置max_requests]]></content>
      <categories>
        <category>技术文档</category>
        <category>php</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[systemd一些命令]]></title>
    <url>%2F2019%2F11%2F26%2F31-systemd%E4%B8%80%E4%BA%9B%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[记录一些需要systemd命令 systemd命令 查看服务启动配置: systemctl cat k3s 查看开机启动的服务列表：systemctl list-unit-files|grep enabled 查看启动失败的服务列表：systemctl --failed 重启服务：systemctl restart firewalld.service 显示状态：systemctl status firewalld.service 开机启用服务：systemctl enable firewalld.service 开机禁用服务：systemctl disable firewalld.service 查看开机启动：systemctl is-enabled firewalld.service]]></content>
      <categories>
        <category>linux</category>
        <category>systemd</category>
      </categories>
      <tags>
        <tag>systemd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[29-k3s集群部署项目报挂载nfs错误]]></title>
    <url>%2F2019%2F11%2F25%2F29-k3s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E6%8A%A5%E6%8C%82%E8%BD%BDnfs%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[k3s集群部署项目报挂载nfs错误]]></title>
    <url>%2F2019%2F11%2F25%2F30-k3s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E6%8A%A5%E6%8C%82%E8%BD%BDnfs%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[体验轻量级k8s集群遇到一些nfs问题 部署服务器查看describe信息如下:1234567Mounting command: systemd-runMounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/369daaef-1e90-446b-92ce-3d562f94b429/volumes/kubernetes.io~nfs/pvc-f462c606-5796-4c48-8928-7822f3fa0605 --scope -- mount -t nfs 192.168.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-es520-2-dev-1-pvc-f462c606-5796-4c48-8928-7822f3fa0605 /var/lib/kubelet/pods/369daaef-1e90-446b-92ce-3d562f94b429/volumes/kubernetes.io~nfs/pvc-f462c606-5796-4c48-8928-7822f3fa0605Output: Running scope as unit run-14829.scope.mount: 文件系统类型错误、选项错误、192.168.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-es520-2-dev-1-pvc-f462c606-5796-4c48-8928-7822f3fa0605 上有坏超级块、 缺少代码页或助手程序，或其他错误 (对某些文件系统(如 nfs、cifs) 您可能需要 一款 /sbin/mount.&lt;类型&gt; 助手程序) 分析 猜测1 可能是nfs的系统格式和集群node节点文件格式不同 1234# 查看发现nfs是ext4, 然后集群中其他的磁盘都是xfsdf -T|egrep -v "contai|var|overl"所以新挂了块磁盘,格式化为xfs然后再次实验,发现错误同样... 猜测2 可能是客户端无法识别nfs格式 12345678910111213# 做个测试mkdir /tmp/abcmount -t nfs 192.168.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-es520-2-dev-1-pvc-f462c606-5796-4c48-8928-7822f3fa0605 /tmp/abc# 果然报错mount: wrong fs type, bad option, bad superblock on 192.168.x.x:/data-nfs/nfs/k3s/ns-elastic5-es520-2-dev-nfs-plugins, missing codepage or helper program, or other error (for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.&lt;type&gt; helper program) In some cases useful info is found in syslog - try dmesg | tail or so. 所以安装了nfs即可 1yum install nfs]]></content>
      <categories>
        <category>技术文档</category>
        <category>k3s</category>
        <category>nfs</category>
      </categories>
      <tags>
        <tag>k3s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s1.16使用旧yml部署配置问题]]></title>
    <url>%2F2019%2F11%2F25%2F28-k8s1-16%E4%BD%BF%E7%94%A8%E6%97%A7yml%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[使用k8s 1.16遇到的问题 1 appversion的改变12no matches for kind "StatefulSet" in version "apps/v1beta1"no matches for kind "Deployment" in version "extensions/v1beta1" 1234567891011121314# Deployment(extensions/v1beta1 舍弃)apiVersion: extensions/v1beta1 -&gt; apiVersion: apps/v1# StatefulSetapiVersion: apps/v1beta1 -&gt; apiVersion: apps/v1# 然后根据提示在spec 下添加selector.matchLabelsspec: replicas: 3 selector: matchLabels: app: test1可以通过kubectl convert 更新yaml文件 完整示例: 123456789101112131415161718192021222324252627&gt; nginx1.yml &lt;&lt;- EOF apiVersion: apps/v1kind: Deploymentmetadata: name: nginx1spec: replicas: 1 selector: matchLabels: app: nginx1 template: metadata: labels: app: nginx1 spec: containers: - image: nginx name: nginx1 imagePullPolicy: Always resources: requests: cpu: "10m" memory: "10Mi" limits: cpu: "100m" memory: "50Mi"EOF]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s一条命令部署es5.2.0集群]]></title>
    <url>%2F2019%2F11%2F21%2F27-k8s%E4%B8%80%E6%9D%A1%E5%91%BD%E4%BB%A4%E9%83%A8%E7%BD%B2es5-2-0%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[由于老项目是基于es5.2.0, 所以准备在k8s基于nfs存储搭建一套,下面简单介绍 准备好环境和官方镜像12341 镜像: elasticsearch:5.2.02 k8s环境: k8s.1.103 存储: nfs storageclass 存储4 插件: ik分词压缩包(这里ik分词直接使用旧的es配置, 也可以自行下) 开始部署 部署命令1kubectl apply -f k8s-StatefulSet-es520-nfs.yml 配置文件 k8s-StatefulSet-es520-nfs.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: es520-2-dev namespace: ns-elastic5spec: serviceName: "es520-2-dev" replicas: 2 volumeClaimTemplates: - metadata: name: es520-2-dev-nfs annotations: volume.beta.kubernetes.io/storage-class: "nfs-retain" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 2Gi template: metadata: labels: app: es520-2-dev spec: containers: - name: es520-2-dev image: elasticsearch:5.2.0 ports: - containerPort: 9200 name: es520-2-9200 protocol: TCP - containerPort: 9300 name: es520-2-9300 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "500m" volumeMounts: - name: es520-2-dev-nfs mountPath: /usr/share/elasticsearch/data/ - name: es520-2-dev-cfg mountPath: /usr/share/elasticsearch/config/elasticsearch.yml subPath: elasticsearch.yml - name: es520-2-dev-jvm mountPath: /usr/share/elasticsearch/config/jvm.options subPath: jvm.options - name: es520-2-dev-plu mountPath: /usr/share/elasticsearch/plugins volumes: - name: es520-2-dev-cfg configMap: name: es520-2-dev-cfg items: - key: elasticsearch.yml path: elasticsearch.yml - name: es520-2-dev-jvm configMap: name: es520-2-dev-jvm items: - key: jvm.options path: jvm.options - name: es520-2-dev-plu nfs: server: 192.168.53.106 path: /data/nfs/k3s/ns-elastic5-es520-2-dev-nfs-plugins---kind: ServiceapiVersion: v1metadata: labels: app: es520-2-dev name: es520-2-dev namespace: ns-elastic5spec: type: NodePort ports: - name: es520-2-9200 port: 9200 targetPort: 9200 nodePort: 31201 protocol: TCP - name: es520-2-9300 port: 9300 targetPort: 9300 nodePort: 31301 protocol: TCP selector: app: es520-2-dev---apiVersion: v1kind: Servicemetadata: name: es520-2-dev-hlspec: clusterIP: None selector: app: es520-2-dev ports: - name: es520-2-9200 port: 9200 - name: es520-2-9300 port: 9300---apiVersion: v1kind: ConfigMapmetadata: name: es520-2-dev-cfg namespace: ns-elastic5data: elasticsearch.yml: | cluster.name: k8s-test-nfs network.host: 0.0.0.0 bootstrap.system_call_filter: false discovery.zen.ping.unicast.hosts: ["es520-2-dev-0.es520-2-dev:9300","es520-2-dev-1.es520-2-dev:9300","es520-2-dev-2.es520-2-dev:9300"] http.cors.enabled: true http.cors.allow-origin: "*" thread_pool.bulk.queue_size: 3000---apiVersion: v1kind: ConfigMapmetadata: name: es520-2-dev-jvm namespace: ns-elastic5data: jvm.options: | -Xms512m -Xmx512m -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+DisableExplicitGC -XX:+AlwaysPreTouch -server -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -Djdk.io.permissionsUseCanonicalPath=true -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j.skipJansi=true -XX:+HeapDumpOnOutOfMemoryError 效果图 rancher 上效果 elasticsearch-head 上效果图]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>elk</category>
        <category>elasticsearch5</category>
        <category>elk5</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>elasticsearch5</tag>
        <tag>elk</tag>
        <tag>elk5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash配置]]></title>
    <url>%2F2019%2F11%2F08%2F26-logstash%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[记录一些logstash的配置问题 想要排除某些信息官方文档 12345678if "_grokparsefailure" not in [tags] &#123; if [sqlDuring] &lt; 5 &#123; drop &#123;&#125; &#125;&#125;else &#123; drop &#123;&#125;&#125; 请务必注意 如果failure 可能会导致找不到 sqlDuring变量 而报错 1[2019-12-05T17:53:03,017][FATAL][logstash.runner ] An unexpected error occurred! &#123;:error=&gt;#&lt;NoMethodError: undefined method `&lt;' for nil:NilClass&gt;, :backtrace=&gt;["(eval):139:in `initialize'", "org/jruby/RubyArray.java:1613:in `each'", "(eval):137:in `initialize'", "org/jruby/RubyProc.java:281:in `call'", "(eval):96:in `filter_func'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:260:in `filter_batch'", "org/jruby/RubyProc.java:281:in `call'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb:186:in `each'", "org/jruby/RubyHash.java:1342:in `each'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb:185:in `each'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:258:in `filter_batch'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:246:in `worker_loop'", "/usr/local/logstash-5.0.2/logstash-core/lib/logstash/pipeline.rb:225:in `start_workers'"]&#125;]]></content>
      <categories>
        <category>elk</category>
        <category>logstash</category>
      </categories>
      <tags>
        <tag>logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux一些脚本汇总]]></title>
    <url>%2F2019%2F11%2F01%2F25-linux%E4%B8%80%E4%BA%9B%E8%84%9A%E6%9C%AC%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录一些shell脚本 1 清理es几天前的索引脚本2 从mysql导出表到clickhouse脚本 ###命令汇总 1. 生成字符串1tr -dc A-Za-z0-9_@$\%\^\/\+ &lt; /dev/urandom|head -c 16|xargs grep需要转义的字符123grep '"第一个转义\$第二个转义\[&#123;' a.txt或者直接使用-Fgrep -F '"$[&#123;' a.txt shell参数12345678while [ -n "$1" ]do case "$1" in -a) a=$2;shift 2;; -s) s=$2;shift 2;; *) ;; casedone]]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filebeat7收集mysql慢日志到es]]></title>
    <url>%2F2019%2F10%2F30%2F24-filebeat7%E6%94%B6%E9%9B%86mysql%E6%85%A2%E6%97%A5%E5%BF%97%E5%88%B0es%2F</url>
    <content type="text"><![CDATA[慢日志提供给开发查看, 采用elk统一提供,这里采用k8s环境搭建 原文: ELK收集mysql_slow.log其他: filebeat （7.1.0）docker容器 slowlog内容分析 5.5 版本慢查询日志 12345# Time: 191030 17:03:13# User@Host: myuser[myuser] @ [10.10.0.1]# Query_time: 3.329673 Lock_time: 0.000107 Rows_sent: 0 Rows_examined: 3971182SET timestamp=1572426193;select * from a where name = 1 limit 1; 5.6 版本慢查询日志 123456# Time: 191030 17:03:13# User@Host: myuser[myuser] @ [10.10.0.1] Id: 1111# Query_time: 3.329673 Lock_time: 0.000107 Rows_sent: 0 Rows_examined: 3971182use db_name;SET timestamp=1572426193;select * from a where name = 1 limit 1; 5.7 版本慢查询日志 12345# Time: 2019-10-06T13:25:38.703546+08:00# User@Host: myuser[myuser] @ [10.10.0.1] Id: 1111# Query_time: 3.329673 Lock_time: 0.000107 Rows_sent: 0 Rows_examined: 3971182SET timestamp=1572426193;select * from a where name = 1 limit 1; 除以上格式以外,还需要注意慢查询代码块,可能并不是每次都有 # Time 一条完整的日志：最终将以# User@Host: 开始的行，和以SQL语句结尾的行合并为一条完整的慢日志语句 开始部署filebeat7 准备镜像1docker pull store/elastic/filebeat:7.2.0 filebeat配置文件1234567891011121314151617181920filebeat.inputs:- type: log enabled: true paths: - /opt/slow.log exclude_lines: ['^\# Time'] multiline.pattern: '^\# Time|^\# User' multiline.negate: true multiline.match: after tail_files: trueoutput.elasticsearch: enabled: true hosts: ["10.0.0.100:9200"] protocol: "http" indices: - index: "es-index-name" k8s部署文件12345678910111213141516171819202122232425262728293031323334k8s-filebeat-7.2.0.ymlkind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: filebeat name: filebeat namespace: ns-elastic7spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: filebeat template: metadata: labels: elastic-app: filebeat spec: containers: - name: filebeat image: store/elastic/filebeat:7.2.0 volumeMounts: - name: filebeat-config mountPath: /usr/share/filebeat/filebeat.yml - name: mysql-dev-252 mountPath: /opt/php-mysql-dev-0-slow.log volumes: - name: filebeat-config hostPath: path: /data/k8s-container/elk-7.2.0/filebeat-7.2.0/filebeat.yml - name: mysql-dev-252 hostPath: path: /data/k8s-container/mysql5.5/slow.log logstash分析mysql日志 省略input的kafka 和ouput的es 12345678910111213141516if [type] == "showlog1" or [type] == "showlog2" &#123; json &#123; source =&gt; "message" &#125; mutate &#123; gsub =&gt; [ "message", "\n", "" ] &#125; grok &#123; match =&gt; ["message","(?m)^# User@Host: %&#123;USER:user&#125;\[[^\]]+\] @ \[%&#123;IP:clientip&#125;\] Id: %&#123;NUMBER:Id:int&#125;# Query_time: %&#123;NUMBER:query_time:float&#125;\s+Lock_time: %&#123;NUMBER:lock_time:float&#125;\s+Rows_sent: %&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined: %&#123;NUMBER:rows_examined:int&#125;(?&lt;dbnameall&gt;.*)SET\s+timestamp=%&#123;NUMBER:timestamp_mysql:int&#125;;(?&lt;query&gt;.*)"] &#125; date &#123; match =&gt; ["timestamp_mysql", "UNIX"] target =&gt; "@timestamp" &#125;&#125;]]></content>
      <categories>
        <category>k8s</category>
        <category>elk7</category>
        <category>filebeat7</category>
        <category>elk7</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>filebeat7</tag>
        <tag>elk7</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.5主与mysql5.7从部署配置]]></title>
    <url>%2F2019%2F10%2F29%2F23-mysql5-5%E4%B8%BB%E4%B8%8Emysql5-7%E4%BB%8E%E9%83%A8%E7%BD%B2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[由于需要将旧版mysql5.5的数据同步到新mysql5.7, 并且会对部分表分库 参考教程: mysql从5.5直接升级到5.7 mysql5.5升级到mysql5.7 采用mysql5.5数据目录升级为mysql5.712345678910111 从mysql5.5的从库 copy /data数据2 修改新的mysql5.7配置文件 my.cnf，添加datadir，指向5.5数据目录3 新安装数据库执行(本次不需要执行) /usr/local/mysql57/bin/mysqld --defaults-file=/etc/my57.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/disk/u014 启动mysql5 此时数据目录还是5.5的，需要执行mysql_upgrade进行升级，在执行表修复前，需要确认一个参数innodb_file_per_table，mysql官网对该参数的解释如下 该参数在5.5版本默认为OFF，所有表和索引都导入一个共享文件中，名为ibdata1,但在5.6.7及以后版本，改参数被默认设置为ON，即每张表都有对应的表和索引存储文件，每个schema下，每个frm文件都有对应的ibd文件。 在执行mysql_upgrade时，会修复系统表，并且如果该参数在5.5和5.7版本均使用默认值，则会将之前共享表和索引的存储方式改为每张表单独存储表和索引的形式，故会出现拷贝复制的操作，如果数据量比较大，则用时就会很长， 使用nnodb_file_per_table=1，及表和索引单独存储的优缺点，可查看mysql官网介绍。6 使用mysql_upgrade检测并修复表 /usr/local/mysql57/bin/mysql_upgrade -S /disk/u01/mysql.sock 以上已经完成对mysql5.5数据升级 在mysql5.7运行的功能 配置mysql5.5主与mysql5.7从 将msyql5.7作为mysql5.5的从库123456# 从库执行, POS位置以 show master status\G 查询为准stop slave;SET GLOBAL read_only=0;reset slave all;CHANGE MASTER TO MASTER_HOST='db_master.prod.zhangzw.com',MASTER_PORT=3306,MASTER_USER='xxx',MASTER_PASSWORD='xxx',MASTER_LOG_FILE='m1-master-bin.000001',MASTER_LOG_POS=107;start slave; 在主库测试创建表, 查看是否会同步到mysql5.7从库1234567create table tutorials_tbl( tutorial_id INT NOT NULL AUTO_INCREMENT, tutorial_title VARCHAR(100) NOT NULL, tutorial_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY ( tutorial_id )); 修改mysql5.7库名 没问题之后,我们需要将mysql5.7的mydatabase库改成mydatabasenew库名, 断开mysql5.5 和mysql5.7主从同步(最好设置mysql5.5只读,防止数据差异), 在mysql5.7上执行改库名, 以下有触发器的表会修改失败 测试执行时间在15s左右 123456789101112#!/bin/bash# 假设将sakila数据库名改为new_sakila# MyISAM直接更改数据库目录下的文件即可new_database=mydatabasenewold_database=mydatabasemysql -S /disk/u01/mysql.sock -e 'create database if not exists $&#123;new_database&#125;'list_table=$(mysql -S /disk/u01/mysql.sock -Nse "select table_name from information_schema.TABLES where TABLE_SCHEMA='$&#123;old_database&#125;'")for table in $list_tabledo mysql -S /disk/u01/mysql.sock -e "rename table $&#123;old_database&#125;.$table to $&#123;new_database&#125;.$table"done 此时在配置新的mysql5.7的主从机器1234567891011121314```&lt;center&gt;&lt;img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/&gt;&lt;font color="blue" face="黑体" size=5&gt; 一些配置问题 &lt;/font&gt;&lt;/center&gt;---### GTID_MODE 配置不统一 The replication receiver thread cannot start because the master has GTID_MODE = OFF and this server has GTID_MODE = ON. 永久修改gtid_mode = off 一次性关闭步骤：stop slave;SET GLOBAL GTID_MODE = ‘ON_PERMISSIVE’;SET GLOBAL GTID_MODE = ‘OFF_PERMISSIVE’;SET GLOBAL GTID_MODE = ‘OFF’;start slave; 12### mysql5.7 sql_mode sql_mode=’ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION’ 1234---### 注意一台机器多个mysql启动脚本修改问题 #以下两处修改 /etc/init.d/mysqld57parse_server_arguments $print_defaults -c /etc/my57.cnf mysqld server mysql_server mysql.server$bindir/mysqld_safe –defaults-file=/etc/my57.cnf –pid-file=”$mysqld_pid_file_path” $other_args &gt;/dev/null &amp; 12345678&lt;center&gt;&lt;img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/&gt;&lt;font color="blue" face="黑体" size=5&gt; 一些info信息 &lt;/font&gt;&lt;/center&gt;### /usr/local/mysql57/bin/mysql_upgrade -S /disk/u01/mysql.sock 的部分记录 /usr/local/mysql57/bin/mysql_upgrade -S /disk/u01/mysql.sockChecking if update is needed.Checking server version.Running queries to upgrade MySQL server.mysql_upgrade: (non fatal) [WARNING] 1642: Pre-4.1 password hash found. It is deprecated and will be removed in a future release. Please upgrade it to a new format.Checking system database.mysql.columns_priv OKmysql.db OKmysql.engine_cost OKmysql.event OKmysql.func OKmysql.general_log OKmysql.gtid_executed OKmysql.help_category OKmysql.help_keyword OKmysql.help_relation OKmysql.help_topic OKmysql.host OKmysql.innodb_index_stats OKmysql.innodb_table_stats OKmysql.ndb_binlog_index OKmysql.plugin OKmysql.proc OKmysql.procs_priv OKmysql.proxies_priv OKmysql.server_cost OKmysql.servers OKmysql.slave_master_info OKmysql.slave_relay_log_info OKmysql.slave_worker_info OKmysql.slow_log OK… 12### 附录 my57.cnf [client]port = 3308socket = /disk/u01/mysql.sock [mysql]prompt=”\u@m1_618_u [\d]&gt; “no-auto-rehash [mysqld]sql_mode=’ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION’replicate-wild-do-table=mydatabase.% binlog-ignore-db=information_schemabinlog-ignore-db=mysqlbinlog-ignore-db=performance_schemabinlog-ignore-db=test binlog-do-db=mydatabase user = mysqlport = 3308basedir = /usr/local/mysql57datadir = /disk/u01socket = /disk/u01/mysql.sockpid-file = /disk/u01/dbm1_u.pidtmpdir = /disk/u02server-id = 123character-set-server = utf8skip_name_resolve = 1innodb_file_per_table = 1explicit_defaults_for_timestamp = 0read_only = 1 buffer&amp;cachetable_open_cache = 100table_definition_cache = 400table_open_cache_instances = 64sort_buffer_size = 4Mjoin_buffer_size = 4Mread_buffer_size = 8Mread_rnd_buffer_size = 4M thread&amp;connectionthread_stack = 256Kthread_cache_size = 768back_log = 1024max_connections = 3000max_connect_errors = 1000000 temptabletmp_table_size = 32Mmax_heap_table_size = 32M networkmax_allowed_packet = 32M #lock_wait_timeout = 3600 #interactive_timeout = 600 #wait_timeout = 600 query cachequery_cache_size = 0query_cache_type = 0 设置errorlog、slowlog和generallog的时区，默认UTClog_timestamps = SYSTEM error-loglog_error = /disk/u02/mysqld.log slow-logslow_query_log = 1slow_query_log_file = /disk/u02/slow.loglong_query_time = 0.1log_queries_not_using_indexes =1log_throttle_queries_not_using_indexes = 60min_examined_row_limit = 100log_slow_admin_statements = 1log_slow_slave_statements = 1 general log#general-log = 1general_log_file=/disk/u02/query.log binlogbinlog_format = rowbinlog_checksum = 1log-bin = /disk/u02/m1-binlog-bin-index = /disk/u02/m1-bin.indexsync_binlog = 0binlog_cache_size = 4Mmax_binlog_cache_size = 1Gmax_binlog_size = 512Mexpire_logs_days = 15 GTIDgtid_mode = offenforce_gtid_consistency = 1log_slave_updates Replicationmaster_info_repository = TABLErelay_log_info_repository = TABLEslave-rows-search-algorithms = ‘INDEX_SCAN,HASH_SCAN’relay_log_recovery = 1relay_log_purge = 1relay-log=/disk/u02/m1-relay-binrelay-log-index=/disk/u02/m1-relay-bin.index innodb-buffer&amp;cacheinnodb_buffer_pool_size = 1Ginnodb_buffer_pool_instances = 4 #innodb_additional_mem_pool_size = 16Minnodb_max_dirty_pages_pct = 50 innodb loginnodb_data_file_path = ibdata1:256M:autoextendinnodb_log_file_size = 256Minnodb_log_files_in_group = 2innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 32M #innodb_max_undo_log_size = 4G #innodb_undo_directory = undologinnodb_undo_tablespaces = 0 innodb-ioinnodb_flush_method = O_DIRECTinnodb_io_capacity = 600innodb_io_capacity_max = 2000innodb_flush_sync = 0innodb_flush_neighbors = 0 #innodb_lru_scan_depth = 4000innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_purge_threads = 4innodb_page_cleaners = 4 transaction,lock#innodb_sync_spin_loops = 100 #innodb_spin_wait_delay = 30innodb_lock_wait_timeout = 10innodb_print_all_deadlocks = 1innodb_rollback_on_timeout = 1 innodb_open_files = 65535 innodb_online_alter_log_max_size = 1G innodb statusinnodb_status_file = 1 注意: 开启 innodb_status_output &amp; innodb_status_output_locks 后, 可能会导致log-error文件增长较快innodb_status_output = 0innodb_status_output_locks = 0 #performance_schemaperformance_schema = 1performance_schema_instrument = ‘%=on’ #innodb monitorinnodb_monitor_enable=”module_innodb”innodb_monitor_enable=”module_server”innodb_monitor_enable=”module_dml”innodb_monitor_enable=”module_ddl”innodb_monitor_enable=”module_trx”innodb_monitor_enable=”module_os”innodb_monitor_enable=”module_purge”innodb_monitor_enable=”module_log”innodb_monitor_enable=”module_lock”innodb_monitor_enable=”module_buffer”innodb_monitor_enable=”module_index”innodb_monitor_enable=”module_ibuf_system”innodb_monitor_enable=”module_buffer_page”innodb_monitor_enable=”module_adaptive_hash” MyISAMkey_buffer_size = 1024Mbulk_insert_buffer_size = 64Mmyisam_sort_buffer_size = 128Mmyisam_repair_threads = 1 [mysqldump]quickmax_allowed_packet = 32M]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
        <category>主从</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[es5集群磁盘扩容]]></title>
    <url>%2F2019%2F10%2F28%2F22-es%E9%9B%86%E7%BE%A4%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[es集群磁盘不足,对磁盘扩容遇到一些的问题 重启集群前，先设置集群停止分片移动：12345curl -XPUT http://localhost:9200/_cluster/settings -d '&#123;"transient" : &#123;"cluster.routing.allocation.enable" : "none"&#125;&#125;' 对磁盘进行扩容,每次操作一个节点123456789101112131415# 直接扩容磁盘到2T//针对ext4文件格式的操作系统（如CentOS6）：//umount /dev/vdbe2fsck -f /dev/vdbresize2fs /dev/vdbmount /dev/vdb /data# 或者新增 2T云盘/dev/vdcumount /data/mkdir /data2mount /dev/vdb /data2mkfs.ext4 /dev/vdcmount /dev/vdc /datacp -ra /data2/* /data/ 重启之后，恢复分片自动分配：12345curl -XPUT http://localhost:9200/_cluster/settings -d '&#123;"transient" : &#123;"cluster.routing.allocation.enable" : "all"&#125;&#125;' 如果需要下线其中的节点, 先将分片都转义到其他节点123456# 执行以下命令会自动将10.10.0.1 节点上的分片全部迁移到其他机器, 等待迁移完成, 将改空机器下线即可curl -XPUT 127.0.0.1:9200/_cluster/settings -d '&#123;"transient" :&#123;"cluster.routing.allocation.exclude._ip" : "10.10.0.1"&#125;&#125;' 另外对于 path.data 配置多快盘的问题1234比如es8配置了三块盘:/disk4/data -&gt; sde, /disk5/data -&gt; sdf, disk6/data -&gt; sdg这里注意 es node的data path尽量保证盘的大小差别不要太大, sde,sdf,sdg的大小保障差不多, 否则由于es shard 均衡的时候可能会优先分配到磁盘大的目录, 可能会导致sde(假如这个磁盘最大)的IO高, 而sdf等IO低 简单的配置信息elasticsearch512345678910111213cluster.name: es-devnode.name: es1-upath.data: /data/es/datapath.logs: /data/es/logsnetwork.host: 0.0.0.0discovery.zen.ping.unicast.hosts: ["10.10.0.1:9300","10.10.0.2:9300","10.10.0.3:9300","10.10.0.4:9300"]http.cors.enabled: truehttp.cors.allow-origin: "*"xpack.security.enabled: falsebootstrap.system_call_filter: falsethread_pool.bulk.queue_size: 3000# 防止脑裂discovery.zen.minimum_master_nodes: 2]]></content>
      <categories>
        <category>elk</category>
        <category>elasticsearch5</category>
      </categories>
      <tags>
        <tag>elasticsearch5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流量复制工具gor]]></title>
    <url>%2F2019%2F10%2F28%2F21-%E6%B5%81%E9%87%8F%E5%A4%8D%E5%88%B6%E5%B7%A5%E5%85%B7gor%2F</url>
    <content type="text"><![CDATA[Gor 是一款go语言实现的简单的http流量复制工具，它的主要目的是使你的生产环境HTTP真实流量在测试环境和预发布环境重现 流量复制工具 下载安装github下载地址: https://github.com/buger/goreplay/releases 1234tar -xvf gor_1.0.0_x64.tar.gzmv gor /usr/bin/which gor 命令123456789101112131415161718192021222324252627282930311 保存请求到文件# 将本机所有80请求保存到gor-20171120_0.log文件(注意会生成很多文件)gor --input-raw :80 --output-file gor-%Y%m%d.log# --output-file-append 会生成gor-20171120.log文件gor --input-raw :80 --output-file gor-%Y%m%d.log --output-file-append2 根据文件回放请求# 镜像qps回放gor --input-file gor-aaaa-20171120.log --output-http aaaa-dev.test.com# 两倍镜像qps回放gor --input-file "gor-aaaa-20171120.log|200%" --output-http aaaa-dev.test.com3 过滤url后保存请求到文件# 排除s.test.com的请求gor --input-raw :80 --output-file gor-%Y%m%d.log --output-file-append --http-disallow-header "Host: s.test.com" --http-disallow-header "Host: www.test.com" --http-disallow-header "Host: bbs.test.com"# 只存储aaaa.test.com的请求gor --input-raw :80 --output-file gor-aaaa-%Y%m%d.log --output-file-append --http-allow-header "Host: aaaa.test.com"# https的不能抓包gor --input-raw :443 --output-file gor-ssl-aaaa-%Y%m%d.log --output-file-append --http-allow-header "Host: aaaa.test.com"4 在线镜像复制请求# 将生产aaaa.test.com的请求复制到 aaaa-dev.test.com 环境!gor --input-raw :80 --output-http "aaaa-dev.test.com" --http-allow-header "Host: aaaa.test.com" 离线文件编辑123456文件的每个请求通过 如下字符串分割!ð&lt;9f&gt;&lt;90&gt;µð&lt;9f&gt;&lt;99&gt;&lt;88&gt;ð&lt;9f&gt;&lt;99&gt;&lt;89&gt;并且第一行是 请求的唯一码? 和时间戳!1 9b366a8eab8d6cb8e557cb3bf43f69c36612cffb 1511165572419843000所以可录制比如半小时的然后窃取需要的时间段! 问题 https 不能抓包! 通过添加代理, gor抓取8000端口 1234567891011121314151617181920212223# SSL terminationserver &#123; listen 443 ssl; server_name aaaa.test.com; ssl_certificate /etc/ssl/nginx/server.crt; ssl_certificate_key /etc/ssl/nginx/server.key; location / &#123; proxy_set_header Host $host; proxy_pass http://localhost:8000; &#125;&#125;server &#123; listen 8000; server_name aaaa.test.com; location / &#123; proxy_set_header Host $host; proxy_pass http://production_shop_api_site; &#125;&#125;]]></content>
      <categories>
        <category>流量复制工具</category>
        <category>gor</category>
      </categories>
      <tags>
        <tag>gor</tag>
        <tag>http流量复制工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s搭建mysql5.7.24主从]]></title>
    <url>%2F2019%2F10%2F24%2F20-k8s%E6%90%AD%E5%BB%BAmysql5-7-24%E4%B8%BB%E4%BB%8E%2F</url>
    <content type="text"><![CDATA[k8s上简单部署mysql5.7.24主从 k8s搭建mysql5.7.24主从 参考文档利用Kubernetes搭建mysql主从复制集群官方dockerfile 从hub.docker.com拉取官方镜像1docker pull mysql:5.7.24 build镜像 主库master的Dockerfile12345from mysql:5.7.24run sed -i '/\[mysqld\]/a server-id=1\nlog-bin' /etc/mysql/mysql.conf.d/mysqld.cnfCOPY docker-entrypoint.sh /usr/local/bin/ 主库的docker-entrypoint.sh 先从初始镜像取 或者从github对应版本上 123docker run -dti mysql:5.7.24 /bin/bashdocker cp 2bfa6209d120c23:/usr/local/bin/docker-entrypoint.sh . 修改docker-entrypoint.sh 12345678fi# 添加以下内容echo "CREATE USER '$MYSQL_REPLICATION_USER'@'%' IDENTIFIED BY '$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;"echo "GRANT REPLICATION SLAVE ON *.* TO '$MYSQL_REPLICATION_USER'@'%' IDENTIFIED BY '$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;"echo "FLUSH PRIVILEGES ;" | "$&#123;mysql[@]&#125;"# 添加以上内容echo ls /docker-entrypoint-initdb.d/ &gt; /dev/null build主库镜像 12docker build -t hub.zhangzw.com/bq/mysql-master:5.7.24 .docker push hub.zhangzw.com/bq/mysql-master:5.7.24 从库的docker-entrypoint.sh 同上先从初始镜像取 或者从github对应版本上 或复制上面的文件 修改docker-entrypoint.sh 12345678fi# 添加以下内容 echo "STOP SLAVE;" | "$&#123;mysql[@]&#125;" echo "CHANGE MASTER TO master_host='$MYSQL_MASTER_SERVICE_HOST', master_user='$MYSQL_REPLICATION_USER', master_password='$MYSQL_REPLICATION_PASSWORD' ;" | "$&#123;mysql[@]&#125;" echo "START SLAVE;" | "$&#123;mysql[@]&#125;" # 添加以上内容echo ls /docker-entrypoint-initdb.d/ &gt; /dev/null build从库镜像 12docker build -t hub.zhangzw.com/bq/mysql-slave:5.7.24 .docker push hub.zhangzw.com/bq/mysql-slave:5.7.24 开始部署 k8s-master-mysql_5.7.24.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869---apiVersion: apps/v1beta1kind: StatefulSetmetadata: labels: app: php-mysql-master-dev name: php-mysql-master-dev namespace: dbspec: serviceName: "php-mysql-master-dev" replicas: 1 selector: matchLabels: app: php-mysql-master-dev template: metadata: labels: app: php-mysql-master-dev spec: containers: - name: php-mysql-master-dev image: hub.zhangzw.com/bq/mysql-master:5.7.24 ports: - containerPort: 3306 name: db-port resources: requests: cpu: "50m" limits: cpu: "1000m" env: - name: MYSQL_ROOT_PASSWORD value: "admin" - name: MYSQL_REPLICATION_USER value: "repl" - name: MYSQL_REPLICATION_PASSWORD value: "7a5b21ac65712bd95e39d3c1" volumeMounts: - name: order-master-dev-data mountPath: /var/lib/mysql - name: order-master-dev-cfg mountPath: /etc/mysql volumes: - name: order-master-dev-data hostPath: path: /data/k8s-container/php-mysql-dev/master/data - name: order-master-dev-cfg hostPath: path: /data/k8s-container/php-mysql-dev/master/etc-mysql---kind: ServiceapiVersion: v1metadata: labels: app: php-mysql-master-dev name: php-mysql-master-dev-service namespace: dbspec: type: NodePort ports: - port: 3306 name: db-port targetPort: 3306 nodePort: 23306 protocol: TCP selector: app: php-mysql-master-dev k8s-slave-mysql_5.7.24.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172---apiVersion: apps/v1beta1kind: StatefulSetmetadata: labels: app: php-mysql-slave-dev name: php-mysql-slave-dev namespace: dbspec: serviceName: "php-mysql-slave-dev" replicas: 1 selector: matchLabels: app: php-mysql-slave-dev template: metadata: labels: app: php-mysql-slave-dev spec: containers: - name: php-mysql-slave-dev image: hub.zhangzw.com/bq/mysql-slave:5.7.24 ports: - containerPort: 3306 name: db-port resources: requests: cpu: "50m" limits: cpu: "1000m" env: - name: MYSQL_ROOT_PASSWORD value: "admin" - name: MYSQL_REPLICATION_USER value: "repl" - name: MYSQL_REPLICATION_PASSWORD value: "7a5b21ac65712bd95e39d3c1" - name: MYSQL_MASTER_SERVICE_HOST value: "php-mysql-master-dev-service" volumeMounts: - name: order-slave-dev-data mountPath: /var/lib/mysql - name: order-slave-dev-cfg mountPath: /etc/mysql volumes: - name: order-slave-dev-data hostPath: path: /data/k8s-container/php-mysql-dev/slave/data - name: order-slave-dev-cfg hostPath: path: /data/k8s-container/php-mysql-dev/slave/etc-mysql---kind: ServiceapiVersion: v1metadata: labels: app: php-mysql-slave-dev name: php-mysql-slave-dev-service namespace: dbspec: type: NodePort ports: - port: 3306 name: db-port targetPort: 3306 nodePort: 23307 protocol: TCP selector: app: php-mysql-slave-dev 问题总结 从库的replay log名字会根据docker主机名变化, 也可以写在配置文件 12# Dockerfile中可以添加run sed -i '/\[mysqld\]/a relay-log-index=php-mysql-shoporder-slave-dev-relay-bin.index' /etc/mysql/mysql.conf.d/mysqld.cnf 注意MYSQL_MASTER_SERVICE_HOST 变量的配置, 根据你master的service变化 其次我docker-entrypoint.sh 文件几次手动从页面复制粘贴下来的导致各种语法错误,这里建议找到对的版本从github克隆, 或者从mysql:5.7.24镜像中cp 配置etc-mysql/mysql.conf.d/mysqld.cnf 123456789[mysqld]# 从库配置read_only=1super_read_only=1character-set-server=utf8# 1 去掉STRICT_TRANS_TABLES 表NOT NULL时无法创建表# 2 修改NO_ZERO_DATE为ALLOW_INVALID_DATES 允许’0000-00-00’#sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'sql_mode='ONLY_FULL_GROUP_BY,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' 配置etc-mysql/conf.d/mysql.cnf 123[mysql]no-auto-rehashdefault-character-set=utf8 附录master配置docker-entrypoint.shslave配置docker-entrypoint.sh]]></content>
      <categories>
        <category>mysql</category>
        <category>k8s</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell中gt和>的区别]]></title>
    <url>%2F2019%2F10%2F17%2F19-shell%E4%B8%ADgt%E5%92%8C%3E%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[shell中 gt 和 &gt; 的一些相关问题介绍和测试 以下是bash的测试, 注意如果你是zsh可能会不同喔😯 [[]] , [] 和test比较 [] 和test: 两者是一样的，在命令行里test expr和[ expr ]的效果相同。test中可用的比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq, -gt这种形式。通过which [ 和which test 可以看到是命令 [] 和test 例子 1234567[root@dk-centos6 ~]# a="abcdef"[root@dk-centos6 ~]# test "$a" = "abcdef"[root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ "$a" = "abcdef" ][root@dk-centos6 ~]# echo $?0 [[ ]]具体功能: [[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 支持字符串的模式匹配（使用=~操作符时甚至支持shell的正则表达 式）,右边的字符串不加双引号的情况,可以把右边作为模式. 比如[[ hello == hell? ]]，结果为真。当然加引号就是文本字符串比较. 使用[[ … ]]条件判断结构，而不是[ … ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用if [[ $a != 1 &amp;&amp; $a != 2 ]], 如果不适用双括号, 则为if [ $a -ne 1] &amp;&amp; [ $a != 2 ]或者if [ $a -ne 1 -a $a != 2 ]。 纯数字比较 &gt; 通过比较ASCII值,gt仅能比较数字123456789101112[root@dk-centos6 ~]# [ 2 \&gt; 1 ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ 2 -gt 1 ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [[ 2 &gt; 1 ]][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [[ 2 -gt 1 ]][root@dk-centos6 ~]# echo $?0 字符串比较 单括号中如果要比较符号 “&lt;” “&gt;”, 需要转义, 否则判断结果错误123456789[root@dk-centos6 ~]# [ "b" &gt; "a" ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ "b" &lt; "a" ][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [ "b" \&lt; "a" ][root@dk-centos6 ~]# echo $?1 双括号不用转义 , 直接执行即可123456[root@dk-centos6 ~]# [[ "b" &gt; "a" ]][root@dk-centos6 ~]# echo $?0[root@dk-centos6 ~]# [[ "b" &lt; "a" ]][root@dk-centos6 ~]# echo $?1]]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[收藏链接]]></title>
    <url>%2F2019%2F10%2F17%2F18-%E6%94%B6%E8%97%8F%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[记录一些好的网址 容器 云原生图书编年史 云原生大佬博客汇总 (2000个) kubernetes 社群分享 QA 汇总 k8s CRD(CustomResourceDefinition)自定义controller sample k8s 集群部署运营实践总结 k8s 创建一个只读的用户权限 kubectl 创建 Pod 背后到底发生了什么？ 白话Kubernetes网络(推荐) Linux服务 MySQL 5.6, 5.7, 8.0版本的新特性大全 nginx+lua实现灰度发布 计算机网络 根域名的知识 golang go学习指南 go-mod学习 常用的工具网址 不用翻墙就能谷歌 deepl 比谷歌翻译好用的翻译 tableconvert 在线excel转markdown processon 免费在线作图 表情锅 自制表情动图 在线解析 子网掩码换算等 其他文章 Raft 为什么是更易理解的分布式一致性算法 Raft 一步步动态教你理解raft协议 Raft 更多形象化理解]]></content>
      <categories>
        <category>网址</category>
        <category>收藏</category>
      </categories>
      <tags>
        <tag>收藏</tag>
        <tag>网址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown一些写法记录]]></title>
    <url>%2F2019%2F10%2F16%2F17-markdown%E4%B8%80%E4%BA%9B%E5%86%99%E6%B3%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[记录一些需要注意的写法,markdown 是支持html语法的 这里想写一个居中的标题和图片1234&lt;center&gt;&lt;img src="http://zhangzw001.github.io/images/dockerniu.jpeg" width = "100" height = "100" style="border: 0"/&gt;&lt;font color="blue" face="黑体" size=5&gt; 这个就是效果图咯 &lt;/font&gt;&lt;/center&gt; 效果: 这个就是效果图咯 表格模板1234|标题|内容||----|----||标题1|内容1||标题2|内容2| 效果: 标题 内容 标题1 内容1 标题2 内容2]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile介绍]]></title>
    <url>%2F2019%2F10%2F16%2F16-dockerfile%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Dockerfile 本文摘录于: 如何快速将容器云镜像大小精简98%？ Dockerfile 文件有自己的书写格式和支持的命令，常用的Dockerfile 指令有： FROM 指定基镜像。 MAINTAINER 设置镜像的作者信息，如作者姓名、邮箱等。 COPY 将文件从本地复制到镜像，拷贝前需要保证本地源文件存在。 ADD 与 COPY 类似，复制文件到镜像。不同的是，如果文件是归档文件（tar, zip, tgz, xz 等），会被自动解压。 ENV 设置环境变量，格式: ENV key=value或ENV key value，运行容器后，可直接在容器中使用。 EXPOSE 暴露容器中指定的端口，只是一个声明，主要用户了解应用监听的端口。 VOLUME 挂载卷到容器，需要注意的是，保存镜像时不会保存卷中的数据。 WORKDIR 设置当前工作目录，后续各层的当前目录都被指定。 RUN 在容器中运行指定的命令。 CMD 容器启动时运行的命令。Dockerfile 中可以有多个 CMD 指令，但只有最后一个生效。CMD 可以被 docker run 之后的参数替换。 ENTRYPOINT 设置容器启动时运行的命令。Dockerfile 中可以有多个 ENTRYPOINT 指令，但只有最后一个生效。CMD 或 docker run 之后的参数会被当做参数传递给 ENTRYPOINT，这个是与CMD的区别。 容器的原理 容器镜像中最重要的概念就是layers，即镜像层。 容器的原理 镜像层依赖于一系列的底层技术，比如文件系统(filesystems)、写时复制(copy-on-write)、联合挂载(union mounts)等技术查看Docker 官方文档https://docs.docker.com/storage/storagedriver/进行学习。 每条指令都创建一个镜像层，会增加镜像的大小 下面看个例子这里我有一个1.2M的镜像 12docker images|grep busyboxbusybox latest 19485c79a9bb 5 weeks ago 1.22MB 我们基于busybox写一个Dockerfile来build 1234567#cat Dockerfilefrom busybox:latestrun mkdir /tmp/dir \ &amp;&amp; dd if=/dev/zero of=/tmp/dir/file1 bs=1M count=10run rm -f /tmp/dir/file1 执行build 1234567891011121314151617docker build -t busybox-test .Sending build context to Docker daemon 2.048kBStep 1/3 : from busybox:latest ---&gt; 19485c79a9bbStep 2/3 : run mkdir /tmp/dir &amp;&amp; dd if=/dev/zero of=/tmp/dir/file1 bs=1M count=10 ---&gt; Running in 0426f92c77ed10+0 records in10+0 records out10485760 bytes (10.0MB) copied, 0.003785 seconds, 2.6GB/sRemoving intermediate container 0426f92c77ed ---&gt; 5ec75db090c9Step 3/3 : run rm -f /tmp/dir/file1 ---&gt; Running in 540e7d0a5aeaRemoving intermediate container 540e7d0a5aea ---&gt; 00041489cc0eSuccessfully built 00041489cc0eSuccessfully tagged busybox-test:latest 查看image大小 123docker images|grep busyboxbusybox-test latest 00041489cc0e 10 minutes ago 11.7MBbusybox latest 19485c79a9bb 5 weeks ago 1.22MB ??? 我不是rm删除了创建的/tmp/dir/file1 文件吗? 难道它还在? 来,我们测试一下 12# 查看目录下是否有文件docker run -ti busybox-test ls /tmp/dir 结果显然是空… 喔,,, 因为”在Dockerfile中，每条指令都会创建一个镜像层，继而会增加镜像整体的大小”, 在看我们写的Dockerfile,我们第一个run 执行的时候, 这里假装叫 (run1层), 我们生成了file1文件当执行第二个run的时候, 我们处在了 (run2层), (run1层)已经是父层,是个只读层了,只有当前层可写, 虽然我们在 (run2层)删除了这个文件,但删除的仅仅是份拷贝而已, 这就是写时复制. 所以以上的优化应该是: 写成一条run 123456789#cat Dockerfilefrom busybox:latestrun mkdir /tmp/dir \ &amp;&amp; dd if=/dev/zero of=/tmp/dir/file1 bs=1M count=10 \ &amp;&amp; rm -f /tmp/dir/file1# builddocker build -t busybox-test2 . 结果显然 1234docker images|grep busyboxbusybox-test2 latest faf8b7d4f140 3 seconds ago 1.22MBbusybox-test latest 00041489cc0e 10 minutes ago 11.7MBbusybox latest 19485c79a9bb 5 weeks ago 1.22MB 虽然说这里的测试没有干任何事情, 但我们在写Dockerfile的时候需要注意, 两个run之间是两个不同的 可写层! 简单总结精简镜像大小的方法: 121 使用更小的基础镜像,注意一些很小的镜像可能缺少很多依赖库,例如查看redis依赖库 ldd /usr/bin/redis-cli2 合并Dockerfilec指令精简(可以的话写成一条run) 一些小的镜像 1 scratch: 一个空的镜像, 无法pull -.-!!! , 写在Dockerfile是可以的 2 alpine: 5M的linux镜像,有包管理工具apk 123FROM scratchADD alpine-minirootfs-3.10.2-x86_64.tar.gz /CMD ["/bin/sh"] 3 busybox: 1M多的镜像,称为嵌入式linux的瑞士军刀, Linux和unix一些常用的命令 注意事项 镜像构建的顺序会影响缓存的有效性,经常修改的内容应该放到最后 尽可能的写到同一个RUN,删除不必要的例外 –no-install-recommends, 并且记得删除包管理缓存 rm -rf /var/lib/apt/lists/* 多阶段构建的使用12345678910from maven:3.6-jdk-8-alpine as mavencacheworkdir /optcopy pom.xml .run mvn -e -B xx:xxcopy src ./srcrun mvn -e -B xxfrom openjdk:8-jdk-alpinecopy --from-mavencache /opt/target/xxx.jar /cmd ["java", "-jar", "/xxx.jar"]]]></content>
      <categories>
        <category>docker</category>
        <category>Dockerfile</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker简介和使用]]></title>
    <url>%2F2019%2F10%2F16%2F15-docker%E7%AE%80%E4%BB%8B%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简单介绍docker dockerk8s支持的docker 版本查看这里我们使用年份命名版本的docker-ce，假设我们要安装v1.16.3的k8s，我们去 https://github.com/kubernetes/kubernetes 里进对应版本的CHANGELOG-1.16.md里搜The list of validated docker versions remain查找支持的docker版本，docker版本不一定得在支持列表里，实际上19.03也能使用，这里我们使用docker官方的安装脚本安装docker(该脚本支持centos和ubuntu) 12export VERSION=19.03curl -fsSL "https://get.docker.com/" | bash -s -- --mirror Aliyun]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.5目录copy方式迁移]]></title>
    <url>%2F2019%2F10%2F15%2F14-mysql%E7%9B%AE%E5%BD%95copy%E6%96%B9%E5%BC%8F%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[从现有的一台 从库 全copy data目录到2台新机器上, 再配置mysql主从 目录copy方式迁移 注意 不要删除ibdata1,会导致innodb表不存在 可以不删除ib_logfile0,ib_logfile1, 但my.cnf配置大小一致 区别目录权限为mysql,tmp目录存在 请自行安装好mysql5.5 1234567891 首先停止mysql/etc/init.d/mysqld stop2 同步数据目录到新机器/data/u013 确认新机器上mysql版本并配置/etc/my.cof4 完整迁移时不需要删除内容(innodb_log_file_size = 256M 配置要一致)5 启动mysql 配置主从 1 首先 目录copy方式 同步某个从库到2台新机器并启动完成, 此时两个mysql都开启了slave 2 暂停同步，并设置读写； 123456stop slave;# 该执行仅主库上执行(配置可写)SET GLOBAL read_only=0;reset slave all;-- RESET SLAVE ALL是清除从库的同步复制信息、包括连接信息和二进制文件名、位置-- 从库上执行这个命令后，使用show slave status将不会有输出。 3 2台新的mysql中修改从库slave配置, 连接到新的主库地址(我这里通过域名解析) 12CHANGE MASTER TO MASTER_HOST='a_master.b.com',MASTER_PORT=3306,MASTER_USER='repl_user',MASTER_PASSWORD='xxxx',MASTER_LOG_FILE='m1-master-bin.000001',MASTER_LOG_POS=88; 4 由于本机需要安装mysql5.5和mysql5.7所以注意一下 123456# 初始化指定配置文件/usr/local/mysql57/bin/mysqld --defaults-file=/etc/my57.cnf --initialize-insecure --user=mysql --basedir=/usr/local/mysql57 --datadir=/data/u001# 修改/etc/init.d/mysqld57parse_server_arguments `$print_defaults -c /etc/my57.cnf mysqld server mysql_server mysql.server`$bindir/mysqld_safe --defaults-file=/etc/my57.cnf --pid-file="$mysqld_pid_file_path" $other_args &gt;/dev/null &amp; 报错统计 ERROR 1840 (HY000) at line 24: @@GLOBAL.GTID_PURGED can only be set when @@GLOBAL.GTID_EXECUTED is empty. 1执行reset master; The MySQL server is running with the–read-only option so it cannot execute this statement 1执行 set global read_only=0;]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云原生博客汇总]]></title>
    <url>%2F2019%2F10%2F12%2F13-%E4%BA%91%E5%8E%9F%E7%94%9F%E5%8D%9A%E5%AE%A2%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[记录一些云原生技术博客 来自ServiceMesher[8] 群KaiRen’s Blog Zlatan Eevee 国南之境 博客 | 高策 Kakashi’s Blog Alex Wu’s blog | THINK BIG, START SMALL, DELIVER VALUE TO THE BUSINESS 开元DevOps知识库 - 知识管理，时间管理，自我管理 起风了 茶歇驿站 - Gopher, OpenSource Fans, 成长之路有我相伴。 Polar Snow Documentation 云原生实验室 - 米开朗基杨的博客 Jimmy Song - 宋净超的博客|Cloud Native|云原生布道师 漠然的博客 | mritd Blog DevOps – 成长之路 birdben 浮生若梦 CNCF Cloud Native Interactive Landscape 杜屹东的博客 | 学无止境 梦旭随想 ictfox blog CloudNative 架构 我爱西红柿 Doublemine Bingo Huang Arthur Chunqi Li’s Blog Archive | Arthur Chunqi Li’s Blog IT技术工作学习折腾笔记 李佶澳的博客 墨荷琼林官网-编程日志 Archive - Nolla Tomoya’s Blog 君无止境 Jamin Zhang roc - imroc.io|roc的博客|Cloud Native|Kubernetes|Go|Golang Blog | Sysdig sleele的博客 TauCeti blog · TauCeti blog 水晶命匣 | 生命在于折腾，折腾万岁！ 苏洋博客 LinuxTOY Infvie’s Blog | 运维SRE社区博客 Solar MoeLove Hwchiu Learning Notekubernetes/SDN/DevOps 存储领域 yangguanjun]]></content>
      <categories>
        <category>技术文档</category>
        <category>网址</category>
        <category>大佬博客</category>
      </categories>
      <tags>
        <tag>云原生</tag>
        <tag>cloud native</tag>
        <tag>大佬博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk简单记录]]></title>
    <url>%2F2019%2F10%2F11%2F12-awk%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[记录一些简单使用 实例1: 计算nginx日志中某个接口的次数和平均响应时间例如我的a.txt nginx日志格式如下12a.b.com 1.1.1.1 [08/Sep/2019:23:57:01 +0800] "GET /v1/actionname?xxxx HTTP/1.1" 200 386 "-" "Mozilla/5.0 (Linux; Android 9; V1831A Build/P00610; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/68.0.3440.91 Mobile Safari/537.36" "-" "0.023"a.b.com 1.1.1.1 [08/Sep/2019:23:57:01 +0800] "GET /v1/actionname2?xxxx HTTP/1.1" 200 386 "-" "Mozilla/5.0 (Linux; Android 9; V1831A Build/P00610; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/68.0.3440.91 Mobile Safari/537.36" "-" "0.016" 这里我只想取出接口名: /v1/actionname 和 0.023 响应时间 首先我取出这两列1234567cat a.txt|awk -F '"' '&#123;print $(NF-1),$2&#125;'|awk -F '?' '&#123;print $1&#125;'|awk '&#123;print $1" "$3&#125;' &gt; b.txtcat b.txt0.023 /v1/actionname0.016 /v1/actionname2... 命令详解12345678&gt; 第一步 响应时间求和&#123;s[$2]+=$1&#125;: 每遇到一个$2,比如遇到/v1/actionname,记录一个数组s[/v1/actionname] = 所有$1的值的总和&gt; 第二步 算接口的次数&#123;m[$2]++&#125;: 每遇到一个$2,比如遇到/v1/actionname,记录一个数组m[/v1/actionname] = 所有$1的个数&gt; 第三步 取平均值# 这里输出csv文件cat b.txt|awk '&#123;m[$2]++&#125; &#123;s[$2]+=$1&#125; ; END &#123;for(i in m) &#123;print s[i]/m[i] "," m[i] "," i&#125;&#125;'|awk -F "," '$2 &gt; 20'|sort -k2nr &gt; test.csv 例如计算总响应时间,总数,平均值12345678# 这里$17 是接口名 : /v4.5/?xxx , $4 是响应时间 # 每遇到一个$17, 就把$4响应时间累加存到 s中, s可能是 s["/v4.5/?xxx"]=4.75, s["/v3.8/?xxx"]=1.469# 同理m一样, m["/v4.5/?xxx"]=313, m["/v3.8/?xxx"]=64tail -1000 openapi.access.log |awk '&#123;s[$17]+=$4&#125; &#123;m[$17]++&#125; END &#123;for(i in s) &#123;print m[i],"\t",s[i],"\t",s[i]/m[i],"\t",i&#125; &#125;'|sort -k1nr|head -10313 4.75 0.0151757 /v4.5/?xxx64 1.469 0.0229531 /v3.8/?xxx]]></content>
      <categories>
        <category>linux</category>
        <category>awk</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql简单记录]]></title>
    <url>%2F2019%2F10%2F10%2F11-mysql%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[简单记录一些mysql知识点 SQL 语句主要可以划分为以下 3 个类别123DDL（Data Definition Languages）语句：数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。DML（Data Manipulation Language）语句：数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查）DCL（Data Control Language）语句：数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。 清空表123456789删除表信息的方式有两种 :truncate table table_name;delete * from table_name;注 : truncate操作中的table可以省略，delete操作中的*可以省略truncate、delete 清空表数据的区别 :1&gt; truncate 是整体删除 (速度较快)，delete是逐条删除 (速度较慢)2&gt; truncate 不写服务器 log，delete 写服务器 log，也就是 truncate 效率比 delete高的原因3&gt; truncate 不激活trigger (触发器)，但是会重置Identity (标识列、自增字段)，相当于自增列会被置为初始值，又重新从1开始记录，而不是接着原来的 ID数。而 delete 删除以后，identity 依旧是接着被删除的最近的那一条记录ID加1后进行记录。如果只需删除表中的部分记录，只能使用 DELETE语句配合 where条件 备份12345# 全量锁表备份(不可写)mysqldump --lock-all-tables --all-databases &gt; ALLDB.sql# 仅导出所有表的结构mysqldump --opt -d 数据库名 -u root -p &gt; xxx.sql slave 中修改master_host12345678910# 查看 master.info中信息# 查看 show slave status\G 中 Master_Host# 修改的步骤需要先停止slave1 stop slave ;2 change master to master_host='xxx.xxx.xxx'; 首次配置主库: CHANGE MASTER TO MASTER_HOST='a_master.b.com',MASTER_PORT=3306,MASTER_USER='repl_user',MASTER_PASSWORD='xxxx',MASTER_LOG_FILE='m1-master-bin.000001',MASTER_LOG_POS=88;3 start slave ; mysql问题: navicat连接数据库很慢123456报错: 2013-Lost connection to MYSQL server at 'reading for initial communication packet'说明: 只有windows 的navicat会出现上面报错, windows上通过mysql命令连接时 也很慢#添加如下内容:[mysqld]skip-name-resolve mysql问题: mysql5.7 错误总结-ERROR 1067 (42000): Invalid default value for TIMESTAMP123456show variables like 'sql_mode';+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+| sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |+---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ 这是因为sql_mode中的NO_ZEROR_DATE导制的，在strict mode中不允许’0000-00-00’作为合法日期 将上面的NO_ZERO_DATE改为下面的 ALLOW_INVALID_DATES 12set sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,ALLOW_INVALID_DATES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';set session sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; 上面的设置是临时设置，在重新登陆后，该设置又恢复为NO_ZERO_DATE mysql5.5主+mysql5.7从 问题123ERROR 1794 (HY000): Slave is not configured or failed to initialize properly. You must at least set --server-id to enable either a master or a slave. Additional error messages can be found in the MySQL error log.server_uuid是5.6的gtid特性引入的一个配置，把mysql5.7的 rpl_slave.cc文件中get_master_uuid函数换成5.6对应的函数就可以了。 mysql一些info信息统计123456789101112131415161718#tbl_size.sqluse information_schema;SELECT TABLE_NAME, ENGINE, ROUND((DATA_LENGTH/1024/1024),2) as DataM , ROUND((INDEX_LENGTH/1024/1024),2) as IndexM, ROUND(((DATA_LENGTH+INDEX_LENGTH)/1024/1024),2) as AllM, TABLE_ROWS, TABLE_COMMENTFROM TABLESWHERE TABLE_SCHEMA = 'hzkj_zh'ORDER BY AllM DESC;# 生成excel表格mysql test &lt;tbl_size.sql &gt;tbl_info_20191028.txt 跳过主从同步错误1234stop slave;SET GLOBAL sql_slave_skip_counter =1;start slave;show slave status\G; mysql information_schema.TABLES表中的table_rows 字段值与’count(*)’ 值不同 查看information_schema 123456789101112131415use information_schema;SELECT TABLE_NAME, TABLE_ROWSFROM TABLESWHERE TABLE_SCHEMA = 'zz' and TABLE_NAME = 'zzz';+---------------------+------------+| TABLE_NAME | TABLE_ROWS |+---------------------+------------+| zzz | 42411396 |+---------------------+------------+ 但是会发现和 1" select count(*) from 某张表; " 执行得到的值是不相同的！那是因为： 1: 默认情况下 mysql 对表进行增删操作时，是不会自动更新 information_schema 库中 tables 表的 table_rows 字段的，在网上搜索一下发现说：只有10%的行数发生变化才会自动收集（没有亲自验证过！）； 2: 执行 Analyze table tableName; 会统计所有表数据（在生产环境中不建议使用，因为会锁表！）；原文链接：mysql information_schema.TABLES表中的table_rows 字段值与count值不同]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos6安装nginx1.16+php7.2]]></title>
    <url>%2F2019%2F09%2F27%2F10-centos6%E5%AE%89%E8%A3%85nginx1-16-php7-2%2F</url>
    <content type="text"><![CDATA[记录简单的安装nginx和php的配置,仅供参考 先准备环境123456789# 更新源mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakcurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repoyum clean allyum makecacheyum install -y epel-release# 安装一些依赖包yum -y install gcc make cmake ncurses-devel libxml2-devel libtool-ltdl-devel gcc-c++ autoconf automake bison zlib-devel openssl-devel pcre-devel libxml2 libxml2-devel libcurl libcurl-devel autoconf automake libtool-ltdl libtool-ltdl-devel libjpeg libjpeg-turbo-devel libmcrypt-devel libpng-devel centos6编译安装nginx123456789101112131415#首先官网下载1.16cd /usr/local/src/wget http://nginx.org/download/nginx-1.16.0.tar.gztar -xvf nginx-1.16.0.tar.gz# 编译简单的模块./configure --prefix=/usr/local/nginx/ --with-http_ssl_module --with-http_stub_status_module --with-http_stub_status_modulemake -j4make install# 通过启动脚本启动chmod +x /etc/init.d/nginxservice nginx start# 开机启动chkconfig nginx on nginx 启动脚本 /etc/init.d/nginx12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#!/bin/bash# nginx Startup script for the Nginx HTTP Server# it is v.0.0.2 version.# chkconfig: - 85 15# description: Nginx is a high-performance web and proxy server.# It has a lot of features, but it's not for everyone.# processname: nginx# pidfile: /var/run/nginx.pid# config: /usr/local/nginx/conf/nginx.confnginx=/usr/local/nginx/sbin/nginxnginx_config=/usr/local/nginx/conf/nginx.confnginx_pid=/var/run/nginx.pidRETVAL=0prog="nginx"# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ $&#123;NETWORKING&#125; = "no" ] &amp;&amp; exit 0[ -x $nginx ] || exit 0# Start nginx daemons functions.start() &#123;if [ -e $nginx_pid ];then echo "nginx already running...." exit 1fi echo -n $"Starting $prog: " daemon $nginx -c $&#123;nginx_config&#125; RETVAL=$? echo [ $RETVAL = 0 ] &amp;&amp; touch /var/lock/subsys/nginx return $RETVAL&#125;# Stop nginx daemons functions.stop() &#123; echo -n $"Stopping $prog: " killproc $nginx RETVAL=$? echo [ $RETVAL = 0 ] &amp;&amp; rm -f /var/lock/subsys/nginx /usr/local//nginx/logs/nginx.pid&#125;reload() &#123; echo -n $"Reloading $prog: " #kill -HUP `cat $&#123;nginx_pid&#125;` killproc $nginx -HUP RETVAL=$? echo&#125;# See how we were called.case "$1" instart) start ;;stop) stop ;;reload) reload ;;restart) stop start ;;status) status $prog RETVAL=$? ;;*) echo $"Usage: $prog &#123;start|stop|restart|reload|status|help&#125;" exit 1esacexit $RETVAL nginx.conf简单配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697user nobody nobody;worker_processes auto;worker_rlimit_nofile 102400;pid /var/run/nginx.pid;events &#123; use epoll; worker_connections 102400;&#125;http &#123; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; ###定义一个log_format log_format main '[ $host $request_time $upstream_addr $upstream_response_time ] ' '$status ' '$remote_addr - $remote_user [$time_local] "$request" ' '$body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for " "$bytes_sent" " $request_body"' ; ###日志目录配置 #日志全部写入/nginx_logs/access.log 文件中。关闭最后两个server_name的日志 access_log /data/nginx_logs/access.log server_name_main; error_log /data/nginx_logs/error.log notice; ###杂项配置 charset utf-8; #server name的hash表， server_names_hash_bucket_size 128; #请求头如果过小，那么会引起400错误。一般如果cookie过大，会引起问题。getconf PAGESIZE系统分页 client_header_buffer_size 8k; client_body_buffer_size 512k; large_client_header_buffers 16 16k; client_max_body_size 30m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; #fastcgi通用配置 fastcgi_connect_timeout 600; fastcgi_send_timeout 600; fastcgi_read_timeout 600; fastcgi_buffer_size 128k; fastcgi_buffers 8 256k; fastcgi_busy_buffers_size 256k; fastcgi_temp_file_write_size 256k; ###代理有关的配置 proxy_connect_timeout 600; proxy_read_timeout 600; proxy_send_timeout 600; proxy_buffer_size 512k; proxy_buffers 6 512k; proxy_busy_buffers_size 512k; proxy_temp_file_write_size 512k; #或许在于测试,代理服务器不主动关闭客户端，防止499错误 proxy_ignore_client_abort on; ###gzip配置 gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml; gzip_vary on; include /usr/local/nginx/conf/mime.types; default_type application/octet-stream; # 隐藏nginx版本信息 server_tokens off; server &#123; listen 80 default_server; server_name _; #server_name localhost; index index.html index.htm; root html; deny all; location / &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; centos6编译安装php712345678910111213141516# 首先安装freetype2.4tar -xvf freetype-2.4.0.tar.gzcd freetype-2.4.0./configure --prefix=/usr/local/freetypemake -j4 &amp;&amp; make install# 编译php7(不需要的可以去掉)tar -xvf php-7.2.2.tar.gzcd php-7.2.2./configure --prefix=/usr/local/php --with-libxml-dir=/usr/ --with-pdo-mysql=mysqlnd --with-zlib --with-libxml-dir --with-openssl --enable-mysqlnd --enable-mbstring --with-config-file-path=/usr/local/php/etc/ --with-config-file-scan-dir=/usr/local/php/etc/conf.d --enable-fpm --with-freetype-dir=/usr/local/freetype --with-jpeg-dir --with-png-dir --with-gd --enable-gd-native-ttf --enable-pdo --enable-mbstring --enable-bcmathmake -j 4 &amp;&amp; make install# 安装composercurl -sS https://getcomposer.org/installer | php &amp;&amp; mv composer.phar /usr/bin/composer 配置1234567891011# php.ini配置(具体配置内容自行修改)cp php.ini-production /usr/local/php7/etc/php.inicp /usr/local/php7/etc/php-fpm.conf.default /usr/local/php7/etc/php-fpm.confcp /usr/local/php7/etc/php-fpm.d/www.conf.default /usr/local/php7/etc/php-fpm.d/www.conf# 启动脚本cp ./sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod +x /etc/init.d/php-fpm# 创建linkln -s /usr/local/php7 /usr/local/php 启动php-fpm12service php-fpm startchkconfig php-fpm on]]></content>
      <categories>
        <category>技术文档</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>php7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux遇到一些问题统计总结]]></title>
    <url>%2F2019%2F09%2F26%2F9-linux%E9%81%87%E5%88%B0%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[记录一些Linux,nginx或其他服务一些问题 Linux问题: 升级内核123456789101112131415rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm# 查看可升级的内核yum --disablerepo="*" --enablerepo="elrepo-kernel" list availableyum --enablerepo=elrepo-kernel install kernel-ml# 查看已经安装的内核cat /boot/grub2/grub.cfg |grep menuentry# 设置5.3的为默认grub2-set-default 'CentOS Linux (5.3.13-1.el7.elrepo.x86_64) 7 (Core)'# grub2-editenv listsaved_entry=CentOS Linux (5.3.13-1.el7.elrepo.x86_64) 7 (Core) linux问题: tcpdump抓包tcp第三次握手ack为1 执行命令监听: tcpdump -n port 80 (想要详细信息加 -vv) 客户端 telnet x.x.x.x 80 日志如下: 123456tcpdump -n port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on enp0s3, link-type EN10MB (Ethernet), capture size 262144 bytes11:16:40.689157 IP 192.168.54.141.53444 &gt; 192.168.53.106.http: Flags [SEW], seq 1306124348, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 458678777 ecr 0,sackOK,eol], length 011:16:40.689724 IP 192.168.53.106.http &gt; 192.168.54.141.53444: Flags [S.E], seq 1553518959, ack 1306124349, win 64308, options [mss 1410,sackOK,TS val 4208119240 ecr 458678777,nop,wscale 7], length 011:16:40.690320 IP 192.168.54.141.53444 &gt; 192.168.53.106.http: Flags [.], ack 1, win 4106, options [nop,nop,TS val 458678778 ecr 4208119240], length 0 这里第一和第二次握手都没有问题, 第三次 ack 1, 并非是seq+1 这里提一下ACK, ACK 是确认值, ack 是确认编号, 第一次握手ACK=0,在第二次握手开始ACK=1, 而ack是=seq+1(收到的随机数+1) 那么这里ack 1 是啥呢? … 应该就是默认tcpdump 显示成相对值了, 通过-S 参数会显示绝对值 执行命令监听: tcpdump -S -n port 8012345tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on enp0s3, link-type EN10MB (Ethernet), capture size 262144 bytes11:16:54.806628 IP 192.168.54.141.53516 &gt; 192.168.53.106.http: Flags [S], seq 316359286, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 458692791 ecr 0,sackOK,eol], length 011:16:54.806861 IP 192.168.53.106.http &gt; 192.168.54.141.53516: Flags [S.], seq 1113466641, ack 316359287, win 64308, options [mss 1410,sackOK,TS val 4208133357 ecr 458692791,nop,wscale 7], length 011:16:54.807576 IP 192.168.54.141.53516 &gt; 192.168.53.106.http: Flags [.], ack 1113466642, win 4106, options [nop,nop,TS val 458692792 ecr 4208133357], length 0 三次握手图 四次挥手图 linux问题: 禁ping12345678# 一次性修改echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all# 开机自动修改echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all# 永久禁用,加入到/etc/sysctl.confnet.ipv4.icmp_echo_ignore_all=1 linux问题: 文件锁问题1234567问题描述: php slowlog 出现session_start() 慢问题原因: 我们这边有A 和B 两个二级域名,A 会请求 B, 并且由于测试环境在同一台服务器,公用一个php,所以在发生调用的时候同时写了session,而php的sessions配置是默认的file方式, 这就造成了锁的问题问题解决: 1. 修改代码部分2. php的session配置改成redis session.save_handler = redis session.save_path = "tcp://x.x.x.x:xxxx" linux问题: 内存释放问题123456问题描述: 开发这边写了个统计脚本, 占用49G内存, 从日志发现脚本已经全部执行完成, 但是php脚本依然存在问题原因: 通过 strace -p pid 观察进程, 发现是持续性的做内存释放操作 munmap(0x7f6db77ad000, 266240) = 0持续执行munmap函数是因为一直在释放内存(毕竟49G), 结果 =0 说明内存释放执行函数是返回正常了问题解决: 1. 修改代码降低内存2. 等待一段时间内存会释放完成(测试80分钟释放完毕) nginx问题: 静态文件分离对于一般的nginx+php的方式, 我们php采用nobody用户,而代码/lumen采用web-www用户, 这样的好处是页面访问到/lumen时是nobody用户, 是无法修改代码的 可能我们需求是上用户upload图片等, 这时候就可能被传上某个a.php, 这就有可能被代码注入(一般来说图片是放cdn,配置单独域名回源的,这里是直接存在项目目录) 所以为了防止代码注入,我们需要限制upload目录的访问权限 1234# nginx配置如下 location ~ /images/.*\.(gif|jpg|jpeg|png)$ &#123; root /lumen/storage/uploads/; &#125; 这样我们图片传到 /lumen/storage/uploads/images/ 目录, 访问是 www.xxx.com/images/x.png 来访问 且不允许其他类型文件访问. nginx问题: root 和alias在配置文件映射的时候，如果使用了正则表达式，那么可能会出现无法访问文件，nginx可能会将所有的文件都映射成为文件夹，导致文件映射失败的情况出现； root的例子 1234location /a/ &#123; root /lumen/public;&#125;这里实际访问的路径: www.xxx.com/a/ -&gt; /lumen/public/a/ alias的例子 12345# 注意这里目录最后加上/location /a/ &#123; alias /lumen/public/;&#125;这里实际访问的路径: www.xxx.com/a/ -&gt; /lumen/public/ nginx问题: 隐藏版本信息123Syntax: server_tokens on | off | build | string;Default: server_tokens on;Context: http, server, location nginx问题: 日志出现encode内容如何查看12# python2 执行decode&gt;&gt;&gt; print "\x22content\x22\x0D\x0A\x0D\x0A\xE8\x8A\x8A\xE8\x8A\x8A\xE8\xBF\x98\xE6\x80\x95\xE5\xA6\x9E\xE5\xA6\x9E\xE4\xB8\x8D\xE8\x80\x81\xE5\xAE\x9E\xEF\xBC\x8C\xE7\x89\xB9\xE5\x9C\xB0\xE8\xBF\x87\xE6\x9D\xA5\xE8\xA7\x86\xE5\xAF\x9F\xE4\xB8\x80\xE4\xB8\x8B\x0D\x0A".decode('utf-8') nginx问题: default配置未设置nginx 未设置default时, 如果直接访问服务器外网ip, 会去请求到第一个匹配的server段, 有可能会请求到后端的服务器的内容, 这很有可能暴露我们不想暴露的服务一般来说开头添加如下配置 123456 server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; deny all;&#125; nginx 配置已经配置域名方式访问, 如果访问ip会返回403, 正常来说返回403已经不会对服务器造成压力了 可是万万没想到虽然返回了403, 但是也有700字节大小, 大量请求对小带宽来说还是有压力的 123456789101112# server &#123; listen 80 default_server; listen [::]:80 default_server; server_name _; return 499; &#125;#其他location,if配置,if ($host != a.example.com) &#123; return 499;&#125; nginx1.11以前trace_id 生成问题 如果是前端(upstream) 123456789101112131415161718log_format server_name_main '"$request_trace_id" [ $host $request_time ] ' '[ $upstream_addr $upstream_response_time ] ' '$status ' '$remote_addr - $remote_user [$time_local] "$request" ' '$body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for" "$bytes_sent"' '&#123;$request_body&#125;' ;...server &#123; set $request_trace_id $pid$connection$bytes_sent$msec; if ( $http_x_request_id != "" )&#123; set $request_trace_id $http_x_request_id; &#125; add_header Bq_F_Traceid $request_trace_id; # 一定要写到location中, 因为proxy_pass location / &#123; proxy_pass http://xxxx; proxy_set_header X-Request-Id $request_trace_id; &#125;&#125; 如果是后端节点 12345678910111213141516171819# 一定要写到server段, 否则后端可能报404错误server &#123; listen 80; server_name openapi-community-alpha.zhangzw.com ; set $request_trace_id trace-id-$pid-$connection-$bytes_sent-$msec; # 如果请求头中已有该参数,则获取即可;如果没有,则使用$request_id进行填充 set $temp_request_id $http_x_request_id; if ($temp_request_id = "") &#123; set $temp_request_id $request_trace_id; &#125; # 屏蔽掉原来的请求头参数 # proxy_set_header x_request_id ""; proxy_set_header X-Request-Id ""; # 设置向后转发的请求头参数 proxy_set_header X-Request-Id $temp_request_id; location / &#123; try_files $uri $uri/ /index.php?$query_string; &#125;&#125; 修改swap123456789dd if=/dev/zero of=/data/swapfilenew bs=4096 count=4096000swapoff -a /sbin/mkswap /data/swapfilenew/sbin/swapon /data/swapfilenewvim /etc/fstab/data/swapfilenew none swap defaults 0 0]]></content>
      <categories>
        <category>技术文档</category>
        <category>linux</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql5.7二进制部署]]></title>
    <url>%2F2019%2F09%2F26%2F8-mysql5-7%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[二进制方式部署mysql5.7 下载glibc二进制包12345#打开下载页面, 可能会有小版本更新(注意：选择操作系统时选Linux-Generic）https://dev.mysql.com/downloads/mysql/5.7.html#downloads# 最新的可能有小版本变化wget https://cdn.mysql.com/Downloads/MySQL-5.7/mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz 安装配置1234567891011121314151617181920212223242526272829303132333435363738394041tar -xvf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gzmv mysql-5.7.24-linux-glibc2.12-x86_64 /usr/local/cd /usr/local/# 我的镜像是安装过5.5mysql, 所以需要mv一下mv mysql mysql-5.5.37# 由于以前安装过php指定了该mysq目录, 这可能导致以前安装的php缺少libmysqlclient.so.18ln -s /usr/local/mysql-5.5.37/lib/libmysqlclient.so.18 /usr/local/mysql-5.7.24-linux-glibc2.12-x86_64/lib/libmysqlclient.so.18ln -s mysql-5.7.24-linux-glibc2.12-x86_64 mysql# 添加启动文件\cp mysql/support-files/mysql.server /etc/init.d/mysqldecho "PATH=$PATH:/usr/local/mysql/bin/" &gt;&gt;~/.bashrc# 可选wget http://centos.mirrors.ucloud.cn/centos/6/os/x86_64/Packages/numactl-2.0.9-2.el6.x86_64.rpmyum localinstall numactl-2.0.9-2.el6.x86_64.rpm\rm numactl-2.0.9-2.el6.x86_64.rpmuseradd mysql# 配置下mysql的数据目录cd /data/mkdir u01mkdir u02chown -R mysql.mysql u01chown -R mysql.mysql u02chmod 750 u01chmod 750 u02cd /data/u01/# 初始化/usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/u01cat auto.cnf# 启动服务 (在这之前准备好 /etc/my.cnf)/etc/init.d/mysqld start# 记录下variablesmysql -e "show global variables" &gt;mysql_option_default.log my.cnf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172[client]port = 3306socket = /data/u01/mysql.sock[mysql]prompt="\u@m1-u [\d]&gt; "no-auto-rehash[mysqld]user = mysqlport = 3306basedir = /usr/local/mysqldatadir = /data/u01socket = /data/u01/mysql.sockpid-file = /data/u01/m1-u.pidtmpdir = /data/u02server-id = 1001character-set-server = utf8skip_name_resolve = 1innodb_file_per_table = 1explicit_defaults_for_timestamp = 0# buffer&amp;cachetable_open_cache = 100table_definition_cache = 400table_open_cache_instances = 64sort_buffer_size = 4Mjoin_buffer_size = 4Mread_buffer_size = 8Mread_rnd_buffer_size = 4M# thread&amp;connectionthread_stack = 256Kthread_cache_size = 768back_log = 1024max_connections = 3000max_connect_errors = 1000000# temptabletmp_table_size = 32Mmax_heap_table_size = 32M# networkmax_allowed_packet = 32M#lock_wait_timeout = 3600#interactive_timeout = 600#wait_timeout = 600# query cachequery_cache_size = 0query_cache_type = 0# 设置errorlog、slowlog和generallog的时区，默认UTClog_timestamps = SYSTEM# error-loglog_error = /data/u02/mysqld.log# slow-logslow_query_log = 1slow_query_log_file = /data/u02/slow.loglong_query_time = 0.1log_queries_not_using_indexes =1log_throttle_queries_not_using_indexes = 60min_examined_row_limit = 100log_slow_admin_statements = 1log_slow_slave_statements = 1# general log#general-log = 1general_log_file=/data/u02/query.log# binlogbinlog_format = rowbinlog_checksum = 1log-bin = /data/u02/bdm1-binlog-bin-index = /data/u02/bdm1-bin.indexsync_binlog = 0binlog_cache_size = 4Mmax_binlog_cache_size = 2Gmax_binlog_size = 512Mexpire_logs_days = 15# GTIDgtid_mode = onenforce_gtid_consistency = 1log_slave_updates# Replicationmaster_info_repository = TABLErelay_log_info_repository = TABLEslave-rows-search-algorithms = 'INDEX_SCAN,HASH_SCAN'relay_log_recovery = 1relay_log_purge = 1relay-log=/data/u02/bdm1-relay-binrelay-log-index=/data/u02/bdm1-relay-bin.index# innodb-buffer&amp;cacheinnodb_buffer_pool_size = 2Ginnodb_buffer_pool_instances = 4#innodb_additional_mem_pool_size = 16Minnodb_max_dirty_pages_pct = 50# innodb loginnodb_data_file_path = ibdata1:1G:autoextendinnodb_log_file_size = 1Ginnodb_log_files_in_group = 2innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 32M#innodb_max_undo_log_size = 4G#innodb_undo_directory = undologinnodb_undo_tablespaces = 4# innodb-ioinnodb_flush_method = O_DIRECTinnodb_io_capacity = 600innodb_io_capacity_max = 2000innodb_flush_sync = 0innodb_flush_neighbors = 0#innodb_lru_scan_depth = 4000innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_purge_threads = 4innodb_page_cleaners = 4# transaction,lock#innodb_sync_spin_loops = 100#innodb_spin_wait_delay = 30innodb_lock_wait_timeout = 10innodb_print_all_deadlocks = 1innodb_rollback_on_timeout = 1innodb_open_files = 65535innodb_online_alter_log_max_size = 2G# innodb statusinnodb_status_file = 1# 注意: 开启 innodb_status_output &amp; innodb_status_output_locks 后, 可能会导致log-error文件增长较快innodb_status_output = 0innodb_status_output_locks = 0#performance_schemaperformance_schema = 1performance_schema_instrument = '%=on'#innodb monitorinnodb_monitor_enable="module_innodb"innodb_monitor_enable="module_server"innodb_monitor_enable="module_dml"innodb_monitor_enable="module_ddl"innodb_monitor_enable="module_trx"innodb_monitor_enable="module_os"innodb_monitor_enable="module_purge"innodb_monitor_enable="module_log"innodb_monitor_enable="module_lock"innodb_monitor_enable="module_buffer"innodb_monitor_enable="module_index"innodb_monitor_enable="module_ibuf_system"innodb_monitor_enable="module_buffer_page"innodb_monitor_enable="module_adaptive_hash"# MyISAMkey_buffer_size = 1024Mbulk_insert_buffer_size = 64Mmyisam_sort_buffer_size = 128Mmyisam_repair_threads = 1[mysqldump]quickmax_allowed_packet = 32M]]></content>
      <categories>
        <category>技术文档</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql5.7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s部署storageclass动态创建pv(nfs&rbd)]]></title>
    <url>%2F2019%2F09%2F26%2F7-k8s%E9%83%A8%E7%BD%B2storageclass%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv-nfs-rbd%2F</url>
    <content type="text"><![CDATA[考虑到k8s存储的问题, 本机目录挂载存在太大局限性, 多node多pod的服务存储急迫需要共享存储, 这里简单应用k8s storageclass nfs和rbd存储 第一部分 nfs这里单节点简单配置nfs(高并发可采用nfs+rsync+inotify或Sersync) 高并发参考 NFS高可用(NFS+keepalive+Sersync)inotify+rsync实时备份总结 12345678910#安装nfsyum install -y nfs-utils rpcbind# 创建目录mkdir /data/nfsecho "/data/nfs 192.168.0.0/24(rw,sync,no_root_squash) " &gt;&gt;/etc/exports# 启动服务systemctl start rpcbindsystemctl start nfs k8s部署storageclass环境-nfs导入外部配置12345git clone https://github.com/kubernetes-incubator/external-storage.gitcd external-storage/nfs-client/deploy#注意1 node节点需要安装nfs-utils(centos7),nfs-common(ubuntu) 修改deployment.yaml12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: nfs.com/nfs - name: NFS_SERVER value: 192.168.0.134 - name: NFS_PATH value: /data/nfs volumes: - name: nfs-client-root nfs: server: 192.168.0.134 path: /data/nfs 修改class.yaml1234567apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfsprovisioner: nfs.com/nfsparameters: archiveOnDelete: "false" rbac.yaml不用修改1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859rbac.yamlkind: ServiceAccountapiVersion: v1metadata: name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionerrules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 部署nfs环境(创建nfs存储类)1kubectl apply -f rbac.yaml -f class.yaml -f deployment.yaml k8s中部署nginx项目采用nfs存储部署nginx-deployment-nfs.yaml(测试nfs) 这种方式会创建一个pvc 挂载到多个pod中,这种方式适合nginx-html挂载 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465---kind: PersistentVolumeClaimapiVersion: v1metadata: name: html0-deploy-nfs annotations: volume.beta.kubernetes.io/storage-class: 'nfs'spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx0-deployspec: replicas: 2 template: metadata: labels: app: nginx0-deploy spec: containers: - name: nginx0-deploy image: hub.zhangzw.com/bq/nginx:1.15.12 ports: - containerPort: 80 volumeMounts: - name: html0-deploy-nfs mountPath: /usr/share/nginx/html - name: nginx-config mountPath: "/etc/nginx/conf.d" volumes: - name: nginx-config configMap: name: nginx-config - name: html0-deploy-nfs persistentVolumeClaim: claimName: html0-deploy-nfs---apiVersion: v1kind: ConfigMapmetadata: name: nginx-configdata: default.conf: | server &#123; listen 80; server_name localhost; root /usr/share/nginx/html/; access_log /var/log/nginx/host_access.log; error_log /var/log/nginx/host_error.log debug; location / &#123; root /usr/share/nginx/html/; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; &#125; 部署nginx-statefulset-nfs.yaml(测试nfs) 这里statefulset 方式会创建多个pvc, 每个pod的html就可以都不一样! 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758---apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: nginx3spec: serviceName: "nginx" replicas: 2 volumeClaimTemplates: - metadata: name: html annotations: volume.beta.kubernetes.io/storage-class: "nfs" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 2Gi template: metadata: labels: app: nginx spec: containers: - name: nginx image: hub.zhangzw.com/bq/nginx:1.15.12 volumeMounts: - mountPath: "/usr/share/nginx/html/" name: html - mountPath: "/etc/nginx/conf.d" name: nginx-config volumes: - name: nginx-config configMap: name: nginx-config---apiVersion: v1kind: ConfigMapmetadata: name: nginx-configdata: default.conf: | server &#123; listen 80; server_name localhost; root /usr/share/nginx/html/; access_log /var/log/nginx/host_access.log; error_log /var/log/nginx/host_error.log debug; location / &#123; root /usr/share/nginx/html/; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; &#125; 另外说明一下将nfs作为文件存储类似mount方式,这种方式不适用于多容器自动化部署 ,显然这种并不适合ceph rbd存储, cephfs是可以的首先需要在nfs目录创建需要挂载的目录12#例如mkdir -p /data/nfs/k8s-db-t/mysql-data-dev 在部署的yml中直接mount nfs的目录1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: mysql-server namespace: devopsspec: replicas: 1 template: metadata: labels: app: mysql-server spec: containers: - image: mysql:5.7.16 imagePullPolicy: Always name: mysql-server ports: - containerPort: 3306 protocol: TCP volumeMounts: - name: mysql-data mountPath: /var/lib/mysql resources: requests: cpu: 40m memory: 32Mi limits: cpu: "300m" memory: 256Mi env: - name: MYSQL_ROOT_PASSWORD value: "admin" - name: MYSQL_DATABASE value: "gogs" - name: MYSQL_USER value: "gogs" - name: MYSQL_PASSWORD value: "gogspass" - name: TZ value: "Asia/Shanghai" volumes: - name: mysql-data nfs: server: 192.168.0.134 path: /data/nfs/k8s-db-t/mysql-data-dev---apiVersion: v1kind: Servicemetadata: name: mysql-service namespace: devopsspec: clusterIP: None selector: app: mysql-server ports: - name: http port: 3306 第二部分 cephk8s部署storageclass环境-ceph如果集群是用kubeadm部署的，由于controller-manager官方镜像中没有rbd命令，所以我们要导入外部配置12git clone https://github.com/kubernetes-incubator/external-storage.gitcd external-storage/ceph/rbd/deploy 以下整合在一个文件, 两个版本,默认 和retain storageclass-cepm.com-rbd.yaml123456789101112131415161718192021222324252627282930313233343536---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admintype: kubernetes.io/rbd---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admin namespace: ns-elastictype: kubernetes.io/rbd---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rbd annotations: storageclass.kubernetes.io/is-default-class: "true"provisioner: ceph.com/rbdparameters: monitors: 192.168.0.134:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: storageclass-rbd userId: admin userSecretName: ceph-secret-admin fsType: ext4 imageFormat: "2" imageFeatures: "layering" storageclass-cepm.com-rbd-retain.yaml12345678910111213141516171819202122232425262728293031323334353637---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admintype: kubernetes.io/rbd---apiVersion: v1data: key: QVFEYzJRbGQ1VjI5THhBQU00WUtPUU5sUVJqdWtLSWJ2VDZ0a3c9PQ==kind: Secretmetadata: name: ceph-secret-admin namespace: ns-elastictype: kubernetes.io/rbd---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rbd-retain annotations: storageclass.kubernetes.io/is-default-class: "false"provisioner: ceph.com/rbdreclaimPolicy: Retainparameters: monitors: 192.168.0.134:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: storageclass-rbd-retain userId: admin userSecretName: ceph-secret-admin fsType: ext4 imageFormat: "2" imageFeatures: "layering" k8s中部署nginx项目采用 ceph.com/rbd 和nfs类似, 这里省略 以上采用的是persistentVolumeClaim 方式动态分配全部内容]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>存储</category>
        <category>ceph</category>
        <category>storageclass</category>
        <category>nfs</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ceph</tag>
        <tag>k8s存储</tag>
        <tag>nfs</tag>
        <tag>storageclass</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph安装部署]]></title>
    <url>%2F2019%2F09%2F26%2F6-ceph%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。 简单了解什么是块存储/对象存储/文件系统存储？ceph 目前提供对象存储（RADOSGW）、块存储RDB以及 CephFS 文件系统这 3 种功能。对于这3种功能介绍，分别如下： 对象存储，也就是通常意义的键值存储，其接口就是简单的GET、PUT、DEL 和其他扩展，代表主要有 Swift 、S3 以及 Gluster 等； 块存储，这种接口通常以 QEMU Driver 或者 Kernel Module 的方式存在，这种接口需要实现 Linux 的 Block Device 的接口或者 QEMU 提供的 Block Driver 接口，如 Sheepdog，AWS 的 EBS，青云的云硬盘和阿里云的盘古系统，还有 Ceph 的 RBD（RBD是Ceph面向块存储的接口）。在常见的存储中 DAS、SAN 提供的也是块存储； 文件存储，通常意义是支持 POSIX 接口，它跟传统的文件系统如 Ext4 是一个类型的，但区别在于分布式存储提供了并行化的能力，如 Ceph 的 CephFS (CephFS是Ceph面向文件存储的接口)，但是有时候又会把 GlusterFS ，HDFS 这种非POSIX接口的类文件存储接口归入此类。当然 NFS、NAS也是属于文件系统存储； 参考教程Kubernetes 集成 Ceph 后端存储教程centos7安装ceph集群 准备配置源12345678910111213141516171819202122cat &gt;/etc/yum.repos.d/ceph.repo&lt;&lt;EOF[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMSenabled=0gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1EOF 安装过卸载123ceph-deploy purge dk1-t dk2-tceph-deploy purgedata dk1-t dk2-tceph-deploy forgetkeys 在dk2-t节点创建集群 mon模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647yum install ceph-deploy -yceph-deploy --versionmkdir /data/cephcd /data/cephceph-deploy new dk2-t# 查看配置文件ls -l# 配置ceph.conf[global]...# 如果有多个网卡，应该配置如下选项，# public network是公共网络，负责集群对外提供服务的流量# cluster network是集群网络，负载集群中数据复制传输通信等# 本次实验使用同一块网卡，生境环境建议分别使用一块网卡public network = 192.168.0.0/22cluster network = 192.168.0.0/22osd pool default size = 2# 安装 ceph 包# 如果按照官方文档安装方法 会重新配置安装官方ceph源# 由于网络问题，安装可能会出错，需要多次执行# ceph-deploy install 其实只是会安装 ceph ceph-radosgw 两个包# ceph-deploy install lab1 lab2 lab3# 推荐使用阿里源安装，因为使用ceph-deploy安装会很慢# 使用如下命令手动安装包，替代官方的 ceph-deploy install 命令# 如下操作在所有node节点上执行export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-luminous/el7export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc# 先执行是因为 ceph-deploy install太慢yum install -y ceph ceph-radosgwceph-deploy install dk2-t# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy dk1-t dk2-t# 额外mon节点，mon节点也需要高可用ceph-deploy mon add dk1-t 在dk2-t节点创建集群 mgr模块123# 部署manager （luminous+）12及以后的版本需要部署# 本次部署 jewel 版本 ，不需要执行如下命令 ceph-deploy mgr create dk2-t 在dk2-t节点创建集群 osd模块1234# 12的版本(这里挂载一个 10G的磁盘 /dev/sdb)# create 命令一次完成准备 OSD 、部署到 OSD 节点、并激活它。 create 命令是依次执行 prepare 和 activate 命令的捷径。ceph-deploy osd create --data /dev/sdb dk2-tceph-deploy osd create --data /dev/sdc dk1-t 如何卸载osd1234567891011121314# 查看ceph osd tree# 节点状态标记为outceph osd out osd.0# 从crush中移除节点ceph osd crush remove osd.0# 删除节点ceph osd rm osd.0# 删除节点认证（不删除编号会占住）ceph auth del osd.0 查看 mon 信息12345678ceph mon dumpdumped monmap epoch 1epoch 1fsid 4620d0c7-4458-4ff9-9296-d1318058bafclast_changed 2019-06-19 14:44:41.361005created 2019-06-19 14:44:41.3610050: 192.168.0.134:6789/0 mon.dk2-t 配置文件内容 /etc/ceph/ceph.conf1234567891011[global]public network = 192.168.0.0/22cluster network = 192.168.0.0/22osd pool default size = 2fsid = 4620d0c7-4458-4ff9-9296-d1318058bafcmon_initial_members = dk2-tmon_host = 192.168.0.134auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_max_pg_per_osd = 1000 ceph 一些测试命令创建 rbd pool，名字叫做 kube1ceph osd pool create kube 256 256 如何取得admin的密钥12ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '&#123;print $3&#125;'AQAn/19bbb21GBAA1kc0HRWoGjeoPTRQziA03A== 测试ceph是否正常12345678910111213rbd create kube/test --size 1024 --image-format 2rbd ls kuberbd map kube/test # 如果报错, 警用 rbd info kube/test rbd feature disable kube/test exclusive-lock object-map fast-diff deep-flattenrbd map kube/testrbd showmappedmkfs.ext4 /dev/rbd0mkdir /data/rbd0mount /dev/rbd0 /data/rbd0cd /data/rbd0 &amp;&amp; echo test &gt; test.txt 在k8s 手动创建存储类创建ceph pg123456789101112131415161718192021# Total PGs = (Total_number_of_OSD * 100) / max_replication_count# pg = 1 * 100 /2 ~ 64(取2的次方数)# 这里准备创建2个pool, 每个poolceph osd pool create rbd-k8s 16# 查看ceph osd lspools# 创建imagerbd create rbd-k8s/cephimageredis --size 500M# 查看listrbd list rbd-k8s# 处理新特性# 查看inforbd info rbd-k8s/cephimageredis# 关闭exclusive-lock object-map fast-diff deep-flatten 这些特性rbd feature disable rbd-k8s/cephimageredis exclusive-lock object-map fast-diff deep-flatten 首先创建secret12#获取keygrep key /etc/ceph/ceph.client.admin.keyring |awk '&#123;printf "%s", $NF&#125;'|base64 ceph-secret.yaml1234567apiVersion: v1kind: Secretmetadata: name: ceph-secrettype: "kubernetes.io/rbd"data: key: QVFCTFo2dGNGNXFLRnhBQXBGTXJEdm5CY2k2UGtwZmZrN0JSVEE9PQ== 其次创建pv, pv是没有namespace概念的 persistentVolumeReclaimPolicy是清理规则 (retain: 不清理, Recycle: 回收) redis-ceph-pv.yml 1234567891011121314151617181920apiVersion: v1kind: PersistentVolumemetadata: name: redis2-ceph-rbd-pvspec: capacity: storage: 100Mi accessModes: - ReadWriteOnce rbd: monitors: - '192.168.0.134:6789' pool: rbd-k8s image: cephimageredis user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Recycle 执行部署pv1kubectl create -f redis-ceph-pv.yml 然后创建pvc redis-ceph-pvc.yml 12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: redis2-ceph-rbd-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 100Mi 执行部署pvc1kubectl create -f redis-ceph-pvc.yml 最后在rancher上选择挂载rbd]]></content>
      <categories>
        <category>技术文档</category>
        <category>存储</category>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ceph</tag>
        <tag>cephfs</tag>
        <tag>k8s存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加看板娘]]></title>
    <url>%2F2019%2F09%2F24%2F5-hexo%E6%B7%BB%E5%8A%A0%E7%9C%8B%E6%9D%BF%E5%A8%98%2F</url>
    <content type="text"><![CDATA[hexo6 左下角添加看板娘 github地址: 张书樵大神 下载大神项目 (会说话,换人物,小游戏等功能)12cd themes/nextv6/sourcegit clone https://github.com/stevenjoezhang/live2d-widget.git github说明比较详细, 这里简单说明 由于这里是克隆到了source目录, hexo d -g的时候会生成到public目录, 相当于站点根目录了 123456# 直接开启autoload.js注释const live2d_path = "/live2d-widget/";# 修改 themes/nextv6/layout/_layout.swig, 最后一行添加如下&lt;!-- 看板娘 --&gt;&lt;script src="/live2d-widget/autoload.js"&gt;&lt;/script&gt; 一般小白简单教程(只有看鼠标方向功能) hexo 官方支持版 需要安装模板1npm install --save hexo-helper-live2d 修改主题配置文件各种宠物预览 12345678910111213141516171819202122232425# Live2D## https://github.com/EYHN/hexo-helper-live2dlive2d: enable: true # enable: false scriptFrom: local # 默认 pluginRootPath: live2dw/ # 插件在站点上的根目录(相对路径) pluginJsPath: lib/ # 脚本文件相对与插件根目录路径 pluginModelPath: assets/ # 模型文件相对与插件根目录路径 # scriptFrom: jsdelivr # jsdelivr CDN # scriptFrom: unpkg # unpkg CDN # scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 你的自定义 url tagMode: false # 标签模式, 是否仅替换 live2d tag标签而非插入到所有页面中 debug: false # 调试, 是否在控制台输出日志 model: use: live2d-widget-model-haruto # npm-module package name # use: wanko # 博客根目录/live2d_models/ 下的目录名 # use: ./wives/wanko # 相对于博客根目录的路径 # use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 你的自定义 url display: position: left width: 150 height: 300 mobile: show: true # 手机中是否展示]]></content>
      <categories>
        <category>有趣</category>
        <category>博客</category>
        <category>二次元</category>
        <category>美化</category>
      </categories>
      <tags>
        <tag>hexo6</tag>
        <tag>特效</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo鼠标移动和鼠标点击特效]]></title>
    <url>%2F2019%2F09%2F24%2F4-hexo%E9%BC%A0%E6%A0%87%E7%A7%BB%E5%8A%A8%E5%92%8C%E9%BC%A0%E6%A0%87%E7%82%B9%E5%87%BB%E7%89%B9%E6%95%88%2F</url>
    <content type="text"><![CDATA[hexo6 鼠标添加点击出现桃心的特效 来自: Next主题个性化 所需要的js文件 未压缩 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051! function(e, t, a) &#123; function n() &#123; c( ".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"), o(), r() &#125; function r() &#123; for (var e = 0; e &lt; d.length; e++) d[e].alpha &lt;= 0 ? (t.body.removeChild(d[e].el), d.splice(e, 1)) : (d[e].y--, d[e].scale += .004, d[e].alpha -= .013, d[e].el.style.cssText = "left:" + d[e].x + "px;top:" + d[e].y + "px;opacity:" + d[e].alpha + ";transform:scale(" + d[e].scale + "," + d[e].scale + ") rotate(45deg);background:" + d[e].color + ";z-index:99999"); requestAnimationFrame(r) &#125; function o() &#123; var t = "function" == typeof e.onclick &amp;&amp; e.onclick; e.onclick = function(e) &#123; t &amp;&amp; t(), i(e) &#125; &#125; function i(e) &#123; var a = t.createElement("div"); a.className = "heart", d.push(&#123; el: a, x: e.clientX - 5, y: e.clientY - 5, scale: 1, alpha: 1, color: s() &#125;), t.body.appendChild(a) &#125; function c(e) &#123; var a = t.createElement("style"); a.type = "text/css"; try &#123; a.appendChild(t.createTextNode(e)) &#125; catch (t) &#123; a.styleSheet.cssText = e &#125; t.getElementsByTagName("head")[0].appendChild(a) &#125; function s() &#123; return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")" &#125; var d = []; e.requestAnimationFrame = function() &#123; return e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function(e) &#123; setTimeout(e, 1e3 / 60) &#125; &#125;(), n()&#125;(window, document); 压缩后 1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 将上面的内容贴到新增的themes/nextv6/source/js/src/love.js 文件中修改themes/nextv6/layout/_layout.swig文件, 末尾添加如下内容12&lt;!-- 鼠标桃心动画 --&gt;&lt;script type="text/javascript" src="/js/src/love.js"&gt;&lt;/script&gt; hexo6 鼠标移动添加星星特效来自: 愚人节鼠标跟随特效 新增js文件 themes/nextv6/source/js/src/love2.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143/*! * Fairy Dust Cursor.js * - 90's cursors collection * -- https://github.com/tholman/90s-cursor-effects * -- http://codepen.io/tholman/full/jWmZxZ/ */(function fairyDustCursor() &#123; var possibleColors = ["#D61C59", "#E7D84B", "#1B8798"] var width = window.innerWidth; var height = window.innerHeight; var cursor = &#123;x: width/2, y: width/2&#125;; var particles = []; function init() &#123; bindEvents(); loop(); &#125; // Bind events that are needed function bindEvents() &#123; document.addEventListener('mousemove', onMouseMove); document.addEventListener('touchmove', onTouchMove); document.addEventListener('touchstart', onTouchMove); window.addEventListener('resize', onWindowResize); &#125; function onWindowResize(e) &#123; width = window.innerWidth; height = window.innerHeight; &#125; function onTouchMove(e) &#123; if( e.touches.length &gt; 0 ) &#123; for( var i = 0; i &lt; e.touches.length; i++ ) &#123; addParticle( e.touches[i].clientX, e.touches[i].clientY, possibleColors[Math.floor(Math.random()*possibleColors.length)]); &#125; &#125; &#125; function onMouseMove(e) &#123; cursor.x = e.clientX; cursor.y = e.clientY; addParticle( cursor.x, cursor.y, possibleColors[Math.floor(Math.random()*possibleColors.length)]); &#125; function addParticle(x, y, color) &#123; var particle = new Particle(); particle.init(x, y, color); particles.push(particle); &#125; function updateParticles() &#123; // Updated for( var i = 0; i &lt; particles.length; i++ ) &#123; particles[i].update(); &#125; // Remove dead particles for( var i = particles.length -1; i &gt;= 0; i-- ) &#123; if( particles[i].lifeSpan &lt; 0 ) &#123; particles[i].die(); particles.splice(i, 1); &#125; &#125; &#125; function loop() &#123; requestAnimationFrame(loop); updateParticles(); &#125; /** * Particles */ function Particle() &#123; this.character = "*"; this.lifeSpan = 120; //ms this.initialStyles =&#123; "position": "fixed", "top": "0", //必须加 "display": "block", "pointerEvents": "none", "z-index": "10000000", "fontSize": "20px", "will-change": "transform" &#125;; // Init, and set properties this.init = function(x, y, color) &#123; this.velocity = &#123; x: (Math.random() &lt; 0.5 ? -1 : 1) * (Math.random() / 2), y: 1 &#125;; this.position = &#123;x: x - 10, y: y - 20&#125;; this.initialStyles.color = color; console.log(color); this.element = document.createElement('span'); this.element.innerHTML = this.character; applyProperties(this.element, this.initialStyles); this.update(); document.body.appendChild(this.element); &#125;; this.update = function() &#123; this.position.x += this.velocity.x; this.position.y += this.velocity.y; this.lifeSpan--; this.element.style.transform = "translate3d(" + this.position.x + "px," + this.position.y + "px,0) scale(" + (this.lifeSpan / 120) + ")"; &#125; this.die = function() &#123; this.element.parentNode.removeChild(this.element); &#125; &#125; /** * Utils */ // Applies css `properties` to an element. function applyProperties( target, properties ) &#123; for( var key in properties ) &#123; target.style[ key ] = properties[ key ]; &#125; &#125; init();&#125;)(); 修改themes/nextv6/layout/_layout.swig文件, 末尾添加如下内容12&lt;!-- 鼠标移动星星特效 --&gt;&lt;script type="text/javascript" src="/js/src/love2.js"&gt;&lt;/script&gt;]]></content>
      <categories>
        <category>有趣</category>
        <category>博客</category>
        <category>美化</category>
      </categories>
      <tags>
        <tag>hexo6</tag>
        <tag>特效</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s遇到的一些问题统计总结]]></title>
    <url>%2F2019%2F09%2F20%2F3-k8s%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[不定时更新,文章可能比较散乱,&gt;_&lt; 1. 单机版k8s pod一直是pending的问题 describe一下pod会发现错误: 1 node(s) had taints that the pod didnt tolerate.这是因为master上存在污点,pod不会再改节点上创建两种办法: deploy 的时候加上 容忍该污点 直接取消master上的污点 12345# 取消master上污点 kubectl taint nodes --all node-role.kubernetes.io/master-# 查看taintkubectl describe node node1 2. 修改service-node-port-range 由于traefik部署需要对外开放80端口, 但默认仅允许30000以上端口 123456789# kubeadm 1.14 配置apiServer: extraArgs: authorization-mode: Node,RBAC service-node-port-range: 79-33000# kubeadm 1.10配置apiServerExtraArgs: service-node-port-range: 79-33000 3. traefik断电后重新启动报错 command traefik error: field not found, node: redirect12345看到这个错误猜测可能是用的latest镜像问题, 从hub.docker.com 查看更新了v2.0+的版本将traefik的deployment配置中 image改成 traefik:1.7重新部署后 问题解 4. 查看当前集群的(CustomResourceDefinition)12345678# 查看k8s有哪些apikubectl api-versions# 查看当前crdkubectl get crd# 其次查看该api是什么版本kubectl describe crd destinationrules.networking.istio.io 5. 启用自动轮换kubelet 证书(证书未过期)参考: Kubeadm证书过期时间调整 kubelet证书分为server和client两种， k8s 1.9默认启用了client证书的自动轮换，但server证书自动轮换需要用户开启 增加 kubelet 参数 123# 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 增加如下参数Environment="KUBELET_EXTRA_ARGS=--feature-gates=RotateKubeletServerCertificate=true" 增加 controller-manager 参数 123456# 在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加如下参数 - command: - kube-controller-manager - --experimental-cluster-signing-duration=87600h0m0s - --feature-gates=RotateKubeletServerCertificate=true - .... 创建 rbac 对象创建 rbac对象，允许节点轮换kubelet server证书： 1234567891011121314151617181920212223242526272829303132cat &gt; ca-update.yaml &lt;&lt; EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: - certificates.k8s.io resources: - certificatesigningrequests/selfnodeserver verbs: - create---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubeadm:node-autoapprove-certificate-serverroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserversubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodesEOFkubectl create –f ca-update.yaml 重新启动kubelet 123systemctl daemon-reloadsystemctl enable kubeletsystemctl restart kubelet 6. hpa的一个cpu-percent百分比问题1kubectl autoscale deployment php-admintest-nginx-dev --cpu-percent=80 --min=1 --max=2 -n php-dev 例如以上,我希望在平均cpu超过80%时,pod能自动调整为2个 em…这没啥问题 但我做简单ab压测发现, 我把cpu压到了100000% … 我的deployment 配置中是这样限制cpu的 12345resources: requests: cpu: "1m" limits: cpu: "1000m" 显然我的pod可以使用1个核cpu, 那这个平均cpu是等于啥呢? cpu-percent = 1000m/resources.request.cpu =&gt; 1000m/1m =100000% -_-!!! 稍微解释下,为啥我要设置request.cpu=1m:比如单机4核k8s,我启动了1个pod,limit是4cpu,那么我request.cpu其实默认也是1, 所以集群就已经预留了4cpu, 此时如果在启动pod, 在配置limit的时候就无法成功启动pod,因为核心不够了,都给那什么玩意了…(当然这样改动也的确会出现过度分配) 因此建议 修改requests.cpu=500m, –cpu-percent可设范围: 0200% ,或者低一些 250m -&gt; 0400% 总结:request.cpu 必须设置, 这个是对比的对象 另外:对于扩容而言，这个时间段为3分钟，缩容为5分钟(可以通过 –horizontal-pod-autoscaler-downscale-delay ， –horizontal-pod-autoscaler-upscale-delay 进行调整)。 7 k8s1.16.2+metrics v0.3.5 deployment重启之后hpa就失效,无法获取到数据1The HPA was unable to compute the replica count: unable to get metrics for resource cpu: no metrics returned from resource metrics API 8 k8s机器进程数达到500多的时候, ssh连接到服务报错 shell request failed on channel 0123456789原因：目标主机的系统进程数太小，导致不能连接解决：需要修改/etc/security/limits.d/20-nproc.conf文件中的值，把4096改大一点，如 65535#cat /etc/security/limits.d/20-nproc.conf* soft nproc 4096root soft nproc unlimited重新ssh，即可。]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>问题总结</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署elk7.2.0]]></title>
    <url>%2F2019%2F09%2F19%2F2-%E9%83%A8%E7%BD%B2elk7-2-0%2F</url>
    <content type="text"><![CDATA[说明: 121 单台k8s,本机目录挂载(未配置cephfs)2 如果replicas大于1, 就会出现多个es挂载同一个目录,会出现报错(uuid block) 1. es配置本地挂载 k8s-es-7.2.0.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128---apiVersion: v1kind: ServiceAccountmetadata: labels: app: elasticsearch name: elasticsearch7-admin namespace: ns-elastic7---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: elasticsearch7-admin labels: app: elasticsearchroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: elasticsearch7-admin namespace: ns-elastic7---apiVersion: apps/v1kind: StatefulSetmetadata: labels: app: elasticsearch role: master name: elasticsearch-master namespace: ns-elastic7spec: replicas: 1 serviceName: elasticsearch-master selector: matchLabels: app: elasticsearch role: master template: metadata: labels: app: elasticsearch role: master spec: serviceAccountName: elasticsearch7-admin restartPolicy: Always securityContext: fsGroup: 1000 containers: - name: elasticsearch-master image: hub.zhangzw.com/bq/elasticsearch:7.2.0 command: ["bash", "-c", "ulimit -l unlimited &amp;&amp; sysctl -w vm.max_map_count=262144 &amp;&amp; chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data &amp;&amp; exec su elasticsearch docker-entrypoint.sh"] imagePullPolicy: IfNotPresent securityContext: privileged: true ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "800m" env: - name: cluster.name value: "es_cluster" - name: node.master value: "true" - name: node.data value: "true" - name: cluster.initial_master_nodes value: "elasticsearch-master-0" # 根据副本数和name配置 - name: discovery.zen.ping_timeout value: "5s" - name: node.ingest value: "false" - name: ES_JAVA_OPTS value: "-Xms1g -Xmx1g" - name: "discovery.zen.ping.unicast.hosts" value: "elasticsearch-discovery" # Disvocery Service - name: "http.cors.enabled" value: "true" - name: "http.cors.allow-origin" value: "*" volumeMounts: - name: elasticsearch-data-volume mountPath: /usr/share/elasticsearch/data volumes: - name: elasticsearch-data-volume hostPath: path: /data/k8s-container/elk-7.2.0/es-7.2.0/data---apiVersion: v1kind: Servicemetadata: labels: app: elasticsearch name: elasticsearch-discovery namespace: ns-elastic7spec: publishNotReadyAddresses: true ports: - name: transport port: 9300 targetPort: 9300 selector: app: elasticsearch role: master---kind: ServiceapiVersion: v1metadata: labels: app: elasticsearch name: elasticsearch-service namespace: ns-elastic7spec: type: NodePort ports: - port: 9200 targetPort: 9200 nodePort: 19230 protocol: TCP selector: app: elasticsearch 2. es配置nfs动态挂载 k8s-es-7.2.0-nfs.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129---apiVersion: v1kind: ServiceAccountmetadata: labels: app: elasticsearch name: elasticsearch-admin namespace: ns-elastic---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: elasticsearch-admin labels: app: elasticsearchroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: elasticsearch-admin namespace: ns-elastic---apiVersion: apps/v1kind: StatefulSetmetadata: labels: app: elasticsearch role: master name: elasticsearch-master namespace: ns-elasticspec: replicas: 2 volumeClaimTemplates: - metadata: name: elasticsearch-data-nfs annotations: volume.beta.kubernetes.io/storage-class: "nfs" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 2Gi serviceName: elasticsearch-master selector: matchLabels: app: elasticsearch role: master template: metadata: labels: app: elasticsearch role: master spec: serviceAccountName: elasticsearch-admin restartPolicy: Always securityContext: fsGroup: 1000 containers: - name: elasticsearch-master image: elasticsearch:7.2.0 command: ["bash", "-c", "ulimit -l unlimited &amp;&amp; sysctl -w vm.max_map_count=262144 &amp;&amp; chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/data &amp;&amp; exec su elasticsearch docker-entrypoint.sh"] imagePullPolicy: IfNotPresent volumeMounts: - name: elasticsearch-data-nfs mountPath: /usr/share/elasticsearch/data securityContext: privileged: true ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: cluster.name value: "es_cluster" - name: node.master value: "true" - name: node.data value: "true" - name: cluster.initial_master_nodes value: "elasticsearch-master-0,elasticsearch-master-1" # 根据副本数和name配置 - name: discovery.zen.ping_timeout value: "5s" - name: node.ingest value: "false" - name: ES_JAVA_OPTS value: "-Xms1g -Xmx1g" - name: "discovery.zen.ping.unicast.hosts" value: "elasticsearch-discovery" # Disvocery Service - name: "http.cors.enabled" value: "true" - name: "http.cors.allow-origin" value: "*"---apiVersion: v1kind: Servicemetadata: labels: app: elasticsearch name: elasticsearch-discovery namespace: ns-elasticspec: publishNotReadyAddresses: true ports: - name: transport port: 9300 targetPort: 9300 selector: app: elasticsearch role: master---kind: ServiceapiVersion: v1metadata: labels: app: elasticsearch name: elasticsearch-service namespace: ns-elasticspec: type: NodePort ports: - port: 9200 targetPort: 9200 nodePort: 19220 protocol: TCP selector: app: elasticsearch 3. kibana配置k8s-kibana-7.2.0.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485apiVersion: v1kind: ConfigMapmetadata: name: kibana-config namespace: ns-elastic7 labels: elastic-app: kibanadata: kibana.yml: | server.name: kibana server.host: "0" elasticsearch.hosts: [ "http://elasticsearch-service:9200" ] xpack.monitoring.ui.container.elasticsearch.enabled: true---kind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: kibana name: kibana namespace: ns-elastic7spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: kibana template: metadata: labels: elastic-app: kibana spec: containers: - name: kibana image: hub.zhangzw.com/bq/kibana:7.2.0 ports: - containerPort: 5601 protocol: TCP resources: requests: cpu: "50m" limits: cpu: "800m" volumeMounts: - name: kibana-config mountPath: /usr/share/kibana/config volumes: - name: kibana-config configMap: name: kibana-config tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---kind: ServiceapiVersion: v1metadata: labels: elastic-app: kibana name: kibana-service namespace: ns-elastic7spec: ports: - port: 5601 targetPort: 5601 selector: elastic-app: kibana type: NodePort---apiVersion: extensions/v1beta1kind: Ingressmetadata: labels: elastic-app: kibana name: kibana-ingress namespace: ns-elastic7spec: rules: - host: elk-kibana-dev.zhangzw.com http: paths: - backend: serviceName: kibana-service servicePort: 5601 4. logstash配置 本地挂载 k8s-logstash-7.2.0.yml 4.1 config/pipelines.yml 12- pipeline.id: main path.config: "/usr/share/logstash/config/pipeline/*.conf" 4.2 首先配置grok规则 config/pipeline/logstash.conf 12345678910111213141516171819202122input &#123; udp &#123; port =&gt; "10000" &#125; &#125; filter &#123; grok &#123; match =&gt; &#123; "message" =&gt; "\&#123;\"id\":\"(?&lt;id&gt;(.)*)\",\"tag\":\"(?&lt;tag&gt;(.)*)\",\"title\":\"%&#123;GREEDYDATA:title&#125;(?&lt;title&gt;(.|\r|\n)*)\",\"value\":\"%&#123;GREEDYDATA:value&#125;(?&lt;value&gt;(.|\r|\n)*)\",\"createdAt\":\"(?&lt;createdAt&gt;\S+ \S+)\",\"Telephone\":\"(?&lt;Telephone&gt;(.)*)\",\"uid\":\"(?&lt;uid&gt;(.)*)\",\"updateTime\":\"(?&lt;updateTime&gt;(.)*)\",\"appVersion\":\"(?&lt;appVersion&gt;(.)*)\",\"mobileModel\":\"(?&lt;mobileModel&gt;(.)*)\",\"osVersion\":\"(?&lt;osVersion&gt;(.)*)\",\"channel\":\"(?&lt;channel&gt;(.)*)\",\"UDID\":\"(?&lt;UDID&gt;(.)*)\"\&#125;" &#125; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [ "http://elasticsearch-service:9200" ] index =&gt; "k8s2-dev-%&#123;+YYYY.MM.dd&#125;" &#125; &#125; 4.3 配置文件 k8s-logstash-7.2.0.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455---kind: DeploymentapiVersion: apps/v1beta2metadata: labels: elastic-app: logstash name: logstash namespace: ns-elasticspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: elastic-app: logstash template: metadata: labels: elastic-app: logstash spec: containers: - name: logstash image: hub.zhangzw.com/bq/logstash:7.2.0 ports: - containerPort: 10000 protocol: UDP volumeMounts: - name: logstash-config mountPath: /usr/share/logstash/config volumes: - name: logstash-config hostPath: path: /data/k8s-pod/elk-7.2.0/logstash-7.2.0/config tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---kind: ServiceapiVersion: v1metadata: labels: elastic-app: logstash name: logstash-service namespace: ns-elasticspec: type: NodePort ports: - port: 10000 targetPort: 10000 nodePort: 10000 protocol: UDP selector: elastic-app: logstash type: NodePort---]]></content>
      <categories>
        <category>技术文档</category>
        <category>k8s</category>
        <category>elk</category>
        <category>elk7</category>
        <category>elasticsearch7</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>elk7</tag>
        <tag>elk</tag>
        <tag>elasticsearch7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[首次搭建hexo博客系统]]></title>
    <url>%2F2019%2F09%2F19%2F%E9%A6%96%E6%AC%A1%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[1 安装hexo 1.1 在mac上安装 12# 安装nodebrew install node npm 1.2 在linux安装 123# 安装node10curl -sL https://rpm.nodesource.com/setup_10.x | bash -yum install -y nodejs 1.3 安装hexo 12# 安装hexonpm install -g hexo 2 初始化123456789101112cd /data/github/# 初始化hexo init blog# 框架安装npm install#安装 Hexo 关于启动服务器的插件npm install hexo-server --save# 启动服务器, 本地查看效果, 如果不指定端口，默认为4000hexo server 3 主题和配置 下载主题：https://github.com/theme-next/hexo-theme-next 12unzip hexo-theme-next-master.zipmv hexo-theme-next-master $blog/themes/ 修改主题配置 _config.yml 中的其他属性 12345title: Zhangzhiwei's Blog...theme: hexo-theme-next...scheme: Mist 4. 编写更新博客 创建博客 1hexo new '第一个博客' cat source/_posts/第一个博客.md 123456title: 第一个博客date: 2019-09-19 16:58:01tags: - hexocategories: - hexo学习 github 创建一个Repository仓库 1231. 仓库名字必须是 xxx.github.io2. 在settings中 勾选Template repository3. 记得添加自己的ssh github配置 12 # 安装 hexo 关于 git 的组件npm install hexo-deployer-git --save 在_config.yml 中为 git 添加配置 1234deploy: type: git repository: git@github.com:*/*.github.io.git branch: master 查看是否能提交代码github 1ssh -T -ai ~/.ssh/id_rsa git@github.com 部署 1234hexo ghexo d或者hexo d -g 5. next6让首页文字预览显示 5.1 方法一: 自动形成摘要,默认截取的长度为 150 字符 1231. 找到主题的配置文件(themes/next/_config.yml)2. 修改auto_excerpt,把enable改为对应的false改为true3. hexo d -g 5.2 方法二: 博客内容中添加 &lt; !–more–&gt; 123# 安装nodebrew install node npm &lt;!-- more --&gt; 5.3 方法三: 在文章中的front-matter中添加description，并提供文章摘录,这种方式只会在首页列表中显示文章的摘要内容，进入文章详情后不会再显示。 1234567891011title: 部署elk7.2.0date: 2019-09-19 17:59:53copyright: truetags: - k8s - elk - elk7categories: - 技术文档 - elkdescription: 本文主要是简单单机版部署elk7体验, 并非高可用集群方式部署, 部分安装步骤省略. 主要是记录yml配置文件, 仅供参考. 详细内容请点击下方阅读全文, 非常感谢! 6. next6添加搜索功能123456789101. npm install hexo-generator-searchdb --save2. 全局配置文件_config.ymlsearch: path: search.xml field: post format: html limit: 100003. 修改主题的_config.ymllocal_search: enable: true 7. next6 Mist字体的 首页文章间距和首页页宽,字体 7.1 首页文章间距 12345增加一些内容: source/css/_schemes/Mist/_posts-expanded.styl.posts-expand .post &#123; margin-top: 30px; margin-bottom: 30px;&#125; 7.2 页宽 1234source/css/_variables/base.styl$content-desktop = 900px$content-desktop-large = 1000px$content-desktop-largest = 1100px 7.3 字体大小 1234567891011themes/next/source/css/_variables/base.styl$font-size-base = 0.95em;$font-size-base = unit(hexo-config('font.global.size'), em) if hexo-config('font.global.size') is a 'unit';$font-size-smallest = .75em;$font-size-smaller = .8125em;$font-size-small = .855em;$font-size-medium = 0.95em;$font-size-large = 0.975em;$font-size-larger = 1.em;$font-size-largest = 1.125em; 8. 添加网格 8.1 自定义方式修改 1234567891011121314# 新创建自定义文件cat themes/next/source/css/_custom/custom.styl// 主页文章添加阴影效果.post &#123;margin-top: 60px;margin-bottom: 60px;padding: 25px;-webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);-moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);&#125;# 修改config文件vim ./themes/next/_config.ymlcustom: custom 8.2 next6版本修改方式 参考: hexo6–next美化整理 修改 themes/next/layout/_layout.swig 1234&#123;% if theme.canvas_nest %&#125;&lt;script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"&gt;&lt;/script&gt;&#123;% endif %&#125; 将上述代码防止在&lt; /body&gt; 前就可以了(注意不要放在&lt; /head&gt;的后面)。 修改主题的_config.yml 123456canvas_nest: true//color: 线条颜色, 默认: '0,0,0'；三个数字分别为(R,G,B)//opacity: 线条透明度（0~1）, 默认: 0.5//count: 线条的总数量, 默认: 150//zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 注意:我这里打开提示缺少 canvas-nest.min.js文件,这里是手动copy的一份写到 source/lib/canvas-nest/canvas-nest.min.js 1!function()&#123;function o(w,v,i)&#123;return w.getAttribute(v)||i&#125;function j(i)&#123;return document.getElementsByTagName(i)&#125;function l()&#123;var i=j("script"),w=i.length,v=i[w-1];return&#123;l:w,z:o(v,"zIndex",-1),o:o(v,"opacity",0.5),c:o(v,"color","0,0,0"),n:o(v,"count",99)&#125;&#125;function k()&#123;r=u.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,n=u.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight&#125;function b()&#123;e.clearRect(0,0,r,n);var w=[f].concat(t);var x,v,A,B,z,y;t.forEach(function(i)&#123;i.x+=i.xa,i.y+=i.ya,i.xa*=i.x&gt;r||i.x&lt;0?-1:1,i.ya*=i.y&gt;n||i.y&lt;0?-1:1,e.fillRect(i.x-0.5,i.y-0.5,1,1);for(v=0;v&lt;w.length;v++)&#123;x=w[v];if(i!==x&amp;&amp;null!==x.x&amp;&amp;null!==x.y)&#123;B=i.x-x.x,z=i.y-x.y,y=B*B+z*z;y&lt;x.max&amp;&amp;(x===f&amp;&amp;y&gt;=x.max/2&amp;&amp;(i.x-=0.03*B,i.y-=0.03*z),A=(x.max-y)/x.max,e.beginPath(),e.lineWidth=A/2,e.strokeStyle="rgba("+s.c+","+(A+0.2)+")",e.moveTo(i.x,i.y),e.lineTo(x.x,x.y),e.stroke())&#125;&#125;w.splice(w.indexOf(i),1)&#125;),m(b)&#125;var u=document.createElement("canvas"),s=l(),c="c_n"+s.l,e=u.getContext("2d"),r,n,m=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(i)&#123;window.setTimeout(i,1000/45)&#125;,a=Math.random,f=&#123;x:null,y:null,max:20000&#125;;u.id=c;u.style.cssText="position:fixed;top:0;left:0;z-index:"+s.z+";opacity:"+s.o;j("body")[0].appendChild(u);k(),window.onresize=k;window.onmousemove=function(i)&#123;i=i||window.event,f.x=i.clientX,f.y=i.clientY&#125;,window.onmouseout=function()&#123;f.x=null,f.y=null&#125;;for(var t=[],p=0;s.n&gt;p;p++)&#123;var h=a()*r,g=a()*n,q=2*a()-1,d=2*a()-1;t.push(&#123;x:h,y:g,xa:q,ya:d,max:6000&#125;)&#125;setTimeout(function()&#123;b()&#125;,100)&#125;();% 9. 添加评论功能 9.1 注册leancloud 1注册-&gt; 验证邮箱-&gt; 实名认证 -&gt; 设置获取appid和appkey 9.2 修改配置文件 123456valine: enable: true appid: 'appid' appkey: 'appkey' placeholder: "ヾﾉ≧∀≦)o 来呀！快活呀！~啦啦啦~ 啦啦啦啦~" visitor: true //这个打开页会统计文章阅读数 10. next6添加字数统计 和阅读时长 hexo-symbols-count-time 10.1 安装node扩展 1npm install hexo-symbols-count-time --save 10.2 修改全局配置 _config.yml 123456symbols_count_time: symbols: true time: true total_symbols: true total_time: true exclude_codeblock: false 10.3 修改主题配置 _config.yml 1234567symbols_count_time: separated_meta: true item_text_post: true item_text_total: false awl: 4 wpm: 275 suffix: mins. 11. next6 文章置顶功能 11.1 安装node扩展 12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save 11.2 在文章开头添加置顶标识 1top: 10 11.3 首页添加明显置顶标识 123456themes/next/layout/_macro/post.swig 在&lt;div class="post-meta"&gt; 下添加如下代码&#123;% if post.top %&#125; &lt;i class="fa fa-thumb-tack"&gt;&lt;/i&gt; &lt;font color=green&gt;置顶&lt;/font&gt; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt;&#123;% endif %&#125; 12. next6 开启标签和分类 12.1 创建tags相关目录 12hexo new page tagshexo new page categories 12.2 开启tags标签和分类 123vim themes/next/_config.ymltags: /tags/ || tagscategories: /categories/ || th 12.3 修改tags站点文件 12345678cat source/tags/index.md---title: tagsdate: 2019-09-24 10:08:59type: "tags"layout: "tags"comments: false--- 12.4 修改categories站点文件 12345678cat source/categories/index.md---title: categoriesdate: 2019-09-24 10:09:55type: "categories"layout: "categories"comments: false--- 12.5 去掉xxx.github.io/tags/ 页面的post-title(因为我的这个css左对齐了,默认是居中,所以很丑) 123# 注释下面这段代码vim themes/nextv/layout/page.swig &lt;!-- &#123;% include '_partials/page/page-header.swig' %&#125; --&gt; 12.6 文章中多个tag和categories 123456tags: - k8s - k8s安装categories: - [k8s,安装] - [技术文档]]]></content>
      <categories>
        <category>有趣</category>
        <category>博客</category>
        <category>美化</category>
      </categories>
      <tags>
        <tag>hexo6</tag>
        <tag>hexo美化</tag>
      </tags>
  </entry>
</search>
