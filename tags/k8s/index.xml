<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on zhangzw</title>
    <link>https://www.ngirl.xyz/tags/k8s/</link>
    <description>Recent content in k8s on zhangzw</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Wed, 08 Jun 2022 16:35:12 +0800</lastBuildDate><atom:link href="https://www.ngirl.xyz/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>k8s遇到的一些问题统计总结</title>
      <link>https://www.ngirl.xyz/posts/3-k8s%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 10 Nov 2020 14:06:41 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/3-k8s%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E7%BB%9F%E8%AE%A1%E6%80%BB%E7%BB%93/</guid>
      <description>不定时更新,文章可能比较散乱,&amp;gt;_&amp;lt;
  1. 单机版k8s pod一直是pending的问题  describe一下pod会发现错误: 1 node(s) had taints that the pod didnt tolerate. 这是因为master上存在污点,pod不会再改节点上创建 两种办法:
  deploy 的时候加上 容忍该污点 直接取消master上的污点  # 取消master上污点 kubectl taint nodes --all node-role.kubernetes.io/master- # 查看taint kubectl describe node node1   2. 修改service-node-port-range  由于traefik部署需要对外开放80端口, 但默认仅允许30000以上端口
 # kubeadm 1.14 配置 apiServer: extraArgs: authorization-mode: Node,RBAC service-node-port-range: 79-33000 # kubeadm 1.10配置 apiServerExtraArgs: service-node-port-range: 79-33000   3. traefik断电后重新启动报错 command traefik error: field not found, node: redirect 看到这个错误猜测可能是用的latest镜像问题, 从`hub.</description>
    </item>
    
    <item>
      <title>K8s源码学习 简单记录</title>
      <link>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Wed, 08 Jun 2022 16:35:12 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%AE%80%E5%8D%95%E8%AE%B0%E5%BD%95/</guid>
      <description>源码查看rc和rs的区别 通过追寻 NewControllerInitializers 函数中到RS和RC的源码发现, startReplicationController 和 startReplicaSetController 都是 NewBaseController 函数初始化的 ReplicaSetController 结构体, 不过两个传参不通 RS是: ctx.InformerFactory.Apps().V1().ReplicaSets() RC是: ctx.InformerFactory.Core().V1().ReplicationControllers(), 所以我们就需要找到RC和RS的InformerFactory的不同 沿着 NewControllerInitializers 往回找, 我们看到了 StartControllers 中会真正的执行 NewControllerInitializers 初始化的 controllers 代码: for controllerName, initFn := range controllers { debugHandler, started, err := initFn(ctx) } 这里的循环就会执行 startReplicationController(ctx) 和 startReplicaSetController(ctx), 这里的ctx就是StartControllers的第一个参数 回到 (cmd/kube-controller-manager/app/controllermanager.go) Run方法中, StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux) 的controllerContext 正是 我们要找的ctx 往上我们看到 -&amp;gt; CreateControllerContext() -&amp;gt; NewSharedInformerFactory() -&amp;gt; NewSharedInformerFactoryWithOptions 找到了 sharedInformerFactory 结构体 sharedInformerFactory 就是真正的执行了 .Apps().V1().ReplicaSets() 和 .</description>
    </item>
    
    <item>
      <title>K8s源码学习 Job源码分析</title>
      <link>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-job%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 09 Dec 2021 10:22:02 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-job%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</guid>
      <description>首先我们知道cronjob和job是受控于controller manager组件的, 下面我们先看看job controller是怎么初始化的
 初始化的流程 1. cmd/kube-controller-manager/controller-manager.go main() -&amp;gt; app.NewControllerManagerCommand() 2. cmd/kube-controller-manager/app/controllermanager.go NewControllerManagerCommand() *cobra.Command -&amp;gt; Run(...) -&amp;gt; NewControllerInitializers(...) map[string]InitFunc // 这里返回控制器的map,后续会对他遍历执行InitFunc  { controllers[&amp;#34;job&amp;#34;] = startJobController // job的初始化  controllers[&amp;#34;cronjob&amp;#34;] = startCronJobController } 3. cmd/kube-controller-manager/app/batch.go startJobController(...) (...) -&amp;gt; NewJobController(...) *JobController // 返回一个JobController  { jm.syncHandler = jm.syncJob // 核心的handler初始化  } -&amp;gt; (jm *JobController) Run(...) -&amp;gt; (jm *JobController) worker() -&amp;gt; for jm.processNextWorkItem() {} // 这里是循环  -&amp;gt; jm.syncHandler(...) // 最终在processNextWorkItem的函数内部调用了syncHandler, 也就是上面初始化的syncJob JobController 结构体 type JobController struct { kubeClient clientset.</description>
    </item>
    
    <item>
      <title>K8s源码学习 client-go延迟队列</title>
      <link>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-client-go%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97/</link>
      <pubDate>Wed, 08 Dec 2021 16:04:39 +0800</pubDate>
      
      <guid>https://www.ngirl.xyz/k8s/k8s%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-client-go%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97/</guid>
      <description>我们知道在部署 pod的时候, 如果失败了, pod会不断的重试, 重试的时间间隔也是不一样的, 重试到一定次数就不在重试
那我们这里的重试间隔时间是如何控制的呢?
下面我们去了解一下
从 controller 找到 deployment 初始化方法看一下
 1 deployment 初始化  k8s的大多组件代码格式结构是类似的
 1. cmd/kube-controller-manager/controller-manager.go main() -&amp;gt; app.NewControllerManagerCommand() 2. cmd/kube-controller-manager/app/controllermanager.go NewControllerManagerCommand() *cobra.Command -&amp;gt; Run(...) -&amp;gt; NewControllerInitializers(...) map[string]InitFunc // 这里返回控制器的map,后续会对他遍历执行InitFunc  { controllers[&amp;#34;deployment&amp;#34;] = startDeploymentController } 3. cmd/kube-controller-manager/app/apps.go startDeploymentController(...) (...) { dc, err := deployment.NewDeploymentController(...) go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) } 3.1 pkg/controller/deployment/deployment_controller.go -&amp;gt; NewDeploymentController(...) (...) { dc := &amp;amp;DeploymentController{ queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &amp;#34;deployment&amp;#34;), } } -&amp;gt; NewNamedRateLimitingQueue(.</description>
    </item>
    
    <item>
      <title>istio环境下配置nginx&#43;php</title>
      <link>https://www.ngirl.xyz/posts/52-istio%E6%B5%8B%E8%AF%95nginx-php%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Tue, 04 Aug 2020 16:22:10 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/52-istio%E6%B5%8B%E8%AF%95nginx-php%E9%A1%B9%E7%9B%AE/</guid>
      <description>&lt;p&gt;将nginx+php的环境结合istio的智能路由功能做一个简单实践&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s搭建radius</title>
      <link>https://www.ngirl.xyz/posts/51-k8s%E6%90%AD%E5%BB%BAradius/</link>
      <pubDate>Mon, 27 Jul 2020 10:19:25 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/51-k8s%E6%90%AD%E5%BB%BAradius/</guid>
      <description>搭建lnmp+freeradius的账号认证服务
  一 记录 docker: 17.03.2-ce k8s: 1.15.11 php: 7.0.13 mysql: 5.6 freeraidus: 3.0.21 daloradius: 1.1-2 / 08 Aug 2019  配置修改简单说明:
 1. 数据库创建说明 create database radius; grant all on radius.* to radius@&#39;%&#39; identified by &#39;xxx&#39;; flush privileges; # 导入表结构 mysql -hk8s-db-t.xxx.com -P3326 -uradius -p &amp;lt; mods-config/sql/main/mysql/schema.sql mysql -hk8s-db-t.xxx.com -P3326 -uradius -p &amp;lt; daloradius-php/contrib/db/mysql-daloradius.sql mysql -hk8s-db-t.xxx.com -P3326 -uradius -p &amp;lt; daloradius-php/contrib/db/fr2-mysql-daloradius-and-freeradius.sql 2. 修改freeRADIUS配置 vim /etc/raddb/radiusd.conf # 这里我是配合nginx,php统一路径 logdir = /nginx_logs 3.</description>
    </item>
    
    <item>
      <title>k8s升级(1.10-&gt;1.15)</title>
      <link>https://www.ngirl.xyz/posts/48-k8s%E5%8D%87%E7%BA%A7-1-10-1-15/</link>
      <pubDate>Mon, 18 May 2020 17:43:27 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/48-k8s%E5%8D%87%E7%BA%A7-1-10-1-15/</guid>
      <description>&lt;p&gt;k8s升级一般不能跨版本升级, 所以这里间断介绍升级过程, 每次升级一个大版本&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s安装promethues&#43;alertmanager&#43;grafana</title>
      <link>https://www.ngirl.xyz/posts/47-k8s%E5%AE%89%E8%A3%85promethues-alertmanager-grafana/</link>
      <pubDate>Thu, 14 May 2020 18:34:47 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/47-k8s%E5%AE%89%E8%A3%85promethues-alertmanager-grafana/</guid>
      <description>本文主要是针对prometheus 简单部署 , 考虑到测试资源有限,为更精简自定义按照监控,减少多余资源占用, 也同时更了解prometheus的更多细节, 这里没有使用prometheus-operator
 1 K8s上部署原生的prometheus 2 prometheus-operator 方式部署 3 docker-compose快速搭建 Prometheus+Grafana监控系统 4 一套prometheus监控多个k8s集群,详细讲解配置 5 使用prometheus监控traefik、redis、k8s集群各节点、各节点kubelet 6 grafana 监控模板下载     搭建prometheus 创建ns tee ns.yml &amp;lt;&amp;lt;- EOF apiVersion: v1 kind: Namespace metadata: name: monitoring EOF kubectl apply -f ns.yml prometheus-clusterRole.yml tee prometheus-clusterRole.yml &amp;lt;&amp;lt;- EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - &amp;quot;&amp;quot; resources: - nodes/metrics verbs: - get - nonResourceURLs: - /metrics verbs: - get EOF prometheus-clusterRoleBinding.</description>
    </item>
    
    <item>
      <title>gitlab-ci与k8s结合</title>
      <link>https://www.ngirl.xyz/posts/45-gitlab-ci%E4%B8%8Ek8s%E7%BB%93%E5%90%88/</link>
      <pubDate>Thu, 23 Apr 2020 16:20:33 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/45-gitlab-ci%E4%B8%8Ek8s%E7%BB%93%E5%90%88/</guid>
      <description>本文介绍如何通过gitlab-ci整合k8s实现流水线部署
  https://www.cnblogs.com/Sinte-Beuve/p/11739196.html https://www.qikqiak.com/post/gitlab-ci-k8s-cluster-feature/   环境版本统计 1 gitlab/gitlab-runner 0.15.0 2 helm 2.16 3 k8s 1.16.4 4 gitlab 11.5 5 CentOS Linux release 7.7 /kernel 5.2  小节
 1. gitlab 通过admin管理页面的runner配置, 安装gitlab-runner, 安装方式可以是二进制, docker 或k8s (这里是k8s) 2. gitlab 项目目录的 Operations -&amp;gt; kubernetes -&amp;gt; Add Kubernetes Cluster -&amp;gt; Add existing cluster 是结合k8s, 每个项目都需要设置一个k8s集群,k8s集群需要配置rbac权限 3. ci 在提交到私有harbor上是需要验证账号密码, 私有仓库拉取也需要验证  安装方法一   k8s yaml直接部署 gitlab-ci-token-secret.yaml  具体token值 请查看 gitlab的admin页面-&amp;gt; Overview -&amp;gt; Runners 查看, 然后base64加密</description>
    </item>
    
    <item>
      <title>k8s的yaml配置名词解释(模板)</title>
      <link>https://www.ngirl.xyz/posts/43-k8s%E7%9A%84yaml%E9%85%8D%E7%BD%AE%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-%E6%A8%A1%E6%9D%BF/</link>
      <pubDate>Tue, 31 Mar 2020 17:21:23 +0000</pubDate>
      
      <guid>https://www.ngirl.xyz/posts/43-k8s%E7%9A%84yaml%E9%85%8D%E7%BD%AE%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A-%E6%A8%A1%E6%9D%BF/</guid>
      <description>针对Deployment的yaml配置解释说明
   原文: K8s Deployment YAML 名词解释
 Deployment API 版本对照表 Kubernetes 版本 Deployment 版本 v1.5-v1.15 extensions/v1beta1 v1.7-v1.15 apps/v1beta1 v1.8-v1.15 apps/v1beta2 v1.9+ apps/v1 Deployment yaml 名词解释： apiVersion:apps/v1 # 指定api版本，此值必须在kubectl api-versions中 kind:Deployment # 指定创建资源的角色/类型 metadata:# 资源的元数据/属性name:demo # 资源的名字，在同一个namespace中必须唯一namespace:default# 部署在哪个namespace中labels:# 设定资源的标签app:demoversion:stablespec:# 资源规范字段replicas:1# 声明副本数目revisionHistoryLimit:3# 保留历史版本selector:# 选择器matchLabels:# 匹配标签app:demoversion:stablestrategy:# 策略rollingUpdate:# 滚动更新maxSurge:30%# 最大额外可以存在的副本数，可以为百分比，也可以为整数maxUnavailable:30%# 示在更新过程中能够进入不可用状态的 Pod 的最大值，可以为百分比，也可以为整数type:RollingUpdate# 滚动更新策略template:# 模版metadata:# 资源的元数据/属性annotations:# 自定义注解列表sidecar.istio.io/inject:&amp;#34;false&amp;#34;# 自定义注解名字labels:# 设定资源的标签app:demoversion:stablespec:# 资源规范字段containers:- name:demo# 容器的名字 image:demo:v1# 容器使用的镜像地址 imagePullPolicy:IfNotPresent# 每次Pod启动拉取镜像策略，三个选择 Always、Never、IfNotPresent# Always，每次都检查；Never，每次都不检查（不管本地是否有）；IfNotPresent，如果本地有就不检查，如果没有就拉取resources:# 资源管理limits:# 最大使用cpu:300m# CPU，1核心 = 1000mmemory:500Mi# 内存，1G = 1000Mirequests:# 容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行cpu:100mmemory:100MilivenessProbe:# pod 内部健康检查的设置httpGet:# 通过httpget检查健康，返回200-399之间，则认为容器正常path:/healthCheck# URI地址port:8080# 端口scheme:HTTP# 协议# host: 127.</description>
    </item>
    
  </channel>
</rss>
