<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ceph on zhangzw</title>
    <link>https://zhangzw001.github.io/tags/ceph/</link>
    <description>Recent content in ceph on zhangzw</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Thu, 26 Sep 2019 14:12:49 +0000</lastBuildDate><atom:link href="https://zhangzw001.github.io/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>k8s部署storageclass动态创建pv(nfs&amp;rbd)</title>
      <link>https://zhangzw001.github.io/posts/7-k8s%E9%83%A8%E7%BD%B2storageclass%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv-nfs-rbd/</link>
      <pubDate>Thu, 26 Sep 2019 14:12:49 +0000</pubDate>
      
      <guid>https://zhangzw001.github.io/posts/7-k8s%E9%83%A8%E7%BD%B2storageclass%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv-nfs-rbd/</guid>
      <description>考虑到k8s存储的问题, 本机目录挂载存在太大局限性, 多node多pod的服务存储急迫需要共享存储, 这里简单应用k8s storageclass nfs和rbd存储
第一部分 nfs 这里单节点简单配置nfs(高并发可采用nfs+rsync+inotify或Sersync)  高并发参考
 NFS高可用(NFS+keepalive+Sersync) inotify+rsync实时备份总结
#安装nfs yum install -y nfs-utils rpcbind # 创建目录 mkdir /data/nfs echo &amp;quot;/data/nfs 192.168.0.0/24(rw,sync,no_root_squash) &amp;quot; &amp;gt;&amp;gt;/etc/exports # 启动服务 systemctl start rpcbind systemctl start nfs k8s部署storageclass环境-nfs 导入外部配置 git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/nfs-client/deploy #注意 1 node节点需要安装nfs-utils(centos7),nfs-common(ubuntu) 修改deployment.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.</description>
    </item>
    
    <item>
      <title>ceph安装部署</title>
      <link>https://zhangzw001.github.io/posts/6-ceph%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Thu, 26 Sep 2019 11:13:40 +0000</pubDate>
      
      <guid>https://zhangzw001.github.io/posts/6-ceph%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</guid>
      <description>Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。
简单了解什么是块存储/对象存储/文件系统存储？ ceph 目前提供对象存储（RADOSGW）、块存储RDB以及 CephFS 文件系统这 3 种功能。对于这3种功能介绍，分别如下：
  对象存储，也就是通常意义的键值存储，其接口就是简单的GET、PUT、DEL 和其他扩展，代表主要有 Swift 、S3 以及 Gluster 等；
  块存储，这种接口通常以 QEMU Driver 或者 Kernel Module 的方式存在，这种接口需要实现 Linux 的 Block Device 的接口或者 QEMU 提供的 Block Driver 接口，如 Sheepdog，AWS 的 EBS，青云的云硬盘和阿里云的盘古系统，还有 Ceph 的 RBD（RBD是Ceph面向块存储的接口）。在常见的存储中 DAS、SAN 提供的也是块存储；
  文件存储，通常意义是支持 POSIX 接口，它跟传统的文件系统如 Ext4 是一个类型的，但区别在于分布式存储提供了并行化的能力，如 Ceph 的 CephFS (CephFS是Ceph面向文件存储的接口)，但是有时候又会把 GlusterFS ，HDFS 这种非POSIX接口的类文件存储接口归入此类。当然 NFS、NAS也是属于文件系统存储；
  参考教程 Kubernetes 集成 Ceph 后端存储教程 centos7安装ceph集群
准备 配置源 cat &amp;gt;/etc/yum.</description>
    </item>
    
  </channel>
</rss>
