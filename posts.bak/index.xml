<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts.baks on zhangzw</title>
    <link>https://blog.k1s.club/posts.bak/</link>
    <description>Recent content in Posts.baks on zhangzw</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Fri, 11 Sep 2020 20:46:38 +0800</lastBuildDate><atom:link href="https://blog.k1s.club/posts.bak/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Go: A Documentary 发布！</title>
      <link>https://blog.k1s.club/posts.bak/reading/documentary-of-go/</link>
      <pubDate>Fri, 11 Sep 2020 20:46:38 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/reading/documentary-of-go/</guid>
      <description>以前经常有读者问我，哪儿可以找到 Go 语言的前世今生，这种时候我们往往会告诉他去看 issues 和 proposals。但资料有点分散，且没有索引体系。因此不少人新入门的读者读着读着就跑偏了，又或是在第一步找资料上就被拦住了。
最近欧神（@changkun）低调的发布了 《Go: A Documentary》，这个文档收集了 Go 开发过程中许多有趣（公开可见的）的问题，讨论，提案，CL 和演讲，其目的是为 Go 历史提供全面的参考。
个人认为这份资料非常的有价值，相当于欧神把资料索引整理好了，强烈推荐对 Go 语言感兴趣的读者进行阅读：
内容索引主要分为：
 Sources Committers  Core Authors Compiler/Runtime Team Library/Tools/Security/Community Group Interviews   Timeline Language Design  Misc Slice Package Management (1.4, 1.5, 1.7) Type alias (1.9) Defer (1.13) Error values (1.13) Channel/Select Generics   Compiler Toolchain  Compiler Linker Debugger Tracer Builder Modules gopls Testing   Runtime Core  Statistics Scheduler Execution Stack Memory Allocator Garbage Collector Memory model ABI   Standard Library  syscall io go/* sync Pool Mutex atomic time context encoding image, x/image misc   Unclassified But Relevant Links Fun Facts Acknowledgements  《Go: A Documentary》 的访问地址是 https://golang.</description>
    </item>
    
    <item>
      <title>微服务的战争：选型？分布式链路追踪</title>
      <link>https://blog.k1s.club/posts.bak/microservice/tracing/</link>
      <pubDate>Thu, 10 Sep 2020 19:53:59 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/microservice/tracing/</guid>
      <description>“微服务的战争” 是一个关于微服务设计思考的系列题材，主要是针对在微服务化后所出现的一些矛盾/冲突点，不涉及具体某一个知识点深入。如果你有任何问题或建议，欢迎随时交流。
 背景 在经历 微服务的战争：级联故障和雪崩 的 P0 级别事件后，你小手一摊便葛优躺了。开始进行自我复盘，想起这次排查经历，由于现在什么基础设施都还没有，因此在接收到客户反馈后，你是通过错误日志进行问题检查的。
但在级联错误中，错误日志产生的实在是太多了，不同的服务不同的链路几乎都挤在一起，修复时间都主要用在了翻日志上，翻了好几页才找到了相对有效的错误信息。
如果下一次在出现类似的问题，可不得了，MTTR 太久了，4 个 9 很快就会用完。这时候你想到了业界里经常被提起的一个利器，那就是 “分布式链路追踪系统”。粗略来讲，能够看到各种应用的调用依赖：
其中最著名的是 Google Dapper 论文所介绍的 Dapper。源于 Google 为了解决可能由不同团队，不同语言，不同模块，部署在不同服务器，不同数据中心的所带来的软件复杂性（很难去分析，无法做定位），构建了一个的分布式跟踪系统：
自此就开启了业界在分布式链路的启发/启蒙之路，很多现在出名的分布式链路追踪系统都是基于 Google Dapper 论文发展而来，基本原理和架构都大同小异。若对此有兴趣的可具体查看 Google Dapper，非常有意思。
（Google Dapper 中存在跟踪树和 Span 的概念）
选型？有哪些 想做链路追踪，那必然要挑选一款开源产品作为你的分布式链路追踪系统，不大可能再造一个全新的，先实现业务目的最重要。因此在网上一搜，发现如下大量产品：
 Twitter：Zipkin。 Uber：Jaeger。 Elastic Stack：Elastic APM。 Apache：SkyWalking（国内开源爱好者吴晟开源）。 Naver：Pinpoint（韩国公司开发）。 阿里：鹰眼。 大众点评：Cat。 京东：Hydra。  随手一搜就发现这类产品特别的多，并且据闻各大公司都有自己的一套内部链路追踪系统，这下你可犯了大难。他们之间都是基于 Google Dapper 演进出来的，那本质上到底有什么区别，怎么延伸出这么多的新产品？
Jaeger 首先看看由 Uber 开发的 Jaeger，Jaeger 目前由 Cloud Native Computing Foundation（CNCF）托管，是 CNCF 的第七个顶级项目（于 2019 年 10 月毕业）：
  Jaeger Client：Jaeger 客户端，是 Jaeger 针对 OpenTracing API 的特定语言实现，可用于手动或通过与 OpenTracing 集成的各种现有开源框架（例如Flask，Dropwizard，gRPC等）来检测应用程序以进行分布式跟踪。</description>
    </item>
    
    <item>
      <title>微服务的战争：级联故障和雪崩</title>
      <link>https://blog.k1s.club/posts.bak/microservice/linkage/</link>
      <pubDate>Tue, 25 Aug 2020 21:08:39 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/microservice/linkage/</guid>
      <description>“微服务的战争” 是一个关于微服务设计思考的系列题材，主要是针对在微服务化后所出现的一些矛盾/冲突点，不涉及具体某一个知识点深入。如果你有任何问题或建议，欢迎随时交流。
 在 微服务的战争：统一且标准化 中，经过好几周与不同业务组不同事业部的跨部门讨论后，终于把初始的标准化方案给定下来了，大家欢快的使用起了内部的统一框架，疯狂的创建起了新服务，没隔多久服务调用链就变成了下图：
服务间存在多次内部调用，服务 A =》服务 B =》服务 C =》服务D，而 服务 E =》 服务 B，服务 F =》服务 E，也就是存在着多个流量入口，且依赖相同的服务。
背景 服务与服务中，总存在业务服务，公共服务，基础服务等类型。但在某一个夜晚，突然发现 BFF 调用后端服务开始逐渐不正常，客户给你截图反馈问题，你发现有点问题：
单从表现来看，你发现是 BFF 调用服务 A 极度缓慢，也不知道怎么了&amp;hellip;正当以为是服务 A 出问题，想着万能重启一下时。你在日志平台和链路追踪系统一看，发现了大量的错误日志和缓慢，让你略微震惊，一时间不知道从何下手。
这可怎么办？
级联故障和雪崩 实际上这是一次很经典的级联故障，最终导致系统雪崩的情景再现。单从上述拓扑来看，问题点之一在于服务 B：
服务 B 本身作为服务 A 和服务 F 的两个流量入口必经之处，想必至少是一个公共服务，但他也依赖了其他多个服务。因此若服务 C 和服务 D 其中一个有问题，在没有熔断措施的情况下，就出现级联故障，系统逐渐崩盘，最后雪崩：
服务 D 所依赖的外部接口出现了故障，而他并没有做任何的控制，因此扩散到了所有调用到他的服务，自然也就包含服务 B，因此最终出现系统雪崩。
这种最经典的是出现在默认 Go http client 调用没有设置 Timeout，从而只要出现一次故障，就足矣让记住这类 “坑”，毕竟崩的 ”慢“，错误日志还多。
解决方法 常见的方式是根据特定的规则/规律进行熔断和降级，避免请求发生堆积：
  超时时间控制。
  慢调用比例。
  错误比例。</description>
    </item>
    
    <item>
      <title>微服务的战争：统一且标准化</title>
      <link>https://blog.k1s.club/posts.bak/microservice/standardization/</link>
      <pubDate>Sat, 22 Aug 2020 21:56:14 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/microservice/standardization/</guid>
      <description>“微服务的战争” 是一个关于微服务设计思考的系列题材，主要是针对在微服务化后所出现的一些矛盾/冲突点，不涉及具体某一个知识点深入。如果你有任何问题或建议，欢迎随时交流。
 开天辟地 在远古开天辟地时，大单体转换成微服务化后，服务的数量越来越多。每起一个新的服务，就得把项目的目录结构，基础代码重新整理一遍，并且很有可能都是从最初的 template 上 ctrl+c，ctrl+v 复制出来的产物，如下：
但是基于 template 的模式，很快就会遇到各种各样的新问题：
随着跨事业部/业务组的使用增多，你根本不知道框架的 template 是什么时间节点被复制粘贴出去的，也不知道所对应的 commit-id 是什么，更不知道先前的 BUG 修复了没，也不知道有没有其他开发人员私下改过被复制走的 template。
简单来讲，就是不具备可维护性，相对独立，BUG 可能一样，但却没有版本可规管。这时候，就可以选择做一个内部基础框架和对应的内部工具（已经有用户市场了），形成一个脚手架闭环：
通过基础工具+基础接口的方式，就可以解决项目A、B、C&amp;hellip;的基础框架版本管理和公共维护的问题，且在遇到框架 BUG 时，只需要直接 upgrade 就好了。而在框架维护者层面，还能通过注册机制知道目前基础框架的使用情况（例如：版本分布），便于后续的迭代和规划。
同时若内部微服务依赖复杂，可以将脚手架直接 “升级”，再做多一层基础平台，通过 CI/CD 平台等关联创建应用，选择应用类型等基本信息，然后关联创建对应的应用模板、构建工具、网关、数据库、接口平台、初始化自动化用例等：
至此，就可以通过结合基础平台（例如：CI/CD）实现流程上的标准化控制，成为一个提效好帮手。
大众创新 但，一切都有 “开天辟地” 那么顺利吗。实际上并不，在很多的公司中，大多数是在不同的时间阶段在不同的团队同时进行了多个开天辟地。
更具现化来讲，就是在一家公司内，不同的团队里做出了多种基础工具和基础框架。更要命的是，他们几家的规范可能还不大一样。例如：框架在 gRPC 错误码的规范处理上的差异：
  业务错误码放在 grpc.status.details 中。
  业务错误码放在 grpc-status 中。
  业务错误码放在 grpc-message 中。
  又或是 HTTP 状态码的差异：
  HTTP Status Code 为金标准，不在主体定义业务错误码。
  HTTP Status Code 都为 200 OK（除宕机导致的 500，503 等），业务错误码由主体另外定义。</description>
    </item>
    
    <item>
      <title>微服务的战争：按什么维度拆分服务</title>
      <link>https://blog.k1s.club/posts.bak/microservice/dismantle/</link>
      <pubDate>Wed, 19 Aug 2020 20:56:55 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/microservice/dismantle/</guid>
      <description>“微服务的战争” 是一个关于微服务设计思考的系列题材，主要是针对在微服务化后所出现的一些矛盾/冲突点，不涉及具体某一个知识点深入。如果你有任何问题或建议，欢迎随时交流。
 微服务，这三个字正在席卷着目前的互联网软件行业，尤其在近几年云原生迸发后，似乎人人都对微服务有了更广泛的使用和理解，张口就是各种各样的问号，有着强大的好奇心。
无独有偶，我有一个朋友鲤鱼在内部微服务的早期（每个业务组起步）就经常遇到下述的对话：
  张三：为什么要拆现在的代码？
  鲤鱼：因为 ！@）&amp;amp;&amp;amp;#@！）&amp;amp;#！&amp;amp;）@！&amp;amp;！ 的原因。
  张三：那即将要做的 “微服务” 是按照什么维度去拆分的服务？
  鲤鱼：常见的一般根据 ！@#*@！#&amp;amp;！（@&amp;amp;！@）#@ 的方式来拆分。
  张三：照你这么说好像也不大对，我看每个业务组拆分的维度似乎都不大一样？
  鲤鱼：嗯，每个业务组还有自己的见解，不会完全相同。
  张三：。。。所以微服务的拆分维度到底是什么？
  为什么想拆 为什么张三会有这个疑问呢，实际上是因为研发内部希望从原先的大单体，大仓库向微服务体系拆分转换，其原先大单体仓库结构，类 Monorepo：
但类 Monorepo 又有不少的问题，像是：
  单个 Repo 体积过大：导致 Git 无法直接拉取。当你设置完再拉取时，在网速慢时还能去泡杯咖啡，并且在开发机性能不佳的情况下，IDE 会比较卡，代码运行起来也慢。
  单个 Repo 存在公共函数/SDK：在代码仓库中，必然存在公共依赖。因此在解决代码冲突时，若遗留了冲突符，且在动态语言中，不涉及便运行正常。但其实在上线后却又影响到其他业务，可真是糟糕透顶，分分钟被迫抱着事故。
  单个 Repo 模块职责/边界不清：在实际的软件开发中，涉及数十个业务组同时在一个大 Repo 下进行开发，没有强控边界的情况下，往往会逐渐模糊，即使在设计时管得住自己，你也不一定能 100% 防止别人模糊你的边界。
  单个 Repo 包含了所有的源码：出现公司源代码泄露时，会导致整个 Repo 外泄，相当的刺激和具有教育意义。因为虽然开放和协同了，不属于你们组的业务代码你也有权限查看了。</description>
    </item>
    
    <item>
      <title>新书《Go语言编程之旅：一起用Go做项目》出版啦！</title>
      <link>https://blog.k1s.club/posts.bak/go-programming-tour-book/</link>
      <pubDate>Fri, 03 Jul 2020 21:06:33 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/go-programming-tour-book/</guid>
      <description>从我开始写技术文章起，不知不觉近三年过去了，咨询和催我出书和读者逐年递增，在 2019 年算是达到一个高峰。当然，综合考虑下我也是一直拒绝的，觉得火候还不够。
直至 2019.09 月，polaris 主动找到了我，说有事情想找我商量，本着 “如果你在纠结一件事情做还是不做，不如先做了看看结果，至少不会后悔” 的想法，更何况是长期被 Ping，因此我一口答应下来，故事自此开始了。
本书定位 本书不直接介绍 Go 语言的语法基础，内容将面向项目实践，同时会针对核心细节进行分析。而在实际项目迭代中，常常会出现或多或少的事故，因此本书也针对 Go 语言的大杀器（分析工具）以及常见问题进行了全面讲解。
本书适合已经大致学习了 Go 语言的基础语法后，想要跨越到下一个阶段的开发人员，可以填补该阶段的空白和进一步拓展你的思维方向。
读者定位  基本了解 Go 语言的语法和使用方式的开发人员。 想要进行 Go 相关项目实践和进一步摸索的开发人员。 希望熟悉 Go 常用分析工具的开发人员。  本书大纲 本书针对常见的项目类型，主要细分为 5 + 1 板块，分别是命令行、HTTP、RPC、Websocket 应用、进程内缓存以及 Go 中的大杀器。
同时我们在项目开发、细节分析、运行时分析等方方面面都进行了较深入的介绍和说明，能够为 Go 语言开发者提供相对完整的项目实践经验，而如果深入阅读第六章的章节，更能够为未来各类问题出现时的问题排查提供一份强大的知识板块。
如下为本书的思维导图概览：
如何阅读这本书 常规的列目录未免太无趣。我想不如说说从我个人的角度，所看到读者们在近 3 年来是如何阅读/实践我的实践系列文章的，其面向的读者群体是完全一致的。希望能够从另外一个角度告诉你，应当如何阅读这本书，尽可能的效益最大化。
首先，图书，买来要读，而与实战结合的图书，势必需要实践，实践最常见又分为脑内思考和上机实践：
而在持续的交流中，可以发现至少会延伸出以下几类深入层次的不同：
  第一层：只阅读，留有印象，需要时再唤醒，也行。
  第二层：阅读并实践，实打实的完成项目实践，收获丰满。
  第三层：实践的过程中，一定会遇到或大或小的问题，有的人会放弃，这就是分叉点。但有的读者会持续排查，其提升了个人能力（排错能力很重要）。
  第四层：实践完毕后，有自己的想法，认为某某地方还可以这样，也可以再实现更多的功能，举一反三，进一步拓展，并对项目提 issues 或进行 pr。
  第五层：完成整体项目后，抽离业务代码，标准化框架，实现框架的应用脚手架，并有的读者会进一步开源。
  第六层：形成脚手架后，在自己业务组开始落地，实际在项目中使用，由业务学习转化为企业实践。</description>
    </item>
    
    <item>
      <title>为什么容器内存占用居高不下，频频 OOM（续）</title>
      <link>https://blog.k1s.club/posts.bak/why-container-memory-exceed2/</link>
      <pubDate>Fri, 19 Jun 2020 21:29:08 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/why-container-memory-exceed2/</guid>
      <description>在上周的文章《为什么容器内存占用居高不下，频频 OOM》 中，我根据现状进行了分析和说明，收到了很多读者的建议和疑惑，因此有了这一篇文章，包含更进一步的说明和排查。
疑问 一般系统内存过高的情况下，可以通过 free -m 查看当前系统的内存使用情况：
在发现是系统内存占用高后，就会有读者会提到，为什么不 “手动清理 Cache”，因为 Cache 高的话，可以通过 drop_caches 的方式来清理：
 清理 page cache：  $ echo 1 &amp;gt; /proc/sys/vm/drop_caches 清理 dentries 和 inodes：  $ echo 2 &amp;gt; /proc/sys/vm/drop_caches 清理 page cache、dentries 和 inodes：  $ echo 3 &amp;gt; /proc/sys/vm/drop_caches 但新问题又出现了，因为我们的命题是在容器中，在 Kubernetes 中，若执行 drop_caches 相关命令，将会对 Node 节点上的所有其他应用程序产生影响，尤其是那些占用大量 IO 并由于缓冲区高速缓存而获得更好性能的应用程序，可能会产生 “负面” 后果。
我想这并不是一个好办法。
表象 回归原始，那就是为什么要排查这个问题，本质原因就是容器设置了 Memory Limits，而容器在运行中达到了 Limits 上限，被 OOM 掉了，所以我们想知道为什么会出现这个情况。
在前文中我们针对了五大类情况进行了猜想：
 频繁申请重复对象。 不知名内存泄露。 madvise 策略变更。 监控/判别条件有问题。 容器环境的机制。  在逐一排除后，后续发现容器的 Memory OOM 判定标准是 container_memory_working_set_bytes 指标，其实际组成为 RSS + Cache（最近访问的内存、脏内存和内核内存）。</description>
    </item>
    
    <item>
      <title>为什么容器内存占用居高不下，频频 OOM</title>
      <link>https://blog.k1s.club/posts.bak/why-container-memory-exceed/</link>
      <pubDate>Sun, 07 Jun 2020 14:52:19 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/why-container-memory-exceed/</guid>
      <description>最近我在回顾思考（写 PPT），整理了现状，发现了这个问题存在多时，经过一番波折，最终确定了元凶和相对可行的解决方案，因此也在这里分享一下排查历程。
时间线：
  在上 Kubernetes 的前半年，只是用 Kubernetes，开发没有权限，业务服务极少，忙着写新业务，风平浪静。
  在上 Kubernetes 的后半年，业务服务较少，偶尔会阶段性被运维唤醒，问之 “为什么你们的服务内存占用这么高，赶紧查”。此时大家还在为新业务冲刺，猜测也许是业务代码问题，但没有调整代码去尝试解决。
  在上 Kubernetes 的第二年，业务服务逐渐增多，普遍增加了容器限额 Limits，出现了好几个业务服务是内存小怪兽，因此如果不限制的话，服务过度占用会导致驱逐，因此反馈语也就变成了：“为什么你们的服务内存占用这么高，老被 OOM Kill，赶紧查”。据闻也有几个业务大佬有去排查（因为 OOM 反馈），似乎没得出最终解决方案。
  不禁让我们思考，为什么个别 Go 业务服务，Memory 总是提示这么高，经常达到容器限额，以至于被动 OOM Kill，是不是有什么安全隐患？
现象 内存居高不下 发现个别业务服务内存占用挺高，触发告警，且通过 Grafana 发现在凌晨（没有什么流量）的情况下，内存占用量依然拉平，没有打算下降的样子，高峰更是不得了，像是个内存炸弹：
并且我所观测的这个服务，早年还只是 100MB。现在随着业务迭代和上升，目前已经稳步 4GB，容器限额 Limits 纷纷给它开道，但我想总不能是无休止的增加资源吧，这是一个大问题。
进入重启怪圈 有的业务服务，业务量小，自然也就没有调整容器限额，因此得不到内存资源，又超过额度，就会进入疯狂的重启怪圈：
重启将近 300 次，非常不正常了，更不用提所接受到的告警通知。
排查 猜想一：频繁申请重复对象 出现问题的个别业务服务都有几个特点，那就是基本为图片处理类的功能，例如：图片解压缩、批量生成二维码、PDF 生成等，因此就怀疑是否在量大时频繁申请重复对象，而 Go 本身又没有及时释放内存，因此导致持续占用。
sync.Pool 基本上想解决 “频繁申请重复对象”，我们大多会采用多级内存池的方式，也可以用最常见的 sync.Pool，这里可参考全成所借述的《Go 夜读》上关于 sync.Pool 的分享，关于这类情况的场景：
 当多个 goroutine 都需要创建同⼀个对象的时候，如果 goroutine 数过多，导致对象的创建数⽬剧增，进⽽导致 GC 压⼒增大。形成 “并发⼤－占⽤内存⼤－GC 缓慢－处理并发能⼒降低－并发更⼤”这样的恶性循环。</description>
    </item>
    
    <item>
      <title>Proto 代码到底放哪里？</title>
      <link>https://blog.k1s.club/posts.bak/where-is-proto/</link>
      <pubDate>Sat, 23 May 2020 15:07:37 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/where-is-proto/</guid>
      <description>虽然公司已经从大单体切换为微服务化有一定的年头了，但一些细节方面的处理总会有不同的人有不同的看法，这其中一个讨论点，就是 Proto 这个 IDL 的代码到底放在哪里？
目前来看，一共有如下方案， 我们一起来探讨一下 Proto 的存储方式和对应带来的优缺点。
方案一：存放在代码仓库 直接将项目所依赖到的所有 Proto 文件都存放在 proto/ 目录下，不经过开发工具的自动拉取和发布：
缺点   项目所有依赖的 Proto 都存储在代码仓库下，因此所有依赖 Proto 都需要人工的向其它业务组 “要” 来，再放到 proto/ 目录下，人工介入极度麻烦。
  Proto 升级和变更，经常要重复第一步，沟通成本高。
  优点   项目所有依赖的 Proto 都存储在代码仓库下，因此不涉及个人开仓库权限的问题。
  多 Proto 的切换开销减少，因为都在代码仓库下，不需要看这看那。
  方案二：独立仓库 独立仓库存储是我们最早采取的方式，也就是每个服务对应配套一个 Proto 仓库：
这个方案的好处就是可以独立管理所有 Proto 仓库，并且权限划分清晰。但最大的优点也是最大的缺点，因为一个服务会依赖多个 Proto 仓库，并且存在跨业务组调用的情况：
如上图所示，svc-user 服务分别依赖了三块 Proto 仓库，分别是自己组的、业务组 A、业务组 B 总共的 6 个 Proto 仓库。
缺点  假设你是一个新入职的开发人员，那么你就需要找不同的业务组申请不同的仓库权限，非常麻烦。如果没有批量赋权工具，也没有管理者权限，那么就需要一个个赋权，非常麻烦。 在运行服务的时候，你需要将所有相关联的 Proto 仓库拉取下来，如果没有工具做半自动化的支持，麻烦程度无法忍受。  优点   使得安全性较高（但 IDL 本身没有太多的秘密）。</description>
    </item>
    
    <item>
      <title>使用 Prometheus 对 Go 程序进行指标采集</title>
      <link>https://blog.k1s.club/posts.bak/prometheus/2020-05-16-pull/</link>
      <pubDate>Sun, 17 May 2020 17:52:37 +0800</pubDate>
      
      <guid>https://blog.k1s.club/posts.bak/prometheus/2020-05-16-pull/</guid>
      <description>在前面的章节中，已经知道了如何对应用程序进行 Prometheus metrics 的注册和暴露，那么接下来如何让 Prometheus 对应用程序进行采集呢。
设置采集配置 首先打开先前所安装的 prometheus 软件目录：
$ ls LICENSE data promtool NOTICE prometheus rules console_libraries prometheus.yml tsdb consoles prometheus.yml.default 打开并修改 prometheus.yml 文件，查看到 scrape_configs 配置选项，进行如下调整：
... scrape_configs: - job_name: &amp;#39;test01&amp;#39; static_configs: - targets: [&amp;#39;127.0.0.1:10001&amp;#39;] scheme: http tls_config: insecure_skip_verify: false 先前所启动的应用程序是在本地，且端口号为 10001，协议为 http，对应的配置简述：
  job_name：采集的任务名。
  static_configs.targets：设置要采集的目标对象列表。
  scheme：采集的目标协议（例如：http、https）。
  tls_config.insecure_skip_verify：是否跳过证书校验。
  常用的配置项如上几个，其默认的 metrics path 为 /metrics，若需要调整则增加 metrics_path 配置项进行调整就可以了，接着重新启动 prometheus 就可以了。
其他配置项 再返回来看看 prometheus.</description>
    </item>
    
  </channel>
</rss>
